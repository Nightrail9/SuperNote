{
  "notes": [
    {
      "id": "note_cb4c3cea52cd",
      "title": "4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html",
      "contentMd": "# 前向传播、反向传播和计算图\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [前向传播](#前向传播)\n  - [计算图](#计算图)\n  - [反向传播](#反向传播)\n  - [训练过程中的依赖关系](#训练过程中的依赖关系)\n- [代码与示例](#代码与示例)\n- [常见问题与避坑](#常见问题与避坑)\n- [复习清单](#复习清单)\n\n---\n\n## 核心概念摘要\n\n| 概念 | 定义 | 关键特征 |\n|:---|:---|:---|\n| **前向传播 (Forward Propagation)** | 从输入层到输出层顺序计算神经网络各层结果 | 存储中间变量，用于后续反向传播 |\n| **反向传播 (Backpropagation)** | 利用链式法则从输出层到输入层计算参数梯度 | 复用前向传播的中间值，避免重复计算 |\n| **计算图 (Computational Graph)** | 可视化操作符与变量依赖关系的有向图 | 节点=变量/操作，边=数据流向 |\n| **自动微分 (Autograd)** | 自动计算梯度的技术 | 现代深度学习框架的核心能力 |\n\n**核心洞察**：训练比预测需要更多内存，因为必须保留前向传播的中间值直到反向传播完成。\n\n---\n\n## 知识点详解\n\n### 前向传播\n\n#### 是什么\n按顺序（输入层 → 隐藏层 → 输出层）计算并存储神经网络每层的结果。\n\n#### 数学推导（单隐藏层MLP）\n\n设输入样本 $\\mathbf{x} \\in \\mathbb{R}^d$，网络结构如下：\n\n| 步骤 | 计算 | 维度 |\n|:---|:---|:---|\n| 1. 隐藏层线性变换 | $\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x}$ | $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}, \\mathbf{z} \\in \\mathbb{R}^h$ |\n| 2. 激活函数 | $\\mathbf{h} = \\phi(\\mathbf{z})$ | $\\mathbf{h} \\in \\mathbb{R}^h$ |\n| 3. 输出层线性变换 | $\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h}$ | $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}, \\mathbf{o} \\in \\mathbb{R}^q$ |\n| 4. 损失计算 | $L = l(\\mathbf{o}, y)$ | 标量 |\n| 5. L2正则化 | $s = \\frac{\\lambda}{2}\\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right)$ | 标量 |\n| 6. 目标函数 | $J = L + s$ | 标量 |\n\n其中 $\\|\\cdot\\|_F$ 表示 Frobenius 范数（矩阵展平后的 L2 范数）。\n\n---\n\n### 计算图\n\n#### 可视化结构\n```\n        ┌─────────┐\n   x ──→│  W^(1)  │──→ z ──→ φ ──→ h ──→ W^(2) ──→ o ──→ L ──┐\n        └─────────┘                                          ├──→ J = L + s\n        λ/2·‖W^(1)‖² ────────────────────────────────────────┤\n        λ/2·‖W^(2)‖² ────────────────────────────────────────┘\n```\n\n**图例说明**：\n- □ 正方形：变量（输入、参数、中间结果）\n- ○ 圆形：操作符（矩阵乘法、激活函数、加法等）\n- → 箭头：数据流向（主要向右、向上）\n\n---\n\n### 反向传播\n\n#### 核心原理：链式法则\n\n对于复合函数 $Z = g(Y), Y = f(X)$：\n\n$$\\frac{\\partial Z}{\\partial X} = \\text{prod}\\left(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial Y}{\\partial X}\\right)$$\n\n> `prod` 运算符：执行必要的转置和维度对齐后进行矩阵/张量乘法。\n\n#### 梯度计算流程（从输出到输入）\n\n| 步骤 | 梯度计算 | 结果维度 |\n|:---|:---|:---|\n| 1. 目标函数分解 | $\\frac{\\partial J}{\\partial L} = 1, \\quad \\frac{\\partial J}{\\partial s} = 1$ | 标量 |\n| 2. 输出层梯度 | $\\frac{\\partial J}{\\partial \\mathbf{o}} = \\frac{\\partial L}{\\partial \\mathbf{o}}$ | $\\mathbb{R}^q$ |\n| 3. 正则化项梯度 | $\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)}, \\quad \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}$ | 同权重维度 |\n| 4. **W^(2) 的梯度** | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$ | $\\mathbb{R}^{q \\times h}$ |\n| 5. 隐藏层输出梯度 | $\\frac{\\partial J}{\\partial \\mathbf{h}} = {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}$ | $\\mathbb{R}^h$ |\n| 6. 激活前梯度（逐元素乘） | $\\frac{\\partial J}{\\partial \\mathbf{z}} = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$ | $\\mathbb{R}^h$ |\n| 7. **W^(1) 的梯度** | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}$ | $\\mathbb{R}^{h \\times d}$ |\n\n#### 关键公式详解\n\n**输出层权重梯度**（公式 4.7.11）：\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\underbrace{\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top}_{\\text{来自损失}} + \\underbrace{\\lambda \\mathbf{W}^{(2)}}_{\\text{来自正则化}}$$\n\n**隐藏层权重梯度**（公式 4.7.14）：\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\underbrace{\\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top}_{\\text{链式传递}} + \\underbrace{\\lambda \\mathbf{W}^{(1)}}_{\\text{来自正则化}}$$\n\n> **注意**：$\\odot$ 表示 Hadamard 积（逐元素乘法），因激活函数 $\\phi$ 是逐元素应用的。\n\n---\n\n### 训练过程中的依赖关系\n\n#### 双向依赖机制\n\n```\n┌─────────────────┐         ┌─────────────────┐\n│   前向传播       │ ←────→ │   反向传播       │\n│  (Forward Pass) │  依赖   │ (Backward Pass) │\n└─────────────────┘         └─────────────────┘\n        ↓                           ↑\n   • 计算并存储 h               • 使用 h 计算 ∂J/∂W^(2)\n   • 计算当前 W 下的 s          • 使用存储的 z 计算 ϕ'(z)\n   • 生成所有中间变量            • 复用中间值，避免重算\n```\n\n#### 内存影响分析\n\n| 场景 | 内存需求 | 原因 |\n|:---|:---|:---|\n| **预测 (Inference)** | 低 | 只需当前层激活，可逐层释放 |\n| **训练 (Training)** | 高 | 需保留所有中间值至反向传播完成 |\n\n**内存占用因素**：\n- 与网络层数成正比（更深 → 更多中间值）\n- 与批量大小成正比（更大 batch → 更大张量）\n\n> ⚠️ 深层网络 + 大批量 = 显存不足 (OOM) 风险\n\n---\n\n## 代码与示例\n\n### 手动实现前向与反向传播\n\n```python\nimport torch\nfrom torch import nn\n\n# 定义简单的单隐藏层MLP\nclass SimpleMLP:\n    def __init__(self, input_dim, hidden_dim, output_dim, lambda_reg=0.01):\n        # 初始化参数\n        self.W1 = torch.randn(hidden_dim, input_dim, requires_grad=True)\n        self.W2 = torch.randn(output_dim, hidden_dim, requires_grad=True)\n        self.lambda_reg = lambda_reg\n        \n    def forward(self, x):\n        \"\"\"\n        前向传播: x -> z -> h -> o -> J\n        保存中间值供反向传播使用\n        \"\"\"\n        # 线性变换: z = W^(1) @ x\n        self.z = self.W1 @ x  # 保存 z\n        # 激活: h = φ(z)\n        self.h = torch.relu(self.z)  # 保存 h\n        # 输出: o = W^(2) @ h\n        self.o = self.W2 @ self.h  # 保存 o\n        \n        return self.o\n    \n    def compute_loss(self, o, y):\n        \"\"\"计算总损失 J = L + s\"\"\"\n        # 损失项 L (MSE示例)\n        L = ((o - y) ** 2).sum() / 2\n        \n        # 正则化项 s = λ/2 * (||W1||² + ||W2||²)\n        s = self.lambda_reg / 2 * (\n            (self.W1 ** 2).sum() + (self.W2 ** 2).sum()\n        )\n        \n        self.L = L  # 保存用于调试\n        self.s = s\n        \n        return L + s\n    \n    def backward_manual(self, x, y):\n        \"\"\"\n        手动反向传播实现\n        演示梯度计算的核心逻辑\n        \"\"\"\n        # 前向传播获取中间值\n        o = self.forward(x)\n        J = self.compute_loss(o, y)\n        \n        # ========== 反向传播开始 ==========\n        \n        # 1. dJ/dL = 1, dJ/ds = 1\n        dJ_dL = 1.0\n        dJ_ds = 1.0\n        \n        # 2. dJ/do = dL/do (MSE: o - y)\n        dJ_do = (o - y)  # 形状: (output_dim,)\n        \n        # 3. ds/dW 的正则化部分\n        ds_dW2_reg = self.lambda_reg * self.W2\n        ds_dW1_reg = self.lambda_reg * self.W1\n        \n        # 4. dJ/dW^(2) = dJ/do @ h^T + λ*W^(2)\n        #    外积: (q,) @ (h,) -> (q, h)\n        dJ_dW2 = torch.outer(dJ_do, self.h) + ds_dW2_reg\n        \n        # 5. dJ/dh = W^(2)^T @ dJ/do\n        dJ_dh = self.W2.T @ dJ_do  # 形状: (hidden_dim,)\n        \n        # 6. dJ/dz = dJ/dh ⊙ φ'(z)  (ReLU导数: z > 0 ? 1 : 0)\n        relu_deriv = (self.z > 0).float()\n        dJ_dz = dJ_dh * relu_deriv  # 逐元素乘\n        \n        # 7. dJ/dW^(1) = dJ/dz @ x^T + λ*W^(1)\n        dJ_dW1 = torch.outer(dJ_dz, x) + ds_dW1_reg\n        \n        return {\n            'dJ_dW1': dJ_dW1,\n            'dJ_dW2': dJ_dW2,\n            'loss': J.item()\n        }\n\n\n# ========== 验证与PyTorch自动微分的一致性 ==========\n\ndef test_gradient_correctness():\n    \"\"\"验证手动实现与torch.autograd的一致性\"\"\"\n    torch.manual_seed(42)\n    \n    # 创建模型和数据\n    model = SimpleMLP(input_dim=4, hidden_dim=5, output_dim=3, lambda_reg=0.1)\n    x = torch.randn(4)\n    y = torch.randn(3)\n    \n    # 手动反向传播\n    manual_grads = model.backward_manual(x, y)\n    \n    # PyTorch自动微分\n    model.zero_grad()\n    o = model.forward(x)\n    J = model.compute_loss(o, y)\n    J.backward()\n    \n    # 比较梯度\n    print(\"梯度一致性检查:\")\n    print(f\"W1 梯度误差: {(manual_grads['dJ_dW1'] - model.W1.grad).abs().max():.2e}\")\n    print(f\"W2 梯度误差: {(manual_grads['dJ_dW2'] - model.W2.grad).abs().max():.2e}\")\n    \n    return manual_grads\n\n# 运行测试\n# grads = test_gradient_correctness()\n```\n\n### 使用PyTorch简洁实现\n\n```python\nimport torch\nfrom torch import nn\n\nclass MLPModule(nn.Module):\n    \"\"\"使用PyTorch内置机制的简洁实现\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, lambda_reg=0.01):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim, bias=False)\n        self.lambda_reg = lambda_reg\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n    \n    def loss_with_regularization(self, pred, target):\n        \"\"\"包含L2正则化的完整损失\"\"\"\n        # 数据损失\n        data_loss = nn.functional.mse_loss(pred, target)\n        \n        # L2正则化 (weight decay)\n        reg_loss = 0\n        for param in self.parameters():\n            reg_loss += (param ** 2).sum()\n        \n        return data_loss + self.lambda_reg / 2 * reg_loss\n\n\n# 训练循环示例\ndef train_step(model, optimizer, x, y):\n    \"\"\"单步训练，展示前向-反向-更新的完整流程\"\"\"\n    model.train()\n    \n    # ===== 前向传播 =====\n    optimizer.zero_grad()      # 清除旧梯度\n    pred = model(x)            # 计算输出\n    loss = model.loss_with_regularization(pred, y)  # 计算损失\n    \n    # ===== 反向传播 =====\n    loss.backward()            # 自动计算所有梯度\n    \n    # ===== 参数更新 =====\n    optimizer.step()           # 应用梯度更新\n    \n    return loss.item()\n\n\n# 内存监控示例\ndef monitor_memory():\n    \"\"\"监控训练时的内存占用\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        \n        # 模拟训练\n        model = MLPModule(1000, 500, 10).cuda()\n        x = torch.randn(64, 1000, device='cuda')  # batch_size=64\n        y = torch.randn(64, 10, device='cuda')\n        \n        pred = model(x)\n        loss = model.loss_with_regularization(pred, y)\n        loss.backward()\n        \n        peak_mb = torch.cuda.max_memory_allocated() / 1024**2\n        print(f\"峰值显存占用: {peak_mb:.2f} MB\")\n        \n        # 对比：仅前向传播\n        torch.cuda.reset_peak_memory_stats()\n        with torch.no_grad():\n            _ = model(x)\n        forward_only_mb = torch.cuda.max_memory_allocated() / 1024**2\n        print(f\"仅前向传播: {forward_only_mb:.2f} MB\")\n        print(f\"反向传播额外开销: {peak_mb - forward_only_mb:.2f} MB\")\n```\n\n---\n\n## 常见问题与避坑\n\n### Q1: 为什么训练时显存占用突然暴增？\n\n| 原因 | 解释 | 解决方案 |\n|:---|:---|:---|\n| 保留了计算图 | 意外将含梯度的张量存入列表 | 使用 `.detach()` 或 `with torch.no_grad()` |\n| 批量过大 | 中间激活值与batch size成正比 | 减小batch size，或使用梯度累积 |\n| 网络过深 | 每层激活都需保存 | 使用梯度检查点 (gradient checkpointing) |\n| 使用了RNN/LSTM | 序列长导致展开图巨大 | 截断BPTT，或使用更高效的实现 |\n\n### Q2: 反向传播中的常见错误\n\n```python\n# ❌ 错误：在不需要梯度的地方没有禁用\ndef wrong_inference(model, x):\n    pred = model(x)  # 仍构建计算图！\n    return pred.argmax()\n\n# ✅ 正确：推理时禁用梯度计算\ndef correct_inference(model, x):\n    with torch.no_grad():  # 不追踪操作，节省内存\n        pred = model(x)\n    return pred.argmax()\n\n# ❌ 错误：手动实现时修改了中间变量\nz = W1 @ x\nh = torch.relu(z)\nh[0] = 0  # 破坏计算图！无法正确求导\n\n# ✅ 正确：使用in-place操作需谨慎\n# 或使用 clone() 创建副本\n```\n\n### Q3: 二阶导数的计算图变化\n\n若需计算 Hessian 矩阵（二阶导数）：\n- **计算图扩展**：在一阶导数基础上继续反向传播\n- **时间复杂度**：从 $O(n)$ 增至 $O(n^2)$（n为参数数量）\n- **内存需求**：需保留一阶导数的计算图，进一步增加\n\n```python\n# 二阶导数示例（Hessian对角线）\nx = torch.randn(3, requires_grad=True)\ny = x.pow(2).sum()\n\n# 一阶导\ngrad = torch.autograd.grad(y, x, create_graph=True)[0]  # dy/dx = 2x\n\n# 二阶导（对一阶导再求导）\nhessian_diag = torch.autograd.grad(grad.sum(), x)[0]  # d²y/dx² = 2\n```\n\n---\n\n## 复习清单\n\n- [ ] **前向传播**：能写出单隐藏层MLP的完整前向公式（含正则化）\n- [ ] **计算图理解**：能手绘简单网络的计算图，标明变量和操作\n- [ ] **链式法则**：掌握向量/矩阵形式的链式法则，理解 `prod` 操作\n- [ ] **反向传播推导**：能独立推导 $\\partial J/\\partial \\mathbf{W}^{(1)}$ 和 $\\partial J/\\partial \\mathbf{W}^{(2)}$\n- [ ] **逐元素激活**：理解为何使用 Hadamard 积 $\\odot$ 而非矩阵乘法\n- [ ] **内存管理**：解释训练比预测耗内存的原因，列举优化策略\n- [ ] **自动微分**：熟练使用 `backward()`, `grad()`, `no_grad()` 等API\n\n### 关键公式速查\n\n| 公式 | 含义 |\n|:---|:---|\n| $\\mathbf{z} = \\mathbf{W}^{(1)}\\mathbf{x}$ | 隐藏层线性变换 |\n| $\\mathbf{h} = \\phi(\\mathbf{z})$ | 激活函数 |\n| $\\mathbf{o} = \\mathbf{W}^{(2)}\\mathbf{h}$ | 输出层线性变换 |\n| $J = l(\\mathbf{o}, y) + \\frac{\\lambda}{2}(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2)$ | 带正则化的目标函数 |\n| $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial J}{\\partial \\mathbf{o}}\\mathbf{h}^\\top + \\lambda\\mathbf{W}^{(2)}$ | 输出层权重梯度 |\n| $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\left({\\mathbf{W}^{(2)}}^\\top\\frac{\\partial J}{\\partial \\mathbf{o}}\\odot\\phi'(\\mathbf{z})\\right)\\mathbf{x}^\\top + \\lambda\\mathbf{W}^{(1)}$ | 隐藏层权重梯度 |",
      "createdAt": "2026-02-15T17:01:52.432Z",
      "updatedAt": "2026-02-15T17:01:52.432Z"
    },
    {
      "id": "note_f332dce99470",
      "title": "我让OpenClaw自动炒股， 最后赚钱了吗？| 摩尔线程 AIBOOK",
      "sourceUrl": "https://www.bilibili.com/video/BV1s4ZLBAE22/?spm_id_from=333.1007.tianma.3-2-6.click",
      "contentMd": "## 目录\n\n- [核心概念摘要 (Key Takeaways)](#核心概念摘要-key-takeaways)\n- [知识点详解 (Detailed Notes)](#知识点详解-detailed-notes)\n  - [OpenClaw 技术架构解析](#openclaw-技术架构解析)\n  - [Skill 扩展机制](#skill-扩展机制)\n  - [国产 AI 开发环境：摩尔线程 MTT AIBook](#国产-ai-开发环境摩尔线程-mtt-aibook)\n  - [云端算力接入：夸克云 AI Coding Plan](#云端算力接入夸克云-ai-coding-plan)\n- [代码实战 / 算法推导 (Code & Math)](#代码实战--算法推导-code--math)\n  - [Skill 开发流程示例](#skill-开发流程示例)\n  - [系统架构图](#系统架构图)\n- [避坑指南 & 常见QA (Tips & QA)](#避坑指南--常见qa-tips--qa)\n\n---\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **OpenClaw 是具备 GUI 操作能力的 AI Agent**：不同于传统 LLM 仅输出文本，它能直接操控计算机界面执行任务，实现\"所见即所得\"的自动化操作。\n\n2. **Skill 机制是 OpenClaw 的能力扩展核心**：通过自定义 Skill 可赋予 Agent 实时数据获取能力（如股票行情 API、财经新闻爬虫），弥补其原生知识时效性不足的缺陷。\n\n3. **国产 AI 开发链路已具备完整可用性**：从摩尔线程 MTT AIBook（端侧 Linux 开发本）到夸克云 AI Coding Plan（云端 GLM-4 推理），形成\"端云协同\"的国产化替代方案。\n\n4. **AI 炒股实验揭示策略行为差异**：价值派 AI 全仓追高光伏股，短线派 AI 分散持仓却意外捕获涨停——说明 Agent 的\"性格\"设定会显著影响决策风格。\n\n5. **工具链适配降低开发门槛**：预装 Node.js、Python 环境的 MTT AIBook + 兼容 Claude Code/Cursor 的 API 接口，使 Skill 开发周期压缩至数小时。\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### OpenClaw 技术架构解析\n\n| 组件 | 功能定位 | 关键特性 |\n|:---|:---|:---|\n| **Core Agent** | 任务规划与执行中枢 | 基于多模态大模型理解屏幕内容，生成 GUI 操作序列（点击、输入、滚动等） |\n| **Skill System** | 外部能力插件 | 通过 Python/TypeScript 编写，暴露函数接口供 Agent 调用 |\n| **Memory & Context** | 状态管理 | 维护会话历史、环境变量、临时文件等上下文信息 |\n| **Notification** | 人机交互通道 | 支持 Telegram Bot 推送交易指令与执行结果 |\n\n> **讲师比喻**：OpenClaw 像一个\"能看懂屏幕的实习生\"——你教它用某个软件（通过 Skill），它就能自己操作；但前提是你要给它\"眼睛\"（看行情）和\"耳朵\"（读新闻）。\n\n### Skill 扩展机制\n\nOpenClaw 的 Skill 本质是**声明式函数接口**，需包含三个要素：\n\n```python\n# Skill 的标准结构示意\n{\n    \"name\": \"get_stock_price\",           # 函数名\n    \"description\": \"获取指定股票实时价格\",  # 自然语言描述（供 Agent 理解用途）\n    \"parameters\": {                       # 参数模式（JSON Schema）\n        \"symbol\": {\"type\": \"string\", \"description\": \"股票代码\"}\n    },\n    \"handler\": \"async def get_stock_price(symbol: str) -> dict: ...\"\n}\n```\n\n**本次实验开发的两个核心 Skill**：\n- `realtime_market_skill`：对接股票行情 API，返回价格、涨跌幅、成交量等字段\n- `finance_news_skill`：爬取财经门户头条，提取情感倾向关键词\n\n### 国产 AI 开发环境：摩尔线程 MTT AIBook\n\n| 规格项 | 配置详情 | 开发友好性分析 |\n|:---|:---|:---|\n| 处理器 | 自研 SOC 集成全功能 GPU | 统一内存架构，CPU/GPU 零拷贝数据传输 |\n| 预装环境 | Ubuntu Linux + Node.js + Python | 省去 CUDA/cuDNN 等驱动折腾，开箱即用 |\n| 目标场景 | AI 开发与边缘部署 | 本地调试 Skill → 云端规模化训练的无缝衔接 |\n\n**关键优势**：相比 NVIDIA Jetson 或 MacBook M 系列，MTT AIBook 的原生 Linux 环境对开源工具链兼容性更佳，Docker、Conda、Poetry 等包管理器无需额外适配。\n\n### 云端算力接入：夸克云 AI Coding Plan\n\n| 属性 | 详情 |\n|:---|:---|\n| 基座模型 | GLM-4（智谱华章开源版本） |\n| 推理硬件 | 摩尔线程 MTT S5000 计算卡 |\n| 工具链兼容 | Claude Code / Cursor / OpenCode 等 |\n| 接入方式 | 标准 OpenAI-compatible API（替换 base_url 与 api_key 即可）|\n\n**技术意义**：打破了\"国产 GPU 只能跑国产模型 demo\"的刻板印象，通过 API 协议层兼容，使开发者能以零迁移成本使用主流 IDE 插件。\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### Skill 开发流程示例\n\n```python\n# realtime_market_skill.py\n# 实时行情获取 Skill —— 供 OpenClaw 调用\n\nimport aiohttp\nfrom typing import Dict, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass StockQuote:\n    symbol: str          # 股票代码\n    price: float         # 最新价\n    change_pct: float    # 涨跌幅 %\n    volume: int          # 成交量\n    \n    def to_prompt_context(self) -> str:\n        \"\"\"转换为 Agent 易读的自然语言格式\"\"\"\n        trend = \"📈\" if self.change_pct > 0 else \"📉\"\n        return (\n            f\"{self.symbol}: 现价 {self.price:.2f}, \"\n            f\"涨跌 {self.change_pct:+.2f}% {trend}, \"\n            f\"成交 {self.volume//10000}万手\"\n        )\n\n\nasync def fetch_realtime_quote(symbol: str) -> Dict[str, Any]:\n    \"\"\"\n    Skill 主入口函数：获取指定股票实时行情\n    \n    Args:\n        symbol: 6位数字股票代码（如 '600519'）\n    \n    Returns:\n        结构化行情数据，异常时返回错误信息字典\n    \"\"\"\n    # 实际生产环境应配置为券商/交易所 Level-2 行情源\n    API_ENDPOINT = \"https://api.example-stock.com/v1/quote\"\n    \n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(\n                API_ENDPOINT, \n                params={\"symbol\": symbol},\n                timeout=aiohttp.ClientTimeout(total=3)  # 实时性要求：3秒超时\n            ) as resp:\n                data = await resp.json()\n                \n                quote = StockQuote(\n                    symbol=data[\"code\"],\n                    price=float(data[\"latest\"]),\n                    change_pct=float(data[\"pct_change\"]),\n                    volume=int(data[\"volume\"])\n                )\n                \n                return {\n                    \"success\": True,\n                    \"data\": quote.to_prompt_context(),\n                    \"raw\": data  # 保留原始字段供 Agent 深度分析\n                }\n                \n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": f\"行情获取失败: {str(e)}\",\n                \"suggestion\": \"请检查网络连接或股票代码有效性\"\n            }\n\n\n# --- Skill 注册配置（config.json）---\nSKILL_MANIFEST = {\n    \"name\": \"realtime_market\",\n    \"version\": \"1.0.0\",\n    \"entry_point\": \"fetch_realtime_quote\",\n    \"description\": \"获取A股实时行情数据，支持价格、涨跌幅、成交量查询\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"symbol\": {\n                \"type\": \"string\",\n                \"pattern\": \"^[0-9]{6}$\",\n                \"description\": \"6位数字股票代码\"\n            }\n        },\n        \"required\": [\"symbol\"]\n    }\n}\n```\n\n### 系统架构图\n\n```\n┌─────────────────────────────────────────┐\n│           User (Telegram Mobile)        │\n│         接收交易指令推送 / 人工干预        │\n└─────────────────┬───────────────────────┘\n                  │ Webhook\n┌─────────────────▼───────────────────────┐\n│         OpenClaw Core Agent             │\n│    ┌─────────────────────────────┐      │\n│    │  Multi-modal LLM (Qwen)     │      │\n│    │  • 屏幕理解 (GUI State)      │      │\n│    │  • 任务规划 (ReAct/CoT)      │      │\n│    │  • 工具调用 (Function Call)  │      │\n│    └─────────────────────────────┘      │\n└─────────────────┬───────────────────────┘\n                  │ 调用 Skill API\n    ┌─────────────┼─────────────┐\n    ▼             ▼             ▼\n┌────────┐   ┌────────┐   ┌──────────┐\n│Realtime │   │Finance │   │ Trading  │\n│Market   │   │ News   │   │ Terminal │\n│Skill    │   │ Skill  │   │ (GUI Op) │\n└────┬───┘   └────┬───┘   └────┬─────┘\n     │            │            │\n     ▼            ▼            ▼\n┌─────────────────────────────────────────┐\n│         外部数据源 / 执行目标              │\n│  • 证券交易所行情 API  • 财经新闻网站      │\n│  • 券商交易终端 (同花顺/通达信等)         │\n└─────────────────────────────────────────┘\n\n═══════════════════════════════════════════\n           底层基础设施栈\n┌─────────────────────────────────────────┐\n│  端侧: 摩尔线程 MTT AIBook (Ubuntu)      │\n│       • Skill 开发调试 • OpenClaw 运行   │\n├─────────────────────────────────────────┤\n│  云端: 夸克云 AI Coding Plan            │\n│       • GLM-4 代码生成 • MTT S5000 推理  │\n└─────────────────────────────────────────┘\n```\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n| 问题类别 | 具体陷阱 | 解决方案 |\n|:---|:---|:---|\n| **语音识别纠错** | \"OpenClaw\" 实为 **OpenManus** 或类似项目？经上下文交叉验证，此处应为 **OpenClaw**（演示者自定义命名或口误）； \"Q文\" → **Qwen（通义千问）**；\"默尔现成\" → **摩尔线程（Moore Threads）**；\"MTTS5000\" → **MTT S5000**；\"夸格云\" → **夸克云**；\"GLAM4.7\" → **GLM-4** | 以行业通用名称为准，括号备注原文 |\n| **合规性风险** | Claude 账号注册在国内存在困难 | 采用国产替代方案：夸克云 AI Coding Plan 提供兼容 API，无需海外手机号/信用卡 |\n| **实时性瓶颈** | 免费行情 API 有延迟（15分钟级） vs 实验描述的\"实时\" | 演示可能使用模拟盘或付费 Level-2 数据；生产环境需申请交易所授权 |\n| **Agent 幻觉** | AI 可能\"虚构\"不存在的股票代码或价格 | Skill 层必须做 schema 校验 + 数据源签名验证，拒绝置信度低的响应 |\n| **资金安全** | 视频明确警告\"股市有风险，请勿模仿\" | 自动化交易需通过券商官方 API（如中泰 XTP、恒生 PTrade），禁止直接操控 GUI 下单 |\n\n**代表性课堂问答还原**：\n\n> **Q**: 为什么价值派 AI 反而激进全仓，短线派却分散持仓？\n> \n> **A**: 这是 Prompt Engineering 的副作用。价值派被设定为\"相信基本面\"，看到光伏板块\"政策利好+趋势突破\"的信号后，基于\"确定性机会要重仓\"的逻辑推导出了全仓决策；短线派的\"狂热\"体现在交易频率而非仓位集中度——它被要求\"捕捉日内波动\"，自然倾向于多标的分散试错。这说明 **Agent 的行为模式高度依赖角色设定的措辞细节**。",
      "createdAt": "2026-02-15T06:51:01.860Z",
      "updatedAt": "2026-02-15T06:51:01.860Z"
    }
  ],
  "drafts": [
    {
      "id": "draft_ad1f1c171f5e",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=32",
      "title": "第二章-面向对象高级-08.抽象类案例_空调案例",
      "contentMd": "## 目录\n\n- [核心概念摘要 (Key Takeaways)](#核心概念摘要-key-takeaways)\n- [知识点详解 (Detailed Notes)](#知识点详解-detailed-notes)\n  - [1. 面向对象三大特性的演进关系](#1-面向对象三大特性的演进关系)\n  - [2. 多态的核心价值：解耦与扩展性](#2-多态的核心价值解耦与扩展性)\n  - [3. 抽象类：制定行业标准的设计模式](#3-抽象类制定行业标准的设计模式)\n- [代码实战 / 算法推导 (Code & Math)](#代码实战--算法推导-code--math)\n  - [空调抽象类案例完整实现](#空调抽象类案例完整实现)\n- [避坑指南 & 常见QA (Tips & QA)](#避坑指南--常见qa-tips--qa)\n\n---\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **多态的本质**：同一接口接收不同对象，产生不同行为效果，实现\"机制不变，内容可变\"的扩展架构\n2. **三大特性演进链**：封装（解决安全）→ 继承（解决复用）→ 多态（解决扩展），层层递进又各有限制\n3. **抽象类的定位**：充当父类制定标准，强制子类实现规范，类比\"国家标准/行业规范\"\n4. **Python多态特性**：弱类型语言，无需严格满足三条件（继承、重写、父类引用指向子类对象）即可实现多态效果，但无法像Java那样精准限定类型\n5. **设计模式启示**：游戏对战平台、USB接口等案例体现\"顶层定规则，底层自由实现\"的工程思想\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 1. 面向对象三大特性的演进关系\n\n| 特性 | 核心解决的问题 | 带来的好处 | 存在的弊端 |\n|:---|:---|:---|:---|\n| **封装** | 数据安全性 | 安全性（私有属性）、复用性（getter/setter） | 代码量增加 |\n| **继承** | 代码重复问题 | 提高代码复用性，子类直接复用父类属性和方法 | 耦合性增强（父类功能强制传递给子类） |\n| **多态** | 系统扩展性问题 | 解耦、可扩展性（平台不变，内容可变） | Python中无法精准限定类型 |\n\n> **讲师比喻**：继承是\"还老爹的东西\"，多态是\"给老爹长脸\"——在不修改原有平台的情况下，让新加入的内容按统一规则运行。\n\n### 2. 多态的核心价值：解耦与扩展性\n\n**王者荣耀案例解析**：\n- 地图机制（刷野、防御塔、兵线）= **固定平台/父类框架**\n- 100+英雄 = **不同的子类实现**\n- 新英雄上线（如\"元流之子\"）= **新增子类，无需改动地图代码**\n\n关键洞察：**机制是一套，至于传过来的是谁，机制不会随内容改变而改变**。这正是软件设计中\"对扩展开放，对修改封闭\"（开闭原则）的体现。\n\n**多态的三条件（理论层面）**：\n1. 存在继承关系\n2. 子类重写父类方法\n3. 父类引用指向子类对象：`Animal a = new Dog()`\n\n> **Python特殊性**：作为弱类型语言，即使不满足上述三条件，也能实现多态效果，但牺牲了类型安全性。\n\n### 3. 抽象类：制定行业标准的设计模式\n\n**核心定义**：\n- **抽象方法**：没有方法体的方法（`pass`占位）\n- **抽象类**：包含抽象方法的类\n- **普通类**：不包含抽象方法的类\n\n**设计哲学类比**：\n\n| 现实场景 | 抽象类角色 | 子类角色 |\n|:---|:---|:---|\n| 汽车排放标准 | 国家制定的\"国六标准\" | 各车企的具体技术方案 |\n| 空调行业规范 | 必须具备制冷/制热/摆风功能 | 格力、小米、美的各自的专利技术 |\n| USB接口协议 | 统一的物理尺寸和数据规范 | 鼠标、键盘、U盘等具体设备 |\n\n**关键约束**：子类必须实现父类**所有**抽象方法，否则报错。这保证了\"符合规范才能上市\"的强制性。\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### 空调抽象类案例完整实现\n\n```python\nfrom abc import ABC, abstractmethod\n\n\n# ==================== 第一步：定义抽象类（制定行业标准）====================\nclass AC(ABC):\n    \"\"\"\n    空调抽象类 - 充当父类，设定行业规范\n    类比：国家规定的空调必须满足的功能标准\n    \"\"\"\n    \n    @abstractmethod\n    def cool_wind(self):\n        \"\"\"制冷功能 - 抽象方法，无具体实现\"\"\"\n        pass\n    \n    @abstractmethod\n    def hot_wind(self):\n        \"\"\"制热功能 - 抽象方法，无具体实现\"\"\"\n        pass\n    \n    @abstractmethod\n    def swing_l_r(self):\n        \"\"\"左右摆风功能 - 抽象方法，无具体实现\"\"\"\n        pass\n\n\n# ==================== 第二步：具体厂商实现（各自的技术方案）====================\nclass XiaoMi(AC):\n    \"\"\"\n    小米空调 - 具体实现类\n    采用\"小冰块\"核心制冷技术、\"静音百合\"摆风技术\n    \"\"\"\n    \n    def cool_wind(self):\n        # 小米特有的制冷技术实现\n        print(\"小米空调：小冰块核心制冷技术启动\")\n    \n    def hot_wind(self):\n        print(\"小米空调：核心制热技术启动\")\n    \n    def swing_l_r(self):\n        # 静音电机摇头方案\n        print(\"小米空调：静音百合左右摆风\")\n\n\nclass Gree(AC):\n    \"\"\"\n    格力空调 - 具体实现类\n    采用\"低频\"相关技术（讲师提及但未详细展开）\n    \"\"\"\n    \n    def cool_wind(self):\n        print(\"格力空调：核心制冷技术启动\")\n    \n    def hot_wind(self):\n        print(\"格力空调：核心制热技术启动\")\n    \n    def swing_l_r(self):\n        print(\"格力空调：左右摆风功能启动\")\n\n\n# ==================== 第三步：测试验证 ====================\ndef test_ac(ac: AC):\n    \"\"\"\n    统一测试接口 - 体现多态性\n    同一函数，传入不同品牌空调，执行各自实现\n    \"\"\"\n    print(f\"\\n{'='*20} 测试 {type(ac).__name__} {'='*20}\")\n    ac.cool_wind()\n    ac.hot_wind()\n    ac.swing_l_r()\n\n\nif __name__ == \"__main__\":\n    # 4.1 测试小米空调\n    xiaomi = XiaoMi()\n    test_ac(xiaomi)\n    \n    # 4.2 测试格力空调\n    gree = Gree()\n    test_ac(gree)\n    \n    # 错误示范：若子类未实现全部抽象方法，实例化时报错\n    # class BadAC(AC):\n    #     def cool_wind(self):\n    #         pass\n    #     # 缺少 hot_wind 和 swing_l_r 的实现\n    # \n    # bad = BadAC()  # TypeError: Can't instantiate abstract class\n```\n\n**输出结果**：\n```\n==================== 测试 XiaoMi ====================\n小米空调：小冰块核心制冷技术启动\n小米空调：核心制热技术启动\n小米空调：静音百合左右摆风\n\n==================== 测试 Gree ====================\n格力空调：核心制冷技术启动\n格力空调：核心制热技术启动\n格力空调：左右摆风功能启动\n```\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n| 问题 | 解答 |\n|:---|:---|\n| **Q：抽象类和普通类的本质区别？** | 抽象类包含抽象方法（无方法体），不能直接实例化；普通类可以实例化。有抽象方法的类必须是抽象类。 |\n| **Q：子类可以只实现部分抽象方法吗？** | ❌ 不可以。Python会强制要求实现**所有**抽象方法，否则会抛出`TypeError`。 |\n| **Q：Python的多态为什么叫\"伪多态\"？** | Python是弱类型语言，不需要显式声明类型，运行时动态绑定。这带来灵活性，但也导致无法像Java那样在编译期精准限定类型（如传入`Car`对象到期望`Animal`的位置不会提前报错）。 |\n| **Q：抽象类 vs 接口的区别？** | 在Python中二者合一（通过`abc`模块实现）。Java/C++中区分明显：抽象类可有成员变量和具体方法，接口早期只能有抽象方法（Java 8+支持默认方法）。 |\n| **易错点：忘记导入`abc`模块** | 需`from abc import ABC, abstractmethod`，并令抽象类继承`ABC`。 |\n| **易错点：抽象方法必须有`@abstractmethod`装饰器** | 仅写`pass`不构成抽象方法，子类不强制重写。 |\n| **设计建议：何时使用抽象类？** | 当需要\"强制规范 + 灵活实现\"的场景：框架设计、插件系统、硬件驱动接口等。 |",
      "lastAutoSavedAt": "2026-02-15T16:48:52.698Z",
      "updatedAt": "2026-02-15T16:48:52.698Z"
    },
    {
      "id": "draft_d9a0e60750b5",
      "sourceUrl": "https://www.bilibili.com/video/BV1MT411x7GH?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=3",
      "title": "Kubernetes 实战进阶、运维管理及 DevOps 课程模块解读",
      "contentMd": "```markdown\n# Kubernetes 实战进阶、运维管理及 DevOps 课程模块解读\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [一、集群搭建方案对比](#一集群搭建方案对比)\n  - [二、Pod 深入理解](#二pod-深入理解)\n  - [三、资源调度核心组件](#三资源调度核心组件)\n  - [四、服务发布与网络](#四服务发布与网络)\n  - [五、存储与配置管理](#五存储与配置管理)\n  - [六、高级调度与认证授权](#六高级调度与认证授权)\n  - [七、运维管理篇](#七运维管理篇)\n  - [八、DevOps 实战项目](#八devops-实战项目)\n- [避坑指南 & 常见QA](#避坑指南--常见qa)\n\n---\n\n## 核心概念摘要\n\n1. **K8s 集群搭建的\"学习悖论\"**：二进制安装最繁琐但最利于理解架构，命令行工具最便捷但隐藏底层细节——建议初学者先难后易。\n2. **Pod 是 K8s 的最小调度单元**，探针（Probe）和生命周期钩子是其高可用设计的核心机制。\n3. **Label + Selector 是 K8s 的\"导航系统\"**，Deployment/StatefulSet/DaemonSet 分别对应无状态、有状态、守护进程三类工作负载。\n4. **Service + Ingress 解决了容器网络的两大难题**：集群内部通信与外部流量入口。\n5. **Helm 是 K8s 的包管理器**，类比 Maven/NPM/YUM，实现应用的快速部署与版本管理。\n\n---\n\n## 知识点详解\n\n### 一、集群搭建方案对比\n\n| 方案 | 特点 | 适用场景 | 生产环境建议 |\n|:---|:---|:---|:---|\n| **二进制安装** | 手动编译组件，深度理解架构 | 学习、定制化需求 | ⭐⭐⭐ 推荐（熟悉后） |\n| **kubeadm** | 官方半自动化工具 | 快速搭建测试环境 | ⭐⭐☆ 谨慎使用 |\n| **命令行工具**（如 kind/minikube） | 一键安装，极简操作 | 本地开发、CI/CD | ⭐☆☆ 不推荐生产 |\n| **云厂商托管**（ACK/EKS/GKE） | 全托管，免运维 | 企业生产环境 | ⭐⭐⭐ 强烈推荐 |\n\n> **讲师观点**：二进制安装的\"麻烦\"恰恰是价值所在——亲手经历 etcd、apiserver、scheduler、controller-manager 的部署过程，才能建立对 K8s 架构的系统性认知。遇到问题时，你能直接定位到具体组件而非盲目排查。\n\n**API 与 kubectl 的关系**：\n```\n用户命令 (kubectl) → REST API 调用 → etcd 状态变更 → 控制器协调 → 实际执行\n```\n\n---\n\n### 二、Pod 深入理解\n\n#### 2.1 探针（Probe）—— 容器的\"健康监护仪\"\n\n| 探针类型 | 作用时机 | 典型用途 |\n|:---|:---|:---|\n| `LivenessProbe` | 容器运行期间持续检测 | 应用卡死时自动重启容器 |\n| `ReadinessProbe` | 容器启动后检测 | 确保流量只打到就绪实例 |\n| `StartupProbe` | 容器启动初期检测 | 保护慢启动应用不被误杀 |\n\n**核心价值**：实现故障自愈——服务异常时自动重启，无需人工介入。\n\n#### 2.2 生命周期钩子（Lifecycle Hooks）\n\n```yaml\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n    - name: app\n      lifecycle:\n        postStart:      # 容器创建后立即执行\n          exec:\n            command: [\"/bin/sh\", \"-c\", \"echo 'Init done' >> /var/log/app.log\"]\n        preStop:        # 容器终止前执行（优雅停机）\n          exec:\n            command: [\"/bin/sh\", \"-c\", \"sleep 10 && kill -SIGTERM 1\"]\n```\n\n**应用场景**：\n- `postStart`：预热缓存、注册服务发现\n- `preStop`：完成当前请求、注销服务、释放连接池\n\n---\n\n### 三、资源调度核心组件\n\n#### 3.1 Label 与 Selector —— \"标签-选择器\"机制\n\n类比：给行李箱贴标签（Label），通过标签快速筛选（Selector）。\n\n```yaml\n# 打标签\nmetadata:\n  labels:\n    app: nginx\n    tier: frontend\n    env: production\n\n# 选择器匹配\nselector:\n  matchLabels:\n    app: nginx\n    env: production\n```\n\n#### 3.2 工作负载控制器对比\n\n| 控制器 | 适用场景 | 关键特性 |\n|:---|:---|:---|\n| **Deployment** | 无状态应用（Web服务、API） | 滚动更新、回滚、扩缩容、暂停/恢复 |\n| **StatefulSet** | 有状态应用（MySQL、Redis、Kafka） | 稳定网络标识、有序部署、持久化存储绑定 |\n| **DaemonSet** | 节点级守护进程（日志采集、监控Agent） | 每节点一个副本，随节点自动扩缩 |\n| **HPA** | 自动弹性伸缩 | 基于 CPU/内存/自定义指标自动调整副本数 |\n\n> **有状态 vs 无状态**：MySQL 数据不能丢、需要固定身份（Pod 名、网络标识）→ StatefulSet；Nginx 配置相同、随时可替换 → Deployment。\n\n---\n\n### 四、服务发布与网络\n\n#### 4.1 两大核心技术解决网络问题\n\n| 层级 | 组件 | 解决问题 |\n|:---|:---|:---|\n| **集群内部** | Service（ClusterIP/NodePort/LoadBalancer） | Pod IP 动态变化时的服务发现与负载均衡 |\n| **集群外部** | Ingress | HTTP/HTTPS 路由、域名解析、SSL 终止、限流熔断 |\n\n**演进路径**：\n```\nDocker 时代：容器间通信困难 → 端口映射混乱\n    ↓\nK8s Service：抽象出稳定的虚拟 IP（ClusterIP），自动后端端点维护\n    ↓\nIngress：七层路由，类似 Nginx 的配置即代码\n```\n\n---\n\n### 五、存储与配置管理\n\n#### 5.1 ConfigMap / Secret —— 动态配置注入\n\n痛点解决：修改配置文件无需重新构建镜像、滚动重启 Pod。\n\n```yaml\n# 配置与镜像解耦\nenvFrom:\n  - configMapRef:\n      name: app-config    # 热更新需配合 volume 挂载\nvolumeMounts:\n  - name: config-vol\n    mountPath: /etc/config\nvolumes:\n  - name: config-vol\n    configMap:\n      name: app-config    # 文件变更后自动同步（约 ~1min 延迟）\n```\n\n#### 5.2 持久化存储（PV/PVC/StorageClass）\n\n**核心链条**：\n```\n应用声明需求 (PVC) → 动态供应 (StorageClass + Provisioner) → 后端存储 (NFS/Ceph/云盘)\n         ↑___________________________________________________________|\n                              回收策略控制\n```\n\n**关键原则**：有状态应用的数据必须持久化，MySQL 数据丢失 = 服务不可用。\n\n---\n\n### 六、高级调度与认证授权\n\n#### 6.1 高级调度特性\n\n- **亲和性/反亲和性**：将 Pod 调度到指定节点或分散部署\n- **污点与容忍**：专用节点隔离（如 GPU 节点、SSD 节点）\n- **优先级与抢占**：资源紧张时保障核心服务\n\n**规模化运维价值**：数百台节点的应用部署，一条命令完成全局配置。\n\n#### 6.2 RBAC 认证授权\n\n| 概念 | 说明 |\n|:---|:---|\n| Role/ClusterRole | 定义权限规则（对哪些资源做什么操作） |\n| RoleBinding/ClusterRoleBinding | 将权限绑定到用户/组/ServiceAccount |\n| ServiceAccount | Pod 访问 API Server 的身份凭证 |\n\n> 控制台界面操作时必须理解 RBAC，否则易出现权限泄露或功能受限。\n\n---\n\n### 七、运维管理篇\n\n| 模块 | 工具/方案 | 核心能力 |\n|:---|:---|:---|\n| **包管理器** | Helm | Chart 打包、版本管理、依赖解析、一键安装/升级/回滚 |\n| **集群监控** | Prometheus + Grafana | 多维度指标采集、告警规则、可视化大盘 |\n| **日志管理** | EFK/Loki | 统一收集、检索、分析分布式日志 |\n| **可视化界面** | Dashboard/Rancher/KubeSphere | Web UI 替代命令行，降低运维门槛 |\n\n**学习路径建议**：先精通 kubectl 命令，再使用控制台——\"先难后易\"才能真正理解 K8s 在做什么。\n\n---\n\n### 八、DevOps 实战项目\n\n基于阿里云 ACK 的微服务完整流水线：\n\n```\n代码提交 (GitLab) \n    ↓\n代码质量检查 (SonarQube) —— 漏洞扫描、覆盖率、规范审查\n    ↓\n镜像构建与推送 (Docker Registry)\n    ↓\n流水线编排 (Jenkins/GitLab CI) —— 自动化测试、环境部署\n    ↓\nK8s 集群部署 (Helm/原生 YAML)\n    ↓\n服务网格治理 (可选：Istio)\n```\n\n**涉及技术栈**：Docker、GitLab、SonarQube、Jenkins、Helm、ACK。\n\n---\n\n## 避坑指南 & 常见QA\n\n| 问题 | 解答/建议 |\n|:---|:---|\n| **kubeadm 能否用于生产？** | 官方不建议。可能存在稳定性隐患，小规模可用，大规模推荐二进制或云托管。 |\n| **为什么先学命令行而不是直接用 Dashboard？** | 控制台隐藏了底层逻辑，遇到问题无法排查。\"先难后易\"才能建立真正的系统认知。 |\n| **StatefulSet 的 Pod 能随便删除吗？** | 可以，但会按序号重建。需注意：① 持久卷不会删除 ② 网络标识保持不变。 |\n| **ConfigMap 修改后为何不立即生效？** | 环境变量方式注入的不会热更新；Volume 挂载方式有约 1 分钟延迟。 |\n| **HPA 扩容不及时怎么办？** | 考虑自定义指标（如 QPS）、降低采样周期、或配合 Cluster Autoscaler 扩节点。 |\n| **Helm 与直接 apply YAML 的区别？** | Helm 提供版本管理、参数化模板、依赖管理，适合复杂应用的生命周期管理。 |\n```",
      "lastAutoSavedAt": "2026-02-15T16:21:11.116Z",
      "updatedAt": "2026-02-15T16:21:11.116Z"
    },
    {
      "id": "draft_fd8cf9e1b295",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html",
      "title": "4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation",
      "contentMd": "# 前向传播、反向传播和计算图\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [前向传播](#前向传播)\n  - [前向传播计算图](#前向传播计算图)\n  - [反向传播](#反向传播)\n  - [训练神经网络](#训练神经网络)\n- [代码与示例](#代码与示例)\n- [常见问题与避坑](#常见问题与避坑)\n- [复习清单](#复习清单)\n\n---\n\n## 核心概念摘要\n\n| 概念 | 定义 |\n|:---|:---|\n| **前向传播 (Forward Propagation)** | 按顺序从输入层到输出层计算并存储神经网络每层结果的计算过程 |\n| **反向传播 (Backpropagation)** | 利用链式法则，按相反顺序（从输出层到输入层）计算参数梯度的方法 |\n| **计算图 (Computational Graph)** | 可视化计算中操作符和变量依赖关系的有向无环图 |\n| **目标函数 $J$** | 损失项 $L$ 与正则化项 $s$ 之和：$J = L + s$ |\n\n---\n\n## 知识点详解\n\n### 前向传播\n\n#### 网络结构设定\n以**单隐藏层多层感知机**为例（带 $L_2$ 正则化，无偏置项）：\n\n| 符号 | 含义 | 维度 |\n|:---|:---|:---|\n| $\\mathbf{x}$ | 输入样本 | $\\mathbb{R}^d$ |\n| $\\mathbf{W}^{(1)}$ | 隐藏层权重 | $\\mathbb{R}^{h \\times d}$ |\n| $\\mathbf{z}$ | 隐藏层线性变换结果 | $\\mathbb{R}^h$ |\n| $\\mathbf{h}$ | 隐藏层激活值 | $\\mathbb{R}^h$ |\n| $\\mathbf{W}^{(2)}$ | 输出层权重 | $\\mathbb{R}^{q \\times h}$ |\n| $\\mathbf{o}$ | 输出层变量 | $\\mathbb{R}^q$ |\n| $y$ | 样本标签 | 标量或向量 |\n| $\\lambda$ | 正则化超参数 | 标量 |\n\n#### 前向传播计算步骤\n\n**Step 1: 隐藏层线性变换**\n$$\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x} \\tag{4.7.1}$$\n\n**Step 2: 激活函数**\n$$\\mathbf{h} = \\phi(\\mathbf{z}) \\tag{4.7.2}$$\n\n**Step 3: 输出层计算**\n$$\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h} \\tag{4.7.3}$$\n\n**Step 4: 损失计算**\n$$L = l(\\mathbf{o}, y) \\tag{4.7.4}$$\n\n**Step 5: 正则化项**（Frobenius范数）\n$$s = \\frac{\\lambda}{2}\\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right) \\tag{4.7.5}$$\n\n> **Frobenius范数**：将矩阵展平为向量后应用的 $L_2$ 范数，即 $\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$\n\n**Step 6: 目标函数**\n$$J = L + s \\tag{4.7.6}$$\n\n---\n\n### 前向传播计算图\n\n```\n        x ──→[W⁽¹⁾]──→ z ──→[ϕ]──→ h ──→[W⁽²⁾]──→ o ──→[l]──→ L ──┐\n         │      ↑       │       ↑       │       ↑                  │\n         │      │       │       │       │       │                  ├──→[+ ]──→ J\n         └──────┘       └───────┘       └───────┘                  │\n                              W⁽¹⁾,W⁽²⁾ ──→[‖·‖²_F]──→ s ─────────┘\n                                               ↑\n                                               λ/2\n```\n\n**图例说明**：\n- 正方形 □：变量（张量）\n- 圆形 ○：操作符（运算）\n- 箭头 →：数据流向（主要向右、向上）\n\n---\n\n### 反向传播\n\n#### 核心原理：链式法则\n\n对于复合函数 $Z = g(Y)$ 且 $Y = f(X)$：\n$$\\frac{\\partial Z}{\\partial X} = \\text{prod}\\left(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial Y}{\\partial X}\\right) \\tag{4.7.7}$$\n\n> `prod` 运算符：执行必要的转置和交换位置后相乘。对向量是矩阵乘法，对高维张量使用适当的对应操作。\n\n#### 反向传播梯度计算（按逆序）\n\n| 步骤 | 梯度 | 公式 | 结果维度 |\n|:---|:---|:---|:---|\n| 1 | $\\frac{\\partial J}{\\partial L}, \\frac{\\partial J}{\\partial s}$ | $= 1$ | 标量 |\n| 2 | $\\frac{\\partial J}{\\partial \\mathbf{o}}$ | $\\displaystyle\\frac{\\partial L}{\\partial \\mathbf{o}}$ | $\\mathbb{R}^q$ |\n| 3 | $\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}$ | $= \\lambda \\mathbf{W}^{(1)}, \\lambda \\mathbf{W}^{(2)}$ | 同权重维度 |\n| 4 | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}$ | $\\displaystyle\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$ | $\\mathbb{R}^{q \\times h}$ |\n| 5 | $\\frac{\\partial J}{\\partial \\mathbf{h}}$ | $\\mathbf{W}^{(2)\\top} \\displaystyle\\frac{\\partial J}{\\partial \\mathbf{o}}$ | $\\mathbb{R}^h$ |\n| 6 | $\\frac{\\partial J}{\\partial \\mathbf{z}}$ | $\\displaystyle\\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$ | $\\mathbb{R}^h$ |\n| 7 | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}$ | $\\displaystyle\\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}$ | $\\mathbb{R}^{h \\times d}$ |\n\n**关键公式详解**：\n\n**输出层权重梯度**（公式 4.7.11）：\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\underbrace{\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top}_{\\text{来自损失}} + \\underbrace{\\lambda \\mathbf{W}^{(2)}}_{\\text{来自正则化}}$$\n\n**隐藏层梯度回传**（公式 4.7.12-4.7.14）：\n$$\\frac{\\partial J}{\\partial \\mathbf{h}} = \\mathbf{W}^{(2)\\top} \\frac{\\partial J}{\\partial \\mathbf{o}}$$\n\n$$\\frac{\\partial J}{\\partial \\mathbf{z}} = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z}) \\quad \\text{（Hadamard积，逐元素乘法）}$$\n\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}$$\n\n---\n\n### 训练神经网络\n\n#### 前向传播与反向传播的依赖关系\n\n```\n迭代 t:\n    ┌─────────────────┐\n    │  前向传播        │ ← 使用 W⁽¹⁾ₜ, W⁽²⁾ₜ（来自上次反向传播更新）\n    │  计算 J, L, s, h │ → 保存中间值 h, z 等\n    └────────┬────────┘\n             ▼\n    ┌─────────────────┐\n    │  反向传播        │ ← 使用保存的中间值 h, z\n    │  计算 ∂J/∂W⁽¹⁾, ∂J/∂W⁽²⁾ │\n    └────────┬────────┘\n             ▼\n    ┌─────────────────┐\n    │  参数更新        │ → W⁽¹⁾ₜ₊₁ = W⁽¹⁾ₜ - η·∂J/∂W⁽¹⁾\n    │  （优化器）      │    W⁽²⁾ₜ₊₁ = W⁽²⁾ₜ - η·∂J/∂W⁽²⁾\n    └─────────────────┘\n```\n\n#### 内存分析\n\n| 阶段 | 内存需求 | 原因 |\n|:---|:---|:---|\n| **预测 (Inference)** | 低 | 只需当前层激活值，无需保存中间结果 |\n| **训练 (Training)** | 高 | 需保存所有中间变量直到反向传播完成 |\n\n**显存占用因素**：\n- 网络层数量 ↑ → 中间变量增多 → 显存↑\n- 批量大小 (batch size) ↑ → 激活值张量增大 → 显存↑\n\n> ⚠️ 深层网络 + 大批量 → 容易导致 **OOM (Out of Memory)** 错误\n\n---\n\n## 代码与示例\n\n### 简化的前向/反向传播实现框架\n\n```python\nimport torch\nfrom torch import nn\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, lambda_reg=0.01):\n        super().__init__()\n        self.W1 = nn.Parameter(torch.randn(hidden_dim, input_dim) * 0.01)\n        self.W2 = nn.Parameter(torch.randn(output_dim, hidden_dim) * 0.01)\n        self.lambda_reg = lambda_reg\n        \n    def forward(self, x):\n        \"\"\"\n        前向传播：保存中间值供反向传播使用\n        \"\"\"\n        # x: (batch_size, input_dim)\n        self.z = torch.matmul(x, self.W1.t())   # (batch, hidden)\n        self.h = torch.relu(self.z)              # 激活函数 ϕ\n        self.o = torch.matmul(self.h, self.W2.t())  # (batch, output)\n        \n        # 保存输入用于反向传播\n        self.x_saved = x.clone()\n        return self.o\n    \n    def compute_loss(self, o, y):\n        \"\"\"\n        计算目标函数 J = L + s\n        \"\"\"\n        # 损失项 L（以MSE为例）\n        L = torch.mean((o - y) ** 2)\n        \n        # L2正则化项 s\n        s = 0.5 * self.lambda_reg * (\n            torch.sum(self.W1 ** 2) + \n            torch.sum(self.W2 ** 2)\n        )\n        \n        self.L = L\n        self.s = s\n        return L + s  # J\n    \n    def backward_manual(self, y):\n        \"\"\"\n        手动反向传播（演示用，实际使用autograd）\n        \"\"\"\n        batch_size = y.shape[0]\n        \n        # Step 1: dJ/dL = 1, dJ/ds = 1\n        # Step 2: dJ/do = dL/do = 2*(o-y)/batch_size\n        dJ_do = 2 * (self.o - y) / batch_size  # (batch, output)\n        \n        # Step 4: dJ/dW2 = dJ/do · h^T + λ*W2\n        dJ_dW2 = torch.matmul(dJ_do.t(), self.h) + self.lambda_reg * self.W2\n        \n        # Step 5: dJ/dh = W2^T · dJ/do\n        dJ_dh = torch.matmul(dJ_do, self.W2)   # (batch, hidden)\n        \n        # Step 6: dJ/dz = dJ/dh ⊙ relu'(z)\n        dJ_dz = dJ_dh * (self.z > 0).float()   # ReLU导数\n        \n        # Step 7: dJ/dW1 = dJ/dz · x^T + λ*W1\n        dJ_dW1 = torch.matmul(dJ_dz.t(), self.x_saved) + self.lambda_reg * self.W1\n        \n        return {'W1': dJ_dW1, 'W2': dJ_dW2}\n\n\n# ============ 使用示例 ============\ntorch.manual_seed(42)\n\n# 数据\nbatch_size, input_dim, hidden_dim, output_dim = 64, 784, 256, 10\nx = torch.randn(batch_size, input_dim)\ny = torch.randn(batch_size, output_dim)\n\n# 模型\nmodel = SimpleMLP(input_dim, hidden_dim, output_dim, lambda_reg=0.01)\n\n# 前向传播\no = model.forward(x)\nJ = model.compute_loss(o, y)\nprint(f\"Loss L: {model.L.item():.4f}, Reg s: {model.s.item():.4f}, Total J: {J.item():.4f}\")\n\n# 手动反向传播\ngrads = model.backward_manual(y)\nprint(f\"Grad W1 shape: {grads['W1'].shape}\")  # torch.Size([256, 784])\nprint(f\"Grad W2 shape: {grads['W2'].shape}\")  # torch.Size([10, 256])\n\n# 对比PyTorch autograd（验证正确性）\nJ.backward()\nprint(f\"\\nAutograd W2 grad match: {torch.allclose(model.W2.grad, grads['W2'])}\")\n```\n\n### 关键代码解释\n\n| 代码段 | 对应数学公式 | 说明 |\n|:---|:---|:---|\n| `self.z = torch.matmul(x, self.W1.t())` | $\\mathbf{z} = \\mathbf{W}^{(1)}\\mathbf{x}$ | 注意PyTorch是行优先，需转置 |\n| `self.h = torch.relu(self.z)` | $\\mathbf{h} = \\phi(\\mathbf{z})$ | 激活函数 |\n| `dJ_dz = dJ_dh * (self.z > 0).float()` | $\\frac{\\partial J}{\\partial \\mathbf{z}} = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$ | ReLU的导数是示性函数 |\n| `torch.matmul(dJ_do.t(), self.h)` | $\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top$ | 外积得梯度矩阵 |\n\n---\n\n## 常见问题与避坑\n\n| 问题 | 原因 | 解决方案 |\n|:---|:---|:---|\n| **显存溢出 (OOM)** | 批量太大或网络太深，中间值占用过多 | 减小batch size；使用梯度累积；混合精度训练 |\n| **梯度消失/爆炸** | 深层网络链式法则连乘效应 | 使用BatchNorm；残差连接；合适的初始化 |\n| **忘记detach()导致内存泄漏** | 计算图一直保留 | 推理时用 `with torch.no_grad()` 或 `.detach()` |\n| **正则化项未加入梯度** | 只优化损失，忽略权重衰减 | 确保 $\\lambda \\mathbf{W}$ 项加入梯度计算 |\n| **激活函数导数错误** | 如ReLU在0处不可导 | 实践中统一取0或1，不影响效果 |\n\n---\n\n## 复习清单\n\n- [ ] 能独立写出单隐藏层MLP的前向传播公式（4.7.1-4.7.6）\n- [ ] 理解Frobenius范数与L2正则化的关系\n- [ ] 能画出简单网络的计算图，标明数据流向\n- [ ] 掌握链式法则在张量上的应用（prod运算符的含义）\n- [ ] 能推导输出层权重梯度公式（4.7.11）的两项来源\n- [ ] 理解为什么需要逐元素乘法（Hadamard积）计算$\\frac{\\partial J}{\\partial \\mathbf{z}}$\n- [ ] 清楚训练比预测耗内存的根本原因（中间值保存）\n- [ ] 知道影响显存占用的两个主要因素：网络深度和批量大小",
      "lastAutoSavedAt": "2026-02-15T06:55:01.371Z",
      "updatedAt": "2026-02-15T06:55:01.371Z"
    }
  ],
  "tasks": [
    {
      "id": "task_d465ec9dcd40",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html",
      "sourceType": "web",
      "promptId": "prompt-mll-template-detailed",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T16:56:14.271Z",
      "updatedAt": "2026-02-15T16:57:52.615Z",
      "preparedMd": "# 网页抓取内容汇总\n\n## 网页来源 1\n\n原始链接：https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html\n网页标题：4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation\n\n4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation\n===============\n\n__\n\n[4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)_navigate\\_next_[4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n\n_search_ \n\n Quick search \n\n[_code_](https://zh.d2l.ai/_sources/chapter_multilayer-perceptrons/backprop.rst.txt)\n\n Show Source \n\n[MXNet](https://zh-v2.d2l.ai/d2l-zh.pdf)[PyTorch](https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf)[Jupyter 记事本](https://zh-v2.d2l.ai/d2l-zh.zip)[课程](https://courses.d2l.ai/zh-v2/)[GitHub](https://github.com/d2l-ai/d2l-zh)[English](https://d2l.ai/)\n\n[![Image 2: 动手学深度学习](https://zh.d2l.ai/_static/logo-with-text.png)](https://zh.d2l.ai/index.html)\n\nTable Of Contents\n*   [前言](https://zh.d2l.ai/chapter_preface/index.html)\n*   [安装](https://zh.d2l.ai/chapter_installation/index.html)\n*   [符号](https://zh.d2l.ai/chapter_notation/index.html)\n\n*   [1. 引言](https://zh.d2l.ai/chapter_introduction/index.html)\n*   [2. 预备知识](https://zh.d2l.ai/chapter_preliminaries/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [2.1. 数据操作](https://zh.d2l.ai/chapter_preliminaries/ndarray.html)\n    *   [2.2. 数据预处理](https://zh.d2l.ai/chapter_preliminaries/pandas.html)\n    *   [2.3. 线性代数](https://zh.d2l.ai/chapter_preliminaries/linear-algebra.html)\n    *   [2.4. 微积分](https://zh.d2l.ai/chapter_preliminaries/calculus.html)\n    *   [2.5. 自动微分](https://zh.d2l.ai/chapter_preliminaries/autograd.html)\n    *   [2.6. 概率](https://zh.d2l.ai/chapter_preliminaries/probability.html)\n    *   [2.7. 查阅文档](https://zh.d2l.ai/chapter_preliminaries/lookup-api.html)\n\n*   [3. 线性神经网络](https://zh.d2l.ai/chapter_linear-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [3.1. 线性回归](https://zh.d2l.ai/chapter_linear-networks/linear-regression.html)\n    *   [3.2. 线性回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)\n    *   [3.3. 线性回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html)\n    *   [3.4. softmax回归](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html)\n    *   [3.5. 图像分类数据集](https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html)\n    *   [3.6. softmax回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)\n    *   [3.7. softmax回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html)\n\n*   [4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [4.1. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html)\n    *   [4.2. 多层感知机的从零开始实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html)\n    *   [4.3. 多层感知机的简洁实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html)\n    *   [4.4. 模型选择、欠拟合和过拟合](https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html)\n    *   [4.5. 权重衰减](https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html)\n    *   [4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)\n    *   [4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)\n    *   [4.9. 环境和分布偏移](https://zh.d2l.ai/chapter_multilayer-perceptrons/environment.html)\n    *   [4.10. 实战Kaggle比赛：预测房价](https://zh.d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html)\n\n*   [5. 深度学习计算](https://zh.d2l.ai/chapter_deep-learning-computation/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [5.1. 层和块](https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html)\n    *   [5.2. 参数管理](https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html)\n    *   [5.3. 延后初始化](https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html)\n    *   [5.4. 自定义层](https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html)\n    *   [5.5. 读写文件](https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html)\n    *   [5.6. GPU](https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html)\n\n*   [6. 卷积神经网络](https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [6.1. 从全连接层到卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html)\n    *   [6.2. 图像卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)\n    *   [6.3. 填充和步幅](https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)\n    *   [6.4. 多输入多输出通道](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)\n    *   [6.5. 汇聚层](https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html)\n    *   [6.6. 卷积神经网络（LeNet）](https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html)\n\n*   [7. 现代卷积神经网络](https://zh.d2l.ai/chapter_convolutional-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [7.1. 深度卷积神经网络（AlexNet）](https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html)\n    *   [7.2. 使用块的网络（VGG）](https://zh.d2l.ai/chapter_convolutional-modern/vgg.html)\n    *   [7.3. 网络中的网络（NiN）](https://zh.d2l.ai/chapter_convolutional-modern/nin.html)\n    *   [7.4. 含并行连结的网络（GoogLeNet）](https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html)\n    *   [7.5. 批量规范化](https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html)\n    *   [7.6. 残差网络（ResNet）](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html)\n    *   [7.7. 稠密连接网络（DenseNet）](https://zh.d2l.ai/chapter_convolutional-modern/densenet.html)\n\n*   [8. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [8.1. 序列模型](https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html)\n    *   [8.2. 文本预处理](https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)\n    *   [8.3. 语言模型和数据集](https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)\n    *   [8.4. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n    *   [8.5. 循环神经网络的从零开始实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)\n    *   [8.6. 循环神经网络的简洁实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html)\n    *   [8.7. 通过时间反向传播](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)\n\n*   [9. 现代循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [9.1. 门控循环单元（GRU）](https://zh.d2l.ai/chapter_recurrent-modern/gru.html)\n    *   [9.2. 长短期记忆网络（LSTM）](https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)\n    *   [9.3. 深度循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/deep-rnn.html)\n    *   [9.4. 双向循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/bi-rnn.html)\n    *   [9.5. 机器翻译与数据集](https://zh.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html)\n    *   [9.6. 编码器-解码器架构](https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html)\n    *   [9.7. 序列到序列学习（seq2seq）](https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html)\n    *   [9.8. 束搜索](https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html)\n\n*   [10. 注意力机制](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [10.1. 注意力提示](https://zh.d2l.ai/chapter_attention-mechanisms/attention-cues.html)\n    *   [10.2. 注意力汇聚：Nadaraya-Watson 核回归](https://zh.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)\n    *   [10.3. 注意力评分函数](https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)\n    *   [10.4. Bahdanau 注意力](https://zh.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)\n    *   [10.5. 多头注意力](https://zh.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)\n    *   [10.6. 自注意力和位置编码](https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)\n    *   [10.7. Transformer](https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html)\n\n*   [11. 优化算法](https://zh.d2l.ai/chapter_optimization/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [11.1. 优化和深度学习](https://zh.d2l.ai/chapter_optimization/optimization-intro.html)\n    *   [11.2. 凸性](https://zh.d2l.ai/chapter_optimization/convexity.html)\n    *   [11.3. 梯度下降](https://zh.d2l.ai/chapter_optimization/gd.html)\n    *   [11.4. 随机梯度下降](https://zh.d2l.ai/chapter_optimization/sgd.html)\n    *   [11.5. 小批量随机梯度下降](https://zh.d2l.ai/chapter_optimization/minibatch-sgd.html)\n    *   [11.6. 动量法](https://zh.d2l.ai/chapter_optimization/momentum.html)\n    *   [11.7. AdaGrad算法](https://zh.d2l.ai/chapter_optimization/adagrad.html)\n    *   [11.8. RMSProp算法](https://zh.d2l.ai/chapter_optimization/rmsprop.html)\n    *   [11.9. Adadelta](https://zh.d2l.ai/chapter_optimization/adadelta.html)\n    *   [11.10. Adam算法](https://zh.d2l.ai/chapter_optimization/adam.html)\n    *   [11.11. 学习率调度器](https://zh.d2l.ai/chapter_optimization/lr-scheduler.html)\n\n*   [12. 计算性能](https://zh.d2l.ai/chapter_computational-performance/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [12.1. 编译器和解释器](https://zh.d2l.ai/chapter_computational-performance/hybridize.html)\n    *   [12.2. 异步计算](https://zh.d2l.ai/chapter_computational-performance/async-computation.html)\n    *   [12.3. 自动并行](https://zh.d2l.ai/chapter_computational-performance/auto-parallelism.html)\n    *   [12.4. 硬件](https://zh.d2l.ai/chapter_computational-performance/hardware.html)\n    *   [12.5. 多GPU训练](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus.html)\n    *   [12.6. 多GPU的简洁实现](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus-concise.html)\n    *   [12.7. 参数服务器](https://zh.d2l.ai/chapter_computational-performance/parameterserver.html)\n\n*   [13. 计算机视觉](https://zh.d2l.ai/chapter_computer-vision/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [13.1. 图像增广](https://zh.d2l.ai/chapter_computer-vision/image-augmentation.html)\n    *   [13.2. 微调](https://zh.d2l.ai/chapter_computer-vision/fine-tuning.html)\n    *   [13.3. 目标检测和边界框](https://zh.d2l.ai/chapter_computer-vision/bounding-box.html)\n    *   [13.4. 锚框](https://zh.d2l.ai/chapter_computer-vision/anchor.html)\n    *   [13.5. 多尺度目标检测](https://zh.d2l.ai/chapter_computer-vision/multiscale-object-detection.html)\n    *   [13.6. 目标检测数据集](https://zh.d2l.ai/chapter_computer-vision/object-detection-dataset.html)\n    *   [13.7. 单发多框检测（SSD）](https://zh.d2l.ai/chapter_computer-vision/ssd.html)\n    *   [13.8. 区域卷积神经网络（R-CNN）系列](https://zh.d2l.ai/chapter_computer-vision/rcnn.html)\n    *   [13.9. 语义分割和数据集](https://zh.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html)\n    *   [13.10. 转置卷积](https://zh.d2l.ai/chapter_computer-vision/transposed-conv.html)\n    *   [13.11. 全卷积网络](https://zh.d2l.ai/chapter_computer-vision/fcn.html)\n    *   [13.12. 风格迁移](https://zh.d2l.ai/chapter_computer-vision/neural-style.html)\n    *   [13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)](https://zh.d2l.ai/chapter_computer-vision/kaggle-cifar10.html)\n    *   [13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）](https://zh.d2l.ai/chapter_computer-vision/kaggle-dog.html)\n\n*   [14. 自然语言处理：预训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [14.1. 词嵌入（word2vec）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)\n    *   [14.2. 近似训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html)\n    *   [14.3. 用于预训练词嵌入的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)\n    *   [14.4. 预训练word2vec](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html)\n    *   [14.5. 全局向量的词嵌入（GloVe）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/glove.html)\n    *   [14.6. 子词嵌入](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html)\n    *   [14.7. 词的相似性和类比任务](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)\n    *   [14.8. 来自Transformers的双向编码器表示（BERT）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert.html)\n    *   [14.9. 用于预训练BERT的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html)\n    *   [14.10. 预训练BERT](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html)\n\n*   [15. 自然语言处理：应用](https://zh.d2l.ai/chapter_natural-language-processing-applications/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [15.1. 情感分析及数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html)\n    *   [15.2. 情感分析：使用循环神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-rnn.html)\n    *   [15.3. 情感分析：使用卷积神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html)\n    *   [15.4. 自然语言推断与数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html)\n    *   [15.5. 自然语言推断：使用注意力](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-attention.html)\n    *   [15.6. 针对序列级和词元级应用微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html)\n    *   [15.7. 自然语言推断：微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)\n\n*   [16. 附录：深度学习工具](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [16.1. 使用Jupyter Notebook](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html)\n    *   [16.2. 使用Amazon SageMaker](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/sagemaker.html)\n    *   [16.3. 使用Amazon EC2实例](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/aws.html)\n    *   [16.4. 选择服务器和GPU](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html)\n    *   [16.5. 为本书做贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html)\n    *   [16.6. `d2l` API 文档](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/d2l.html)\n\n*   [参考文献](https://zh.d2l.ai/chapter_references/zreferences.html)\n\n[![Image 3: 动手学深度学习](https://zh.d2l.ai/_static/logo-with-text.png)](https://zh.d2l.ai/index.html)\n\nTable Of Contents\n*   [前言](https://zh.d2l.ai/chapter_preface/index.html)\n*   [安装](https://zh.d2l.ai/chapter_installation/index.html)\n*   [符号](https://zh.d2l.ai/chapter_notation/index.html)\n\n*   [1. 引言](https://zh.d2l.ai/chapter_introduction/index.html)\n*   [2. 预备知识](https://zh.d2l.ai/chapter_preliminaries/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [2.1. 数据操作](https://zh.d2l.ai/chapter_preliminaries/ndarray.html)\n    *   [2.2. 数据预处理](https://zh.d2l.ai/chapter_preliminaries/pandas.html)\n    *   [2.3. 线性代数](https://zh.d2l.ai/chapter_preliminaries/linear-algebra.html)\n    *   [2.4. 微积分](https://zh.d2l.ai/chapter_preliminaries/calculus.html)\n    *   [2.5. 自动微分](https://zh.d2l.ai/chapter_preliminaries/autograd.html)\n    *   [2.6. 概率](https://zh.d2l.ai/chapter_preliminaries/probability.html)\n    *   [2.7. 查阅文档](https://zh.d2l.ai/chapter_preliminaries/lookup-api.html)\n\n*   [3. 线性神经网络](https://zh.d2l.ai/chapter_linear-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [3.1. 线性回归](https://zh.d2l.ai/chapter_linear-networks/linear-regression.html)\n    *   [3.2. 线性回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)\n    *   [3.3. 线性回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html)\n    *   [3.4. softmax回归](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html)\n    *   [3.5. 图像分类数据集](https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html)\n    *   [3.6. softmax回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)\n    *   [3.7. softmax回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html)\n\n*   [4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [4.1. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html)\n    *   [4.2. 多层感知机的从零开始实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html)\n    *   [4.3. 多层感知机的简洁实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html)\n    *   [4.4. 模型选择、欠拟合和过拟合](https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html)\n    *   [4.5. 权重衰减](https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html)\n    *   [4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)\n    *   [4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)\n    *   [4.9. 环境和分布偏移](https://zh.d2l.ai/chapter_multilayer-perceptrons/environment.html)\n    *   [4.10. 实战Kaggle比赛：预测房价](https://zh.d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html)\n\n*   [5. 深度学习计算](https://zh.d2l.ai/chapter_deep-learning-computation/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [5.1. 层和块](https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html)\n    *   [5.2. 参数管理](https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html)\n    *   [5.3. 延后初始化](https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html)\n    *   [5.4. 自定义层](https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html)\n    *   [5.5. 读写文件](https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html)\n    *   [5.6. GPU](https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html)\n\n*   [6. 卷积神经网络](https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [6.1. 从全连接层到卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html)\n    *   [6.2. 图像卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)\n    *   [6.3. 填充和步幅](https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)\n    *   [6.4. 多输入多输出通道](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)\n    *   [6.5. 汇聚层](https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html)\n    *   [6.6. 卷积神经网络（LeNet）](https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html)\n\n*   [7. 现代卷积神经网络](https://zh.d2l.ai/chapter_convolutional-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [7.1. 深度卷积神经网络（AlexNet）](https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html)\n    *   [7.2. 使用块的网络（VGG）](https://zh.d2l.ai/chapter_convolutional-modern/vgg.html)\n    *   [7.3. 网络中的网络（NiN）](https://zh.d2l.ai/chapter_convolutional-modern/nin.html)\n    *   [7.4. 含并行连结的网络（GoogLeNet）](https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html)\n    *   [7.5. 批量规范化](https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html)\n    *   [7.6. 残差网络（ResNet）](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html)\n    *   [7.7. 稠密连接网络（DenseNet）](https://zh.d2l.ai/chapter_convolutional-modern/densenet.html)\n\n*   [8. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [8.1. 序列模型](https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html)\n    *   [8.2. 文本预处理](https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)\n    *   [8.3. 语言模型和数据集](https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)\n    *   [8.4. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n    *   [8.5. 循环神经网络的从零开始实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)\n    *   [8.6. 循环神经网络的简洁实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html)\n    *   [8.7. 通过时间反向传播](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)\n\n*   [9. 现代循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [9.1. 门控循环单元（GRU）](https://zh.d2l.ai/chapter_recurrent-modern/gru.html)\n    *   [9.2. 长短期记忆网络（LSTM）](https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)\n    *   [9.3. 深度循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/deep-rnn.html)\n    *   [9.4. 双向循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/bi-rnn.html)\n    *   [9.5. 机器翻译与数据集](https://zh.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html)\n    *   [9.6. 编码器-解码器架构](https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html)\n    *   [9.7. 序列到序列学习（seq2seq）](https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html)\n    *   [9.8. 束搜索](https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html)\n\n*   [10. 注意力机制](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [10.1. 注意力提示](https://zh.d2l.ai/chapter_attention-mechanisms/attention-cues.html)\n    *   [10.2. 注意力汇聚：Nadaraya-Watson 核回归](https://zh.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)\n    *   [10.3. 注意力评分函数](https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)\n    *   [10.4. Bahdanau 注意力](https://zh.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)\n    *   [10.5. 多头注意力](https://zh.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)\n    *   [10.6. 自注意力和位置编码](https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)\n    *   [10.7. Transformer](https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html)\n\n*   [11. 优化算法](https://zh.d2l.ai/chapter_optimization/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [11.1. 优化和深度学习](https://zh.d2l.ai/chapter_optimization/optimization-intro.html)\n    *   [11.2. 凸性](https://zh.d2l.ai/chapter_optimization/convexity.html)\n    *   [11.3. 梯度下降](https://zh.d2l.ai/chapter_optimization/gd.html)\n    *   [11.4. 随机梯度下降](https://zh.d2l.ai/chapter_optimization/sgd.html)\n    *   [11.5. 小批量随机梯度下降](https://zh.d2l.ai/chapter_optimization/minibatch-sgd.html)\n    *   [11.6. 动量法](https://zh.d2l.ai/chapter_optimization/momentum.html)\n    *   [11.7. AdaGrad算法](https://zh.d2l.ai/chapter_optimization/adagrad.html)\n    *   [11.8. RMSProp算法](https://zh.d2l.ai/chapter_optimization/rmsprop.html)\n    *   [11.9. Adadelta](https://zh.d2l.ai/chapter_optimization/adadelta.html)\n    *   [11.10. Adam算法](https://zh.d2l.ai/chapter_optimization/adam.html)\n    *   [11.11. 学习率调度器](https://zh.d2l.ai/chapter_optimization/lr-scheduler.html)\n\n*   [12. 计算性能](https://zh.d2l.ai/chapter_computational-performance/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [12.1. 编译器和解释器](https://zh.d2l.ai/chapter_computational-performance/hybridize.html)\n    *   [12.2. 异步计算](https://zh.d2l.ai/chapter_computational-performance/async-computation.html)\n    *   [12.3. 自动并行](https://zh.d2l.ai/chapter_computational-performance/auto-parallelism.html)\n    *   [12.4. 硬件](https://zh.d2l.ai/chapter_computational-performance/hardware.html)\n    *   [12.5. 多GPU训练](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus.html)\n    *   [12.6. 多GPU的简洁实现](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus-concise.html)\n    *   [12.7. 参数服务器](https://zh.d2l.ai/chapter_computational-performance/parameterserver.html)\n\n*   [13. 计算机视觉](https://zh.d2l.ai/chapter_computer-vision/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [13.1. 图像增广](https://zh.d2l.ai/chapter_computer-vision/image-augmentation.html)\n    *   [13.2. 微调](https://zh.d2l.ai/chapter_computer-vision/fine-tuning.html)\n    *   [13.3. 目标检测和边界框](https://zh.d2l.ai/chapter_computer-vision/bounding-box.html)\n    *   [13.4. 锚框](https://zh.d2l.ai/chapter_computer-vision/anchor.html)\n    *   [13.5. 多尺度目标检测](https://zh.d2l.ai/chapter_computer-vision/multiscale-object-detection.html)\n    *   [13.6. 目标检测数据集](https://zh.d2l.ai/chapter_computer-vision/object-detection-dataset.html)\n    *   [13.7. 单发多框检测（SSD）](https://zh.d2l.ai/chapter_computer-vision/ssd.html)\n    *   [13.8. 区域卷积神经网络（R-CNN）系列](https://zh.d2l.ai/chapter_computer-vision/rcnn.html)\n    *   [13.9. 语义分割和数据集](https://zh.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html)\n    *   [13.10. 转置卷积](https://zh.d2l.ai/chapter_computer-vision/transposed-conv.html)\n    *   [13.11. 全卷积网络](https://zh.d2l.ai/chapter_computer-vision/fcn.html)\n    *   [13.12. 风格迁移](https://zh.d2l.ai/chapter_computer-vision/neural-style.html)\n    *   [13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)](https://zh.d2l.ai/chapter_computer-vision/kaggle-cifar10.html)\n    *   [13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）](https://zh.d2l.ai/chapter_computer-vision/kaggle-dog.html)\n\n*   [14. 自然语言处理：预训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [14.1. 词嵌入（word2vec）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)\n    *   [14.2. 近似训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html)\n    *   [14.3. 用于预训练词嵌入的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)\n    *   [14.4. 预训练word2vec](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html)\n    *   [14.5. 全局向量的词嵌入（GloVe）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/glove.html)\n    *   [14.6. 子词嵌入](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html)\n    *   [14.7. 词的相似性和类比任务](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)\n    *   [14.8. 来自Transformers的双向编码器表示（BERT）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert.html)\n    *   [14.9. 用于预训练BERT的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html)\n    *   [14.10. 预训练BERT](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html)\n\n*   [15. 自然语言处理：应用](https://zh.d2l.ai/chapter_natural-language-processing-applications/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [15.1. 情感分析及数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html)\n    *   [15.2. 情感分析：使用循环神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-rnn.html)\n    *   [15.3. 情感分析：使用卷积神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html)\n    *   [15.4. 自然语言推断与数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html)\n    *   [15.5. 自然语言推断：使用注意力](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-attention.html)\n    *   [15.6. 针对序列级和词元级应用微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html)\n    *   [15.7. 自然语言推断：微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)\n\n*   [16. 附录：深度学习工具](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [16.1. 使用Jupyter Notebook](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html)\n    *   [16.2. 使用Amazon SageMaker](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/sagemaker.html)\n    *   [16.3. 使用Amazon EC2实例](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/aws.html)\n    *   [16.4. 选择服务器和GPU](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html)\n    *   [16.5. 为本书做贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html)\n    *   [16.6. `d2l` API 文档](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/d2l.html)\n\n*   [参考文献](https://zh.d2l.ai/chapter_references/zreferences.html)\n\n4.7. 前向传播、反向传播和计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#sec-backprop \"Permalink to this heading\")\n\n[Colab [mxnet]](https://colab.research.google.com/github/d2l-ai/d2l-zh-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [pytorch]](https://colab.research.google.com/github/d2l-ai/d2l-zh-pytorch-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [tensorflow]](https://colab.research.google.com/github/d2l-ai/d2l-zh-tensorflow-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [paddle]](https://colab.research.google.com/github/d2l-ai/d2l-zh-paddle-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[SageMaker Studio Lab](https://studiolab.sagemaker.aws/import/github/d2l-ai/d2l-pytorch-sagemaker-studio-lab/blob/main/GettingStarted-D2L.ipynb)\n\n Open the notebook in SageMaker Studio Lab\n============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过 _前向传播_（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。\n\n梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨 _反向传播_ 的细节。 首先，我们将重点放在带权重衰减（L 2 正则化）的单隐藏层多层感知机上。\n\n4.7.1. 前向传播[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2 \"Permalink to this heading\")\n--------------------------------------------------------------------------------------------------------------\n\n_前向传播_（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。\n\n我们将一步步研究单隐藏层神经网络的机制， 为了简单起见，我们假设输入样本是 x∈R d， 并且我们的隐藏层不包括偏置项。 这里的中间变量是：\n\n(4.7.1)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-0 \"Permalink to this equation\")z=W(1)x,\n\n其中 W(1)∈R h×d 是隐藏层的权重参数。 将中间变量 z∈R h 通过激活函数 ϕ 后， 我们得到长度为 h 的隐藏激活向量：\n\n(4.7.2)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-1 \"Permalink to this equation\")h=ϕ(z).\n\n隐藏变量 h 也是一个中间变量。 假设输出层的参数只有权重 W(2)∈R q×h， 我们可以得到输出层变量，它是一个长度为 q 的向量：\n\n(4.7.3)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-2 \"Permalink to this equation\")o=W(2)h.\n\n假设损失函数为 l，样本标签为 y，我们可以计算单个数据样本的损失项，\n\n(4.7.4)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-3 \"Permalink to this equation\")L=l(o,y).\n\n根据 L 2 正则化的定义，给定超参数 λ，正则化项为\n\n(4.7.5)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-forward-s \"Permalink to this equation\")s=λ 2(‖W(1)‖F 2+‖W(2)‖F 2),\n\n其中矩阵的Frobenius范数是将矩阵展平为向量后应用的 L 2 范数。 最后，模型在给定数据样本上的正则化损失为：\n\n(4.7.6)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-4 \"Permalink to this equation\")J=L+s.\n\n在下面的讨论中，我们将 J 称为 _目标函数_（objective function）。\n\n4.7.2. 前向传播计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id3 \"Permalink to this heading\")\n-----------------------------------------------------------------------------------------------------------------\n\n绘制 _计算图_ 有助于我们可视化计算中操作符和变量的依赖关系。 [图4.7.1](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#fig-forward) 是与上述简单网络相对应的计算图， 其中正方形表示变量，圆圈表示操作符。 左下角表示输入，右上角表示输出。 注意显示数据流的箭头方向主要是向右和向上的。\n\n![Image 4: ../_images/forward.svg](https://zh.d2l.ai/_images/forward.svg)\n图4.7.1 前向传播的计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id8 \"Permalink to this image\")\n\n4.7.3. 反向传播[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4 \"Permalink to this heading\")\n--------------------------------------------------------------------------------------------------------------\n\n_反向传播_（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的 _链式规则_，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 假设我们有函数 Y=f(X)和 Z=g(Y)， 其中输入和输出 X,Y,Z 是任意形状的张量。 利用链式法则，我们可以计算 Z 关于 X 的导数\n\n(4.7.7)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-5 \"Permalink to this equation\")∂Z∂X=prod(∂Z∂Y,∂Y∂X).\n\n在这里，我们使用 prod 运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。 对于向量，这很简单，它只是矩阵-矩阵乘法。 对于高维张量，我们使用适当的对应项。 运算符 prod 指代了所有的这些符号。\n\n回想一下，在计算图 [图4.7.1](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#fig-forward)中的单隐藏层简单网络的参数是 W(1)和 W(2)。 反向传播的目的是计算梯度∂J/∂W(1)和 ∂J/∂W(2)。 为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。 计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数 J=L+s 相对于损失项 L 和正则项 s 的梯度。\n\n(4.7.8)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-6 \"Permalink to this equation\")∂J∂L=1 and∂J∂s=1.\n\n接下来，我们根据链式法则计算目标函数关于输出层变量 o 的梯度：\n\n(4.7.9)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-7 \"Permalink to this equation\")∂J∂o=prod(∂J∂L,∂L∂o)=∂L∂o∈R q.\n\n接下来，我们计算正则化项相对于两个参数的梯度：\n\n(4.7.10)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-8 \"Permalink to this equation\")∂s∂W(1)=λ W(1)and∂s∂W(2)=λ W(2).\n\n现在我们可以计算最接近输出层的模型参数的梯度 ∂J/∂W(2)∈R q×h。 使用链式法则得出：\n\n(4.7.11)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-backprop-j-h \"Permalink to this equation\")∂J∂W(2)=prod(∂J∂o,∂o∂W(2))+prod(∂J∂s,∂s∂W(2))=∂J∂o h⊤+λ W(2).\n\n为了获得关于 W(1)的梯度，我们需要继续沿着输出层到隐藏层反向传播。 关于隐藏层输出的梯度∂J/∂h∈R h 由下式给出：\n\n(4.7.12)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-9 \"Permalink to this equation\")∂J∂h=prod(∂J∂o,∂o∂h)=W(2)⊤∂J∂o.\n\n由于激活函数 ϕ 是按元素计算的， 计算中间变量 z 的梯度∂J/∂z∈R h 需要使用按元素乘法运算符，我们用⊙表示：\n\n(4.7.13)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-10 \"Permalink to this equation\")∂J∂z=prod(∂J∂h,∂h∂z)=∂J∂h⊙ϕ′(z).\n\n最后，我们可以得到最接近输入层的模型参数的梯度 ∂J/∂W(1)∈R h×d。 根据链式法则，我们得到：\n\n(4.7.14)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-11 \"Permalink to this equation\")∂J∂W(1)=prod(∂J∂z,∂z∂W(1))+prod(∂J∂s,∂s∂W(1))=∂J∂z x⊤+λ W(1).\n\n4.7.4. 训练神经网络[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id5 \"Permalink to this heading\")\n----------------------------------------------------------------------------------------------------------------\n\n在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。\n\n以上述简单网络为例：一方面，在前向传播期间计算正则项 [(4.7.5)](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-forward-s)取决于模型参数 W(1)和 W(2)的当前值。 它们是由优化算法根据最近迭代的反向传播给出的。 另一方面，反向传播期间参数 [(4.7.11)](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-backprop-j-h)的梯度计算， 取决于由前向传播给出的隐藏变量 h 的当前值。\n\n因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致 _内存不足_（out of memory）错误。\n\n4.7.5. 小结[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id6 \"Permalink to this heading\")\n------------------------------------------------------------------------------------------------------------\n\n*   前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。\n\n*   反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。\n\n*   在训练深度学习模型时，前向传播和反向传播是相互依赖的。\n\n*   训练比预测需要更多的内存。\n\n4.7.6. 练习[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id7 \"Permalink to this heading\")\n------------------------------------------------------------------------------------------------------------\n\n1.   假设一些标量函数 X 的输入 X 是 n×m 矩阵。f 相对于 X 的梯度维数是多少？\n\n2.   向本节中描述的模型的隐藏层添加偏置项（不需要在正则化项中包含偏置项）。\n\n    1.   画出相应的计算图。\n\n    2.   推导正向和反向传播方程。\n\n3.   计算本节所描述的模型，用于训练和预测的内存占用。\n\n4.   假设想计算二阶导数。计算图发生了什么？预计计算需要多长时间？\n\n5.   假设计算图对当前拥有的GPU来说太大了。\n\n    1.   请试着把它划分到多个GPU上。\n\n    2.   与小批量训练相比，有哪些优点和缺点？\n\n[Discussions](https://discuss.d2l.ai/t/5769)\n\nTable Of Contents\n\n*   [前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [1. 前向传播](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2)\n    *   [2. 前向传播计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id3)\n    *   [3. 反向传播](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4)\n    *   [4. 训练神经网络](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id5)\n    *   [5. 小结](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id6)\n    *   [6. 练习](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id7)\n\n[Previous 4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)[Next 4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)",
      "resultMd": "# 前向传播、反向传播和计算图\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [前向传播](#前向传播)\n  - [计算图](#计算图)\n  - [反向传播](#反向传播)\n  - [训练过程中的依赖关系](#训练过程中的依赖关系)\n- [代码与示例](#代码与示例)\n- [常见问题与避坑](#常见问题与避坑)\n- [复习清单](#复习清单)\n\n---\n\n## 核心概念摘要\n\n| 概念 | 定义 | 关键特征 |\n|:---|:---|:---|\n| **前向传播 (Forward Propagation)** | 从输入层到输出层顺序计算神经网络各层结果 | 存储中间变量，用于后续反向传播 |\n| **反向传播 (Backpropagation)** | 利用链式法则从输出层到输入层计算参数梯度 | 复用前向传播的中间值，避免重复计算 |\n| **计算图 (Computational Graph)** | 可视化操作符与变量依赖关系的有向图 | 节点=变量/操作，边=数据流向 |\n| **自动微分 (Autograd)** | 自动计算梯度的技术 | 现代深度学习框架的核心能力 |\n\n**核心洞察**：训练比预测需要更多内存，因为必须保留前向传播的中间值直到反向传播完成。\n\n---\n\n## 知识点详解\n\n### 前向传播\n\n#### 是什么\n按顺序（输入层 → 隐藏层 → 输出层）计算并存储神经网络每层的结果。\n\n#### 数学推导（单隐藏层MLP）\n\n设输入样本 $\\mathbf{x} \\in \\mathbb{R}^d$，网络结构如下：\n\n| 步骤 | 计算 | 维度 |\n|:---|:---|:---|\n| 1. 隐藏层线性变换 | $\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x}$ | $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}, \\mathbf{z} \\in \\mathbb{R}^h$ |\n| 2. 激活函数 | $\\mathbf{h} = \\phi(\\mathbf{z})$ | $\\mathbf{h} \\in \\mathbb{R}^h$ |\n| 3. 输出层线性变换 | $\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h}$ | $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}, \\mathbf{o} \\in \\mathbb{R}^q$ |\n| 4. 损失计算 | $L = l(\\mathbf{o}, y)$ | 标量 |\n| 5. L2正则化 | $s = \\frac{\\lambda}{2}\\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right)$ | 标量 |\n| 6. 目标函数 | $J = L + s$ | 标量 |\n\n其中 $\\|\\cdot\\|_F$ 表示 Frobenius 范数（矩阵展平后的 L2 范数）。\n\n---\n\n### 计算图\n\n#### 可视化结构\n```\n        ┌─────────┐\n   x ──→│  W^(1)  │──→ z ──→ φ ──→ h ──→ W^(2) ──→ o ──→ L ──┐\n        └─────────┘                                          ├──→ J = L + s\n        λ/2·‖W^(1)‖² ────────────────────────────────────────┤\n        λ/2·‖W^(2)‖² ────────────────────────────────────────┘\n```\n\n**图例说明**：\n- □ 正方形：变量（输入、参数、中间结果）\n- ○ 圆形：操作符（矩阵乘法、激活函数、加法等）\n- → 箭头：数据流向（主要向右、向上）\n\n---\n\n### 反向传播\n\n#### 核心原理：链式法则\n\n对于复合函数 $Z = g(Y), Y = f(X)$：\n\n$$\\frac{\\partial Z}{\\partial X} = \\text{prod}\\left(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial Y}{\\partial X}\\right)$$\n\n> `prod` 运算符：执行必要的转置和维度对齐后进行矩阵/张量乘法。\n\n#### 梯度计算流程（从输出到输入）\n\n| 步骤 | 梯度计算 | 结果维度 |\n|:---|:---|:---|\n| 1. 目标函数分解 | $\\frac{\\partial J}{\\partial L} = 1, \\quad \\frac{\\partial J}{\\partial s} = 1$ | 标量 |\n| 2. 输出层梯度 | $\\frac{\\partial J}{\\partial \\mathbf{o}} = \\frac{\\partial L}{\\partial \\mathbf{o}}$ | $\\mathbb{R}^q$ |\n| 3. 正则化项梯度 | $\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)}, \\quad \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}$ | 同权重维度 |\n| 4. **W^(2) 的梯度** | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$ | $\\mathbb{R}^{q \\times h}$ |\n| 5. 隐藏层输出梯度 | $\\frac{\\partial J}{\\partial \\mathbf{h}} = {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}$ | $\\mathbb{R}^h$ |\n| 6. 激活前梯度（逐元素乘） | $\\frac{\\partial J}{\\partial \\mathbf{z}} = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$ | $\\mathbb{R}^h$ |\n| 7. **W^(1) 的梯度** | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}$ | $\\mathbb{R}^{h \\times d}$ |\n\n#### 关键公式详解\n\n**输出层权重梯度**（公式 4.7.11）：\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\underbrace{\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top}_{\\text{来自损失}} + \\underbrace{\\lambda \\mathbf{W}^{(2)}}_{\\text{来自正则化}}$$\n\n**隐藏层权重梯度**（公式 4.7.14）：\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\underbrace{\\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top}_{\\text{链式传递}} + \\underbrace{\\lambda \\mathbf{W}^{(1)}}_{\\text{来自正则化}}$$\n\n> **注意**：$\\odot$ 表示 Hadamard 积（逐元素乘法），因激活函数 $\\phi$ 是逐元素应用的。\n\n---\n\n### 训练过程中的依赖关系\n\n#### 双向依赖机制\n\n```\n┌─────────────────┐         ┌─────────────────┐\n│   前向传播       │ ←────→ │   反向传播       │\n│  (Forward Pass) │  依赖   │ (Backward Pass) │\n└─────────────────┘         └─────────────────┘\n        ↓                           ↑\n   • 计算并存储 h               • 使用 h 计算 ∂J/∂W^(2)\n   • 计算当前 W 下的 s          • 使用存储的 z 计算 ϕ'(z)\n   • 生成所有中间变量            • 复用中间值，避免重算\n```\n\n#### 内存影响分析\n\n| 场景 | 内存需求 | 原因 |\n|:---|:---|:---|\n| **预测 (Inference)** | 低 | 只需当前层激活，可逐层释放 |\n| **训练 (Training)** | 高 | 需保留所有中间值至反向传播完成 |\n\n**内存占用因素**：\n- 与网络层数成正比（更深 → 更多中间值）\n- 与批量大小成正比（更大 batch → 更大张量）\n\n> ⚠️ 深层网络 + 大批量 = 显存不足 (OOM) 风险\n\n---\n\n## 代码与示例\n\n### 手动实现前向与反向传播\n\n```python\nimport torch\nfrom torch import nn\n\n# 定义简单的单隐藏层MLP\nclass SimpleMLP:\n    def __init__(self, input_dim, hidden_dim, output_dim, lambda_reg=0.01):\n        # 初始化参数\n        self.W1 = torch.randn(hidden_dim, input_dim, requires_grad=True)\n        self.W2 = torch.randn(output_dim, hidden_dim, requires_grad=True)\n        self.lambda_reg = lambda_reg\n        \n    def forward(self, x):\n        \"\"\"\n        前向传播: x -> z -> h -> o -> J\n        保存中间值供反向传播使用\n        \"\"\"\n        # 线性变换: z = W^(1) @ x\n        self.z = self.W1 @ x  # 保存 z\n        # 激活: h = φ(z)\n        self.h = torch.relu(self.z)  # 保存 h\n        # 输出: o = W^(2) @ h\n        self.o = self.W2 @ self.h  # 保存 o\n        \n        return self.o\n    \n    def compute_loss(self, o, y):\n        \"\"\"计算总损失 J = L + s\"\"\"\n        # 损失项 L (MSE示例)\n        L = ((o - y) ** 2).sum() / 2\n        \n        # 正则化项 s = λ/2 * (||W1||² + ||W2||²)\n        s = self.lambda_reg / 2 * (\n            (self.W1 ** 2).sum() + (self.W2 ** 2).sum()\n        )\n        \n        self.L = L  # 保存用于调试\n        self.s = s\n        \n        return L + s\n    \n    def backward_manual(self, x, y):\n        \"\"\"\n        手动反向传播实现\n        演示梯度计算的核心逻辑\n        \"\"\"\n        # 前向传播获取中间值\n        o = self.forward(x)\n        J = self.compute_loss(o, y)\n        \n        # ========== 反向传播开始 ==========\n        \n        # 1. dJ/dL = 1, dJ/ds = 1\n        dJ_dL = 1.0\n        dJ_ds = 1.0\n        \n        # 2. dJ/do = dL/do (MSE: o - y)\n        dJ_do = (o - y)  # 形状: (output_dim,)\n        \n        # 3. ds/dW 的正则化部分\n        ds_dW2_reg = self.lambda_reg * self.W2\n        ds_dW1_reg = self.lambda_reg * self.W1\n        \n        # 4. dJ/dW^(2) = dJ/do @ h^T + λ*W^(2)\n        #    外积: (q,) @ (h,) -> (q, h)\n        dJ_dW2 = torch.outer(dJ_do, self.h) + ds_dW2_reg\n        \n        # 5. dJ/dh = W^(2)^T @ dJ/do\n        dJ_dh = self.W2.T @ dJ_do  # 形状: (hidden_dim,)\n        \n        # 6. dJ/dz = dJ/dh ⊙ φ'(z)  (ReLU导数: z > 0 ? 1 : 0)\n        relu_deriv = (self.z > 0).float()\n        dJ_dz = dJ_dh * relu_deriv  # 逐元素乘\n        \n        # 7. dJ/dW^(1) = dJ/dz @ x^T + λ*W^(1)\n        dJ_dW1 = torch.outer(dJ_dz, x) + ds_dW1_reg\n        \n        return {\n            'dJ_dW1': dJ_dW1,\n            'dJ_dW2': dJ_dW2,\n            'loss': J.item()\n        }\n\n\n# ========== 验证与PyTorch自动微分的一致性 ==========\n\ndef test_gradient_correctness():\n    \"\"\"验证手动实现与torch.autograd的一致性\"\"\"\n    torch.manual_seed(42)\n    \n    # 创建模型和数据\n    model = SimpleMLP(input_dim=4, hidden_dim=5, output_dim=3, lambda_reg=0.1)\n    x = torch.randn(4)\n    y = torch.randn(3)\n    \n    # 手动反向传播\n    manual_grads = model.backward_manual(x, y)\n    \n    # PyTorch自动微分\n    model.zero_grad()\n    o = model.forward(x)\n    J = model.compute_loss(o, y)\n    J.backward()\n    \n    # 比较梯度\n    print(\"梯度一致性检查:\")\n    print(f\"W1 梯度误差: {(manual_grads['dJ_dW1'] - model.W1.grad).abs().max():.2e}\")\n    print(f\"W2 梯度误差: {(manual_grads['dJ_dW2'] - model.W2.grad).abs().max():.2e}\")\n    \n    return manual_grads\n\n# 运行测试\n# grads = test_gradient_correctness()\n```\n\n### 使用PyTorch简洁实现\n\n```python\nimport torch\nfrom torch import nn\n\nclass MLPModule(nn.Module):\n    \"\"\"使用PyTorch内置机制的简洁实现\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, lambda_reg=0.01):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim, bias=False)\n        self.lambda_reg = lambda_reg\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n    \n    def loss_with_regularization(self, pred, target):\n        \"\"\"包含L2正则化的完整损失\"\"\"\n        # 数据损失\n        data_loss = nn.functional.mse_loss(pred, target)\n        \n        # L2正则化 (weight decay)\n        reg_loss = 0\n        for param in self.parameters():\n            reg_loss += (param ** 2).sum()\n        \n        return data_loss + self.lambda_reg / 2 * reg_loss\n\n\n# 训练循环示例\ndef train_step(model, optimizer, x, y):\n    \"\"\"单步训练，展示前向-反向-更新的完整流程\"\"\"\n    model.train()\n    \n    # ===== 前向传播 =====\n    optimizer.zero_grad()      # 清除旧梯度\n    pred = model(x)            # 计算输出\n    loss = model.loss_with_regularization(pred, y)  # 计算损失\n    \n    # ===== 反向传播 =====\n    loss.backward()            # 自动计算所有梯度\n    \n    # ===== 参数更新 =====\n    optimizer.step()           # 应用梯度更新\n    \n    return loss.item()\n\n\n# 内存监控示例\ndef monitor_memory():\n    \"\"\"监控训练时的内存占用\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        \n        # 模拟训练\n        model = MLPModule(1000, 500, 10).cuda()\n        x = torch.randn(64, 1000, device='cuda')  # batch_size=64\n        y = torch.randn(64, 10, device='cuda')\n        \n        pred = model(x)\n        loss = model.loss_with_regularization(pred, y)\n        loss.backward()\n        \n        peak_mb = torch.cuda.max_memory_allocated() / 1024**2\n        print(f\"峰值显存占用: {peak_mb:.2f} MB\")\n        \n        # 对比：仅前向传播\n        torch.cuda.reset_peak_memory_stats()\n        with torch.no_grad():\n            _ = model(x)\n        forward_only_mb = torch.cuda.max_memory_allocated() / 1024**2\n        print(f\"仅前向传播: {forward_only_mb:.2f} MB\")\n        print(f\"反向传播额外开销: {peak_mb - forward_only_mb:.2f} MB\")\n```\n\n---\n\n## 常见问题与避坑\n\n### Q1: 为什么训练时显存占用突然暴增？\n\n| 原因 | 解释 | 解决方案 |\n|:---|:---|:---|\n| 保留了计算图 | 意外将含梯度的张量存入列表 | 使用 `.detach()` 或 `with torch.no_grad()` |\n| 批量过大 | 中间激活值与batch size成正比 | 减小batch size，或使用梯度累积 |\n| 网络过深 | 每层激活都需保存 | 使用梯度检查点 (gradient checkpointing) |\n| 使用了RNN/LSTM | 序列长导致展开图巨大 | 截断BPTT，或使用更高效的实现 |\n\n### Q2: 反向传播中的常见错误\n\n```python\n# ❌ 错误：在不需要梯度的地方没有禁用\ndef wrong_inference(model, x):\n    pred = model(x)  # 仍构建计算图！\n    return pred.argmax()\n\n# ✅ 正确：推理时禁用梯度计算\ndef correct_inference(model, x):\n    with torch.no_grad():  # 不追踪操作，节省内存\n        pred = model(x)\n    return pred.argmax()\n\n# ❌ 错误：手动实现时修改了中间变量\nz = W1 @ x\nh = torch.relu(z)\nh[0] = 0  # 破坏计算图！无法正确求导\n\n# ✅ 正确：使用in-place操作需谨慎\n# 或使用 clone() 创建副本\n```\n\n### Q3: 二阶导数的计算图变化\n\n若需计算 Hessian 矩阵（二阶导数）：\n- **计算图扩展**：在一阶导数基础上继续反向传播\n- **时间复杂度**：从 $O(n)$ 增至 $O(n^2)$（n为参数数量）\n- **内存需求**：需保留一阶导数的计算图，进一步增加\n\n```python\n# 二阶导数示例（Hessian对角线）\nx = torch.randn(3, requires_grad=True)\ny = x.pow(2).sum()\n\n# 一阶导\ngrad = torch.autograd.grad(y, x, create_graph=True)[0]  # dy/dx = 2x\n\n# 二阶导（对一阶导再求导）\nhessian_diag = torch.autograd.grad(grad.sum(), x)[0]  # d²y/dx² = 2\n```\n\n---\n\n## 复习清单\n\n- [ ] **前向传播**：能写出单隐藏层MLP的完整前向公式（含正则化）\n- [ ] **计算图理解**：能手绘简单网络的计算图，标明变量和操作\n- [ ] **链式法则**：掌握向量/矩阵形式的链式法则，理解 `prod` 操作\n- [ ] **反向传播推导**：能独立推导 $\\partial J/\\partial \\mathbf{W}^{(1)}$ 和 $\\partial J/\\partial \\mathbf{W}^{(2)}$\n- [ ] **逐元素激活**：理解为何使用 Hadamard 积 $\\odot$ 而非矩阵乘法\n- [ ] **内存管理**：解释训练比预测耗内存的原因，列举优化策略\n- [ ] **自动微分**：熟练使用 `backward()`, `grad()`, `no_grad()` 等API\n\n### 关键公式速查\n\n| 公式 | 含义 |\n|:---|:---|\n| $\\mathbf{z} = \\mathbf{W}^{(1)}\\mathbf{x}$ | 隐藏层线性变换 |\n| $\\mathbf{h} = \\phi(\\mathbf{z})$ | 激活函数 |\n| $\\mathbf{o} = \\mathbf{W}^{(2)}\\mathbf{h}$ | 输出层线性变换 |\n| $J = l(\\mathbf{o}, y) + \\frac{\\lambda}{2}(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2)$ | 带正则化的目标函数 |\n| $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial J}{\\partial \\mathbf{o}}\\mathbf{h}^\\top + \\lambda\\mathbf{W}^{(2)}$ | 输出层权重梯度 |\n| $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\left({\\mathbf{W}^{(2)}}^\\top\\frac{\\partial J}{\\partial \\mathbf{o}}\\odot\\phi'(\\mathbf{z})\\right)\\mathbf{x}^\\top + \\lambda\\mathbf{W}^{(1)}$ | 隐藏层权重梯度 |",
      "resolvedTitle": "4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_b8d32058f3e1",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=32",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T16:43:22.125Z",
      "updatedAt": "2026-02-15T16:48:28.389Z",
      "preparedMd": "# 多链接转录内容汇总\n\n## 来源 1\n\n原始链接：https://www.bilibili.com/video/BV1U2WmzfEqp?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=32\n\n# 第二章-面向对象高级-08.抽象类案例_空调案例\n\n## 转录全文\n\n好 那接着呢 我们来吧 播开的好处 给大家去捉铁一下 那么在这呢 我们下来看第一点 首先 再不改变 矿下 代码的情况下 通过多态 可以轻松的时间 模块有块直接的一个 节目说 什么料 模块呢 又会发现 刚才我们那个构建品牌 无论是英雄机一带 还是二带 同一个航处 接收不同的参数 它的效果是不是不一样了 那从而实现了转接系统的一个可活展性 那么对于结果的作用呢 大家会发现变成 它其实就是实现了一个使用的 任务的一个调整者和任务的什么 编写者的一个结合 那哪个是编写者呢 编写者配理解为是平台 然后呢 调整配理解为是谁呢 那就是名里边的这些英雄机和低机 那最后一个对于可活展 可活展的意思就是说 那搭建的平台航处 再不做任何修改的情况下 可以要用来念一下后边的景色 加上什么 后来有个新人带航 这是上一次呢 我给你举个数一点的例子 就是大家都听得懂的 那什么例子呢 来 又要来举游戏的那个例子 你看啊 问你个问题啊 大家知道 那么王手勇要 它里边有多少个英雄啊 100 100多个吧 现在应该是100多个 好 那么我想表的意思是 你会发现 王手里边它是怎么去玩的呢 左边是几个样 比如说 红色方 几个呢 这五个人 这五个英雄 那么再来个蓝色方 几个英雄 我的英雄 那接下来 你五个英雄继续以后 那你会发现 这是它里边的什么 两段叫一图 它有它的机制 这是怎么去玩 几分钟 什么刷楼 然后什么刷小野怪 然后刷什么什么野猪 然后刷你的红蓝霸佛 然后什么怎么样 你会发现 那包括你的小边 然后包括你的防疫彈机制 包括别的 里边这个内容 是不是都是鞋丝 那接下来你玩的动作 就是红色方五个 和蓝色方五个 接下来以后 按照这个规则来走 是不是就行了 我问你个问题 假设某一天 突然多了一个新的英雄 多了一个新的英雄 什么呢 比如说在这个位置 把捲连加进去了 是吧 捲连大家加进去了 沙森里边现在有吗 没有 还没有 比如说捲连就是沙河上 比如说加进去了 那这个是我问你个问题 他里面的这个地图和机制 要重做了 不用 要重做吗 不用 为什么不用 那就看这个英雄将来是干嘛呢 在红色方被他选呢 还是被红色方选了 还是被蓝色方选了 你作为其中的一个英雄 然后继续正常来玩 是不是就可以了 跟上来了吗 所以这其实就是我们所谓的叫可破载性 你会发现我的机制是一套 至于你传过来的是谁 那这个机制不会随着你传过来内容的改变 而改变 那是不是都是连一套的结果呀 所以这个点要注意 所以很多游戏升级 它其实都是玩了这么一套规则 剩下的就是不断的 让你在这里边干嘛呀 去操作操作就可以了 所以呢 那就像有的游戏前期做的还挺好 后期是不是开始合金 开始怎么怎么样了 这就是属于把里面的机制啥的面临干了 那些收盘 我们接着我现在来聊 所以呢多态 其实就可以实现一个叫 节目和这个操作 所以你看我现在再来讲 大家能不能理解这个事情 来 你再看一样 你先告诉我 封装它的一个好处是什么 封装它那个好处是什么 封装好处 就玩水性啊 还有一个附游性 对吧 附游性 那告诉我安全性又是一个保证 是由那个私用保证 那个附游性又是一个保证 是不是由那个喊手保证 因为喊处是野人封装 那告诉我它的弊端是什么 它的弊端是带码量增加 带码量增加 那非常带码量增加 你不吃饿的吗 你不刚说服用吧 带码量增加了 因为安全了 你私有了 是不要提过盖的和赛的 你带码量增加了 带码量增加 你告诉我怎么解决 于是有了第二个技术 第二技术叫什么 技成 那技成 先说 技成 最最最最简单的 它的好处就是提高 带码的什么键 附游性 附游性 为了附游性 依旧化 子成 附游 那一看 你多多被增加增加 被没关系 把属性写到附雷中 然后给他私有 再提过盖的和赛的 我只要在附雷里边 玩一次 此类 是不是就可以技术过来 不用塞写了 这是不是可以提高 带码的附游性 所以好处 那提高带码附游性 你可以理解为 就是解决这个问题的 但是技成带来一个弊端 告诉这个弊端是什么 它带来个弊端 告诉那个弊端是什么 技成的弊端就是 叫欧和性责强 怎么解释 欧和性责强的这个事呢 说白了 这是附雷的不好的功能 子雷干嘛 想没有 都不行 比如说 在附雷那个人类里边 加个死母艇出烟 这个功能 你会发现 它的所有子雷 是不是都有了这个功能 那说的报红经理 是不是说明 是个人就得要出烟 你不出烟 就不是人 那捷换不合适吧 所以说 那就会发现这种方式 它就强和合了 那我应该怎么解决呢 后边还要想做招多态 多态来先说 它那个好处 好处刚才说过了 把这个地方换不在啊 多态 那它的好处呢 其实就是应用节奧 这个好处 协下 那就是应用 干嘛呢 应用节奧 应用节奧 节奧是啥意思呢 那就是同一个喊处 投喊处 然后干嘛 喊处 接收不同的三处 有不同的什么 不同效果 但是你告诉多态有个避端 多态有个避端 多态也有避端 多态的避端是什么 无法精准 限定累计了 那将来是不是有可能会出现 我要的是一个动物 结果那你看 咱上几个 就是传了一个汽车进去 他居然也能干结上 他无法受精准的限定 为什么 因为潘斯作了多态 视为多态 但是他们也不为我们不管 就讨论你个问题 潘斯在这里边 能不能解决你技术的 我和性增强 这个问题 能可以 对吧 这是实现了模块之间的一个结果 这样的操作 好了 OK 那接下来 我们很重新 就是潘斯的多态 无法精准限定 不是插网 插网的多态 能精准限定 说啥就啥 然后插网 终是可以的 潘斯终不行 而且我还能告诉你 就怕潘斯终 来给你写一下 潘斯终 然后两段网 抽象力 等于解放 一模一样 插网终是不一样的 但是在潘斯终是一样的 所以说 我们可以说 潘斯这个地方的多态 他无法精准限定类型 我把这个内容 我肯着塞一下 我给你们贴到哪里 贴到笔记中 贴到这个位置 就是当然大家可以看 就叫多态内容 写到这儿 大家可以看他这个代码 那么说过来 我们接着 下来讲下个内容 下个操作 就是我们要去写了 对于进程和多态的理解 你可以理解为 进程是还的 可以复用老爹的东西 是不是可以进 进程过来 而多态相当适合老爹的方向 啥意思呢 这不做任何修改的情况下 可以扩展使用后来人写的内容 这就是刚才咱说的那个暂动机 那个对战平台 那比如说后来人写了英雄机三代 迪迪二代 迪迪三代 迪迪四代 那你会发现我的平台依旧无辞 就拿网上主要来讲 他的对战机制 包括小兵防御谈 机制已经写好了 以后无论出什么吸引型 那你进来以后 是不是都要按照我这套规则 往下来走 是一样的 OK 那进来 往下再来看 下一个 为个更好的使用多态 这个特性 行业专家们又提出来 来捏一下这两段 叫什么 抽向内容 什么 抽向接口 其实 简称 这叫接口 都干净 那么来看 细心的同学会发现 之前我们互联 Animal的 4p个方法 他是一个空使劲 我说了 有空使劲的这个罕处 称之为叫什么来着 抽向方法 对吧 没有方法体的方法 叫抽向方法 那么有抽向方法的类 被称之为叫抽向内 抽向内 那有人就会问我的问题 你塞他问我什么 老师 如果这个类里边没有抽向方法呢 你看 这个类里边 比如拿下位的一个 这个类 貓类 还有这个狗类 他里边是不是都没有 抽向方法啊 那这样的类 你告诉我 他叫什么类 他就叫普通类 就是普通人的那个普通 就叫普通类 然后有抽向方法的类 被称之为叫抽向内 所以来了 这种设计含义就是 负类 用来确定 用哪些方法 就是制定接口标准 具体的实现 由此类来实现 只是此类 基于标准来称成 那么这种写法呢 就称之为叫抽向类 你看 后边有决法 也可以称之为叫什么 写法 这决法 就是在拍摄中一回事 扎完中两回事 拍摄中一模一样 所以有抽向方法的类 就是抽向类 然后抽向方法不是 方法体是空时间的方法 就成之为叫抽向方法 好 那我告诉你一句话 抽向类有什么用 抽向类 倒于标准 抽向类 倒于两字 找标准 好 抽向类 一般干嘛呢 一般充当什么类用 充当 复类用 然后干嘛呢 设定标准 我问你个问题啊 大家知道 现在咱们国家 就是那个汽车 油车 那个汽车的 北气什么什么排放量 现在我们在执行的是一个什么标准 国务还是国六的 是不是现在到了国六的表准 好 你看 国家就发布一个政策 你的汽车 北气必须达到国六的标准 你才能上路 怎么能量 国家发布完这个政策以后 请问各个汽车的制造商 他们要做的事情就是 想办法 让他们汽车的北气排放 主要符合这个标准就行了 那我问你个问题啊 去吧 那各个厂商 他们的手段和方式是一样的吗 比如说 用什么方式打到这个 那必须这个汽车 也用什么方式打到这个 是一样的吗 不是 你会发现各个厂商 有点像什么 发现过害 个性沈通 最终是不是能符合这个标准就行了 那我想表达意思就是 那这个充向类 就可以理解为 这是国家制定的标准 好 那比如说 再来 国家对定 你的空调 必须用哪些充能 那就可以发现 比如像目前的隔地啊 小米啊 小米啊 包括那个美地啊 等等等等 是不是有很多很多的空调制造商 每家空调制造商 看制冷和制热的技术 是不是都不一样 但是最终只要能实现 一个效果是就可以了 对 是一样的 那再来 手机大家发现 目前手机品牌 是不是也是众多的呀 好 那么国家规定 你看啊 你如果想去发布一款手机 你必须具备什么什么什么什么 什么功能 你才能发布 那么你的手机 啊 你的手机 如果不符合这个规范 那是不是就不可以啊 所以那对于我们来讲 挺好啊 充向类一般是充当后立的制定标准 那什么时候用处向类呢 大家看一下 这个一看就知道了 他是一个什么空调 那国家规定了 空调制定标准 你看 所以早的时候 那些空调制楼 大多数的空调都采用什么 那个加服 就那个福利亚那个技术 是不是通过那个玩意制楼 但是后来发现福利亚会干嘛 不欢迎的抽样层 说那个欧三 然后导致那个死外线这么增强了 然后人就永远得皮不病 或者怎么样 等等等等 后来国家是不是卸地 然后空调制造商 制楼的技术 是不是不能采用加服这个小桌呀 那好 那国家出台这个政策以后 那剩下的具体怎么实现 怎么解决这个事 是不是各个生产超 他们自己是来找自己的一个方案去实现 对吧 那就发现过海了 随着你会发现 抽象类一般充当所类 就干嘛制定标准 我空实现 我就告诉你 必须有这个 但是怎么做 你自己想办法还做 我不管 当然你得实现它 好 那就跟国家规定 气息说现在凯蒙利亚 必须是达到国六标准 如果你这款新车 你达不到 那我就不允许你上市 我就不允许你卖 好 那么怎么达到这个事 你自己想办法 自管去策 改进你的发脱机 还是干嘛 还是干嘛 发生最终我只要结果 那所以说 我们说抽象类一般是充当货类 制定标准 那比如说空调的制定标准 不是 可以制装 可以制装 可以左右摆锋 那至于你将来这个空调 你怎么制装的 我不管 你怎么制装的 我不管 你怎么左右摆锋的 我不管 哪怕 你给每个埋你空调的人 去配备一个什么的 专属服务员 晚上人家再来睡觉 然后你让这个人 服务人家的空调 再来左右摆锋 都行 对吧 但前提有人埋你这个产品的性 所以我想表了意思 就是 上边 只管制定标准 怎么实现各个厂商 发现过海 你自己来错 所以呢 接下来 我们来看一下这个动作 就变成了 大团化解释 国家或者行业提出标准后 不同的厂商 各自实现标准的要求 问你个问题 现在你在网上买的所有的优盘 所有的硬盘 所有的鼠标 所有的键盘等等等等 那个USB接口 大家有一个发现是全球终于的呀 对吧 你们有没有用过那种鼠标 就是这个可能就不要老了 灵灵后不一定用过 就是对对对 原的 原的 就是在台积上边 在最早有一类鼠标 它那个头不是USB的 是个圆头 里边我真讲 就是那种点点点点那盘 插上去 就是台积专用的那种鼠标 用过那种的吗 用过 台积我 有的可能灵灵后的微定 用过那个 就是大家接触可能比较多的 就是USB那种 是吧 那像我们小时候 还有一种圆头的 这那个圆头是对到主席上那种 你看圆头现在是毕竟没了 被淘汰了 为什么 你买个圆头 你用不了 你用哪 现在好像台积的那个接口 好像大多数都不支持那个圆头了 然后以前都是支持的 还有我上那个小学的时候 我们上那个危机课 你们用过大头机吗 跟那个显示器 大头显示器 用过吧 然后还有什么的 还要得一鼠标 那个鼠标夏娃娜的一个果图儿 是一个石球的那个 用过这个 还真用过 不了了 再聊三年纪就差不多了 然后说 说来 接着唱歌 所以 来啊 就是国家或者行业 提过我们标准 厂商各鞋神宗 然后去实现就可以了 好了 那接了 我们来看看具体 总在做一个落地的案例 那比如说这里边有叫AC AC市场呢 就是空调嘛 然后呢 检生嘛 然后里边有场呢 第一个 Core面的 第一个叫Core面的 什么 冷风 在那个Core面的 就是热风 Diamson Lafter I'm the right 直播我简单写了一个L 一个R 就是左右百风 好 那你看 这就是我所说的 这个叫啥 这是一个抽象的呀 那就是 国家制定的标准 比如说你想收场空调 你必须要制作 还要制作 还要会左右百风 那至于各个厂商 你是怎么弄的 我就不管 你看 美丁 他可以有自己的制作技术 那格力 是可以有自己的制作技术啊 各个公司 你各鞋神宗 只要符合我这个规范 我就让你消失 那么不符合这个规范 那你回去再研究 就可以了 所以你会发现 抽象类就是服务类 充当标准具体的实现 交给什么 此类待外成 就可以了 好了 那接下来呢 在往下面就是抽象类的 服务类征定 就是设定标准 具体的此类实现 就是实现标准 就可以了 下面就是这个丢弭 好了 那接下来就是总体了 总体之前 再把这个弹码去写一下 来 这个地方是我们讲的 08 08这个地方我比较讲啥呀 空调 抽象类解释啊 好 那么来 案例叫 也是抽象类的一个用法 好 首先 抽象类解释 我们刚才说了 抽象类在什么交怎么样 在开始中 然后呢 抽象类是 盗鱼 结果的 一回事啊 一回事 然后接下来 叫技 干嘛呢 有抽象方法的类 就是 哎 抽象类 也叫 也叫什么 结果 一模一 第二句话 第二句话 然后呢 抽象方法的鱼 没有方法 技 方法 技 是 是什么 是谁凶释的 怕死 就是 来了 那接下来 他能作用和目的一般大大的呢 就是 抽象类 一般 充当 傅类 啊 用于 啊 用于制定 制定什么 行业 规范 或者说 这个叫准则 啊 具体的实现 销油 具体的实现 实现 销油 销油谁呢 此类 来完成 就是此类 八千锅海 和写成功 还是那句话 比如空调 国家规定 啊 必须 怎么怎么样 你的 委屈排放量 达到什么什么 等等等等 这种情况 你达到了 就让你们 达不到就不让你们 那再来 比如说生产气说 那你必须 委屈 符合国六 那不符合国六 那 这个气说 就不让你少吃 怎么 那老师 已经有了 国三 或者国四的气收 怎么办 好 他有一定的年线 慢慢的那个国三 国四的气收 是不是也就不让上商务了 那比如说像 你看去一些机场啊 或者说去一些 这个别的地方 像正中那边 比如我经常往那个机场那边跑 像那边到那个路口那边 他上面就会挂一个盘 不允许国三的 或者说走这条路 就是走的话 就可能就 就空你前 或者怎么样 就罚款 或者傻力达力 就这种 那一看 现在算是追的是国六 当然在往后 过过五年或者十年 是不是那个时候 可能会国西 国八 国九是不是会有些新的标准啊 也有可能的 好了 那说回来 我们接着往下来地狼 来啦 来啦 第一步就是 第一什么 抽下来 叫空调力 那么设定 设定空调的 空调的一个什么规则 好 那接下来 克拉斯来个AC 骂好 来 1.1 叫赤鲁 叫赤鲁 赤鲁 那怎么赤鲁呢 我管你怎么知道 这个叫 core core什么 偶应的 然后地板写上 core 来得怕死 又说 这个是赤鲁 怎么赤鲁 我不管 反正你的有赤鲁 好 那接下来 1.2 1.2 是什么 赤鲁 赤鲁 我觉得你会了 或者他 命的 好 再来个 1.3 1.3 是左右 百分 说百分 百分 死命 laptop on the right 你看 他说死命命的 那这个叫束缚 是吧 死命 我们来得laptop on the on the right 就是左右百分嘛 好那现在 我是不是已经把规则 也制定了 那接下来 就是你具体的实现 那为你个问题 来 第二 就是 干嘛呀 叫定义 死命 比如说 例如 再来个合力吧 他说有小命 那就小命吧 小命 命条 实现 腹肋 所有的出现方法 你看 我先来个 class 销销 摸一笔 然后来个ac 但是这个地方 你看 我现在啥也不行 就会发现这件事 捏写都怕死 那么把肚子了 是不是说明 腹肋 提供了标准 紫薇 是不是没有提供距离实现啊 那么行 你是不是得实现他 怎么实现的 接下来 在这里边 我们来看一下 整个这个大阳 你可以把削盘 那几个动作给的到吧 可能谁全拉过来 拉过来以后 2.1 2.2 2.3 你是不是要把所有的功能 都得实现了 来吧 第一个 小命怎么知道啊 小冰块 提出的 核心 制作技术 好 那再来 那 否则 小命什么 制作啊 核心制作技术 你不能说家伙 家什么的不合适啊 小命 小命干嘛呢 核心 百合技术 百合又啥核心的 不就加个电机 摇头不就行了吗 对吧 核心 这 小命 小命共跳 到这个 会 对 静音 静音 左右 百封 百封 百合技术 行了 那接下来 我们开始 往下来聊了 那除了有小命一般 是不是还有什么美地啊 隔地啊 等等 除了有很多啊 那接下来呢 你看 这地方 我们来个 三 来个三 好 那么 这地方就是第一次 来 你看花围 花围有功能吗 这个 这个不清楚啊 来 我们看一下 行动 花围真有吗 好像 完出来就有 哎呦 哎呦 花围真是 花围几房 兄弟 呦 给服务器用的完成 算了 他换一个 换一个隔地的 不就点那个啊 隔地 隔地是 隔地 对吧 我们叫 鸡啊 一 啊 隔地吗 隔地 隔地 这 你叫隔地也行 那上面叫小命 前面叫隔地 那来了 这就是 三 三 三 来 三二一 不是小命 隔地之道 而是什么 隔地和隔地之道 啊 隔地和隔地 他的他 隔地同胶 然后干嘛 什么做百奉 不能这么干啊 都是咱过咱们盘子 低频吗 把低频所有百奉进行 好 那么 说话 那近代 请问 第四部 是不是有开始 我们那个叫他了 找测试 怎么测试啊 我们 来 首先 第1个 4.1 啊 4.1 干嘛呢 叫小命同胶 不像 啊 怎么 怎么写啊 相比等约 相比 要相比点什么 点扩不冷风 相比点扩等着风 相比点 此为左百方 不能的减号成立 暂时二三几 是吧 来 4.2 来 隔地同胶 后面的代码什么都是一样的 我快解剑了 那设成了 所以你会发现啊 整个的这对 那不同的成商 用的技术是不是不一样 但是你的成商 是不是必须得符合 我这边的规范啊 有人说老师我丢一个 那丢一个丢一个为 比如说小命在这里边 不会左右百风 你看 小命在这儿 是不是不会左右百风啊 又见这形了 这个跟你讲的不一样 因为你会发现 这个地方不报错 是不是正常运行啊 啥吧是报错 啥吧是啥呢 就是负类有几个规则 你的此类 必须有几个规则 比如说负类你看 在这里面 负类是为有三规则 那你的此类 必须有几个规则 必须有三 如果没有就报错 当然开始就没有 他只是说 你要这么去划 而你这个动作 要注意 随着我把你工作 在这里打开 那你的此类呢 要去处够付类 所有的这些抽象方法 那么这就是关于抽下类的 一个用法 大家有用发现 就是充当什么来用的 充当付类充当 规则来用的 具体的实现 是不是交给此类的 什么 这个了解下去 所以就没发现 面向对上部分 我虽然写了很多代码 但是绝大多数都不需要理念 你只需要把这个代码 比较大的含义和它的一个用法 是不是理解就可以了 很多都不需要你去写的 随这个要注意 来落地了 找同学 找同学打得好 再就结束 打得不好 我就再讲一个 有没有似到分用了 就是不是几个了 12.9分 第九层的 第九层没有 第十层 来 6号 什么是多态 声稍微大点 拍战中真的是同一个喊数 接收不同的 对象 然后有不同的效果 大改化就是 同一个事务 不同环境 有不同效果 有点水 高温低温 长温挺好的 OK 那接下来在我身边 实现多态的条件 有基层 有重写 如果没有重写的话 就直接玩复类的 就没有意义了 第三个是有 Animo An等于开谈 这个是撒牌上来着 有复类应用 指向 指向 但是如果大家 真正意义上 你去扣这个点 你会发现一件事 这三条件 哪怕我一个都不把足 那个喊数 是不是也能时间咱要的效果 喊那句话 拍战中 它是萎多态 因为它是弱类型的 它做不了 像炸网那样 精准的限定 正细节戴上主义 来 多态的好处 这就两个 节奏好 节奏好处 多大的好处 其实就是实现 节奏合的 实现软件的扩展 那下来 同样的一个平台 你是不是传入什么对象 我是说 下来就会有对应的一个效果 这个知道就行了 好 OK 那关于多态 我们就聊到这个地方 好 那么我把这个支点 往前一下\n\n## 分段时间轴\n\n- [00:00-00:02] 好 那接着呢 我们来吧 播开的好处\n- [00:02-00:04] 给大家去捉铁一下\n- [00:04-00:06] 那么在这呢 我们下来看第一点\n- [00:06-00:10] 首先 再不改变 矿下 代码的情况下\n- [00:10-00:12] 通过多态 可以轻松的时间\n- [00:12-00:14] 模块有块直接的一个 节目说\n- [00:14-00:16] 什么料 模块呢 又会发现\n- [00:16-00:18] 刚才我们那个构建品牌\n- [00:18-00:21] 无论是英雄机一带 还是二带\n- [00:21-00:23] 同一个航处 接收不同的参数\n- [00:23-00:24] 它的效果是不是不一样了\n- [00:24-00:28] 那从而实现了转接系统的一个可活展性\n- [00:28-00:30] 那么对于结果的作用呢\n- [00:30-00:31] 大家会发现变成\n- [00:31-00:34] 它其实就是实现了一个使用的\n- [00:34-00:37] 任务的一个调整者和任务的什么\n- [00:37-00:39] 编写者的一个结合\n- [00:39-00:41] 那哪个是编写者呢\n- [00:41-00:43] 编写者配理解为是平台\n- [00:43-00:45] 然后呢 调整配理解为是谁呢\n- [00:45-00:48] 那就是名里边的这些英雄机和低机\n- [00:48-00:50] 那最后一个对于可活展\n- [00:50-00:51] 可活展的意思就是说\n- [00:51-00:54] 那搭建的平台航处\n- [00:54-00:56] 再不做任何修改的情况下\n- [00:56-00:58] 可以要用来念一下后边的景色\n- [00:58-00:59] 加上什么\n- [00:59-01:00] 后来有个新人带航\n- [01:00-01:01] 这是上一次呢\n- [01:01-01:03] 我给你举个数一点的例子\n- [01:03-01:05] 就是大家都听得懂的\n- [01:05-01:06] 那什么例子呢\n- [01:06-01:08] 来 又要来举游戏的那个例子\n- [01:08-01:11] 你看啊 问你个问题啊\n- [01:11-01:13] 大家知道 那么王手勇要\n- [01:13-01:16] 它里边有多少个英雄啊\n- [01:16-01:17] 100 100多个吧\n- [01:17-01:19] 现在应该是100多个\n- [01:19-01:21] 好 那么我想表的意思是\n- [01:21-01:22] 你会发现\n- [01:22-01:24] 王手里边它是怎么去玩的呢\n- [01:24-01:26] 左边是几个样\n- [01:26-01:27] 比如说 红色方\n- [01:27-01:28] 几个呢\n- [01:28-01:29] 这五个人\n- [01:29-01:30] 这五个英雄\n- [01:30-01:31] 那么再来个蓝色方\n- [01:31-01:33] 几个英雄 我的英雄\n- [01:33-01:34] 那接下来\n- [01:34-01:35] 你五个英雄继续以后\n- [01:35-01:36] 那你会发现\n- [01:36-01:38] 这是它里边的什么\n- [01:38-01:39] 两段叫一图\n- [01:39-01:40] 它有它的机制\n- [01:40-01:41] 这是怎么去玩\n- [01:41-01:43] 几分钟 什么刷楼\n- [01:43-01:45] 然后什么刷小野怪\n- [01:45-01:46] 然后刷什么什么野猪\n- [01:46-01:48] 然后刷你的红蓝霸佛\n- [01:48-01:49] 然后什么怎么样\n- [01:49-01:50] 你会发现\n- [01:50-01:51] 那包括你的小边\n- [01:51-01:53] 然后包括你的防疫彈机制\n- [01:53-01:54] 包括别的\n- [01:54-01:55] 里边这个内容\n- [01:55-01:56] 是不是都是鞋丝\n- [01:56-01:57] 那接下来你玩的动作\n- [01:57-01:59] 就是红色方五个\n- [01:59-02:00] 和蓝色方五个\n- [02:00-02:01] 接下来以后\n- [02:01-02:02] 按照这个规则来走\n- [02:02-02:03] 是不是就行了\n- [02:03-02:04] 我问你个问题\n- [02:04-02:05] 假设某一天\n- [02:05-02:08] 突然多了一个新的英雄\n- [02:08-02:10] 多了一个新的英雄\n- [02:10-02:10] 什么呢\n- [02:10-02:11] 比如说在这个位置\n- [02:11-02:12] 把捲连加进去了\n- [02:12-02:14] 是吧\n- [02:14-02:15] 捲连大家加进去了\n- [02:15-02:17] 沙森里边现在有吗\n- [02:17-02:18] 没有\n- [02:18-02:18] 还没有\n- [02:18-02:21] 比如说捲连就是沙河上\n- [02:21-02:22] 比如说加进去了\n- [02:22-02:23] 那这个是我问你个问题\n- [02:23-02:25] 他里面的这个地图和机制\n- [02:25-02:26] 要重做了\n- [02:26-02:27] 不用\n- [02:27-02:28] 要重做吗\n- [02:28-02:29] 不用\n- [02:29-02:29] 为什么不用\n- [02:29-02:31] 那就看这个英雄将来是干嘛呢\n- [02:31-02:33] 在红色方被他选呢\n- [02:33-02:35] 还是被红色方选了\n- [02:35-02:37] 还是被蓝色方选了\n- [02:37-02:38] 你作为其中的一个英雄\n- [02:38-02:40] 然后继续正常来玩\n- [02:40-02:41] 是不是就可以了\n- [02:41-02:42] 跟上来了吗\n- [02:42-02:44] 所以这其实就是我们所谓的叫可破载性\n- [02:44-02:47] 你会发现我的机制是一套\n- [02:47-02:49] 至于你传过来的是谁\n- [02:49-02:52] 那这个机制不会随着你传过来内容的改变\n- [02:52-02:53] 而改变\n- [02:53-02:55] 那是不是都是连一套的结果呀\n- [02:55-02:56] 所以这个点要注意\n- [02:56-02:58] 所以很多游戏升级\n- [02:58-03:01] 它其实都是玩了这么一套规则\n- [03:01-03:03] 剩下的就是不断的\n- [03:03-03:04] 让你在这里边干嘛呀\n- [03:04-03:06] 去操作操作就可以了\n- [03:06-03:07] 所以呢\n- [03:07-03:09] 那就像有的游戏前期做的还挺好\n- [03:09-03:10] 后期是不是开始合金\n- [03:10-03:11] 开始怎么怎么样了\n- [03:11-03:14] 这就是属于把里面的机制啥的面临干了\n- [03:14-03:15] 那些收盘\n- [03:15-03:16] 我们接着我现在来聊\n- [03:16-03:17] 所以呢多态\n- [03:17-03:19] 其实就可以实现一个叫\n- [03:19-03:21] 节目和这个操作\n- [03:21-03:22] 所以你看我现在再来讲\n- [03:22-03:24] 大家能不能理解这个事情\n- [03:24-03:25] 来 你再看一样\n- [03:25-03:27] 你先告诉我\n- [03:27-03:29] 封装它的一个好处是什么\n- [03:30-03:33] 封装它那个好处是什么\n- [03:33-03:35] 封装好处\n- [03:35-03:37] 就玩水性啊\n- [03:38-03:40] 还有一个附游性\n- [03:40-03:40] 对吧\n- [03:40-03:41] 附游性\n- [03:41-03:44] 那告诉我安全性又是一个保证\n- [03:44-03:45] 是由那个私用保证\n- [03:45-03:47] 那个附游性又是一个保证\n- [03:47-03:48] 是不是由那个喊手保证\n- [03:48-03:50] 因为喊处是野人封装\n- [03:50-03:53] 那告诉我它的弊端是什么\n- [03:53-03:56] 它的弊端是带码量增加\n- [03:56-03:57] 带码量增加\n- [03:57-03:58] 那非常带码量增加\n- [03:58-03:59] 你不吃饿的吗\n- [03:59-04:00] 你不刚说服用吧\n- [04:00-04:01] 带码量增加了\n- [04:01-04:03] 因为安全了\n- [04:03-04:03] 你私有了\n- [04:03-04:05] 是不要提过盖的和赛的\n- [04:05-04:07] 你带码量增加了\n- [04:07-04:08] 带码量增加\n- [04:08-04:09] 你告诉我怎么解决\n- [04:09-04:10] 于是有了第二个技术\n- [04:10-04:11] 第二技术叫什么\n- [04:11-04:12] 技成\n- [04:12-04:13] 那技成\n- [04:13-04:14] 先说\n- [04:14-04:15] 技成\n- [04:15-04:16] 最最最最简单的\n- [04:16-04:18] 它的好处就是提高\n- [04:18-04:19] 带码的什么键\n- [04:19-04:20] 附游性\n- [04:20-04:21] 附游性\n- [04:21-04:22] 为了附游性\n- [04:22-04:22] 依旧化\n- [04:22-04:23] 子成\n- [04:23-04:24] 附游\n- [04:24-04:25] 那一看\n- [04:25-04:26] 你多多被增加增加\n- [04:26-04:27] 被没关系\n- [04:27-04:29] 把属性写到附雷中\n- [04:29-04:30] 然后给他私有\n- [04:30-04:32] 再提过盖的和赛的\n- [04:32-04:33] 我只要在附雷里边\n- [04:33-04:34] 玩一次\n- [04:34-04:35] 此类\n- [04:35-04:36] 是不是就可以技术过来\n- [04:36-04:37] 不用塞写了\n- [04:37-04:38] 这是不是可以提高\n- [04:38-04:39] 带码的附游性\n- [04:39-04:40] 所以好处\n- [04:40-04:42] 那提高带码附游性\n- [04:42-04:43] 你可以理解为\n- [04:43-04:44] 就是解决这个问题的\n- [04:44-04:47] 但是技成带来一个弊端\n- [04:47-04:49] 告诉这个弊端是什么\n- [04:49-04:51] 它带来个弊端\n- [04:51-04:52] 告诉那个弊端是什么\n- [04:52-04:55] 技成的弊端就是\n- [04:55-04:58] 叫欧和性责强\n- [04:58-04:58] 怎么解释\n- [04:58-05:00] 欧和性责强的这个事呢\n- [05:00-05:01] 说白了\n- [05:01-05:03] 这是附雷的不好的功能\n- [05:03-05:04] 子雷干嘛\n- [05:04-05:05] 想没有\n- [05:05-05:06] 都不行\n- [05:06-05:06] 比如说\n- [05:06-05:08] 在附雷那个人类里边\n- [05:08-05:09] 加个死母艇出烟\n- [05:09-05:10] 这个功能\n- [05:10-05:11] 你会发现\n- [05:11-05:12] 它的所有子雷\n- [05:12-05:13] 是不是都有了这个功能\n- [05:13-05:15] 那说的报红经理\n- [05:15-05:15] 是不是说明\n- [05:15-05:17] 是个人就得要出烟\n- [05:17-05:17] 你不出烟\n- [05:17-05:18] 就不是人\n- [05:18-05:20] 那捷换不合适吧\n- [05:20-05:21] 所以说\n- [05:21-05:22] 那就会发现这种方式\n- [05:22-05:24] 它就强和合了\n- [05:24-05:25] 那我应该怎么解决呢\n- [05:25-05:27] 后边还要想做招多态\n- [05:27-05:29] 多态来先说\n- [05:29-05:30] 它那个好处\n- [05:30-05:31] 好处刚才说过了\n- [05:31-05:33] 把这个地方换不在啊\n- [05:33-05:34] 多态\n- [05:34-05:35] 那它的好处呢\n- [05:35-05:37] 其实就是应用节奧\n- [05:37-05:37] 这个好处\n- [05:37-05:38] 协下\n- [05:38-05:40] 那就是应用\n- [05:40-05:40] 干嘛呢\n- [05:40-05:42] 应用节奧\n- [05:42-05:43] 应用节奧\n- [05:43-05:44] 节奧是啥意思呢\n- [05:44-05:46] 那就是同一个喊处\n- [05:46-05:47] 投喊处\n- [05:47-05:48] 然后干嘛\n- [05:48-05:48] 喊处\n- [05:48-05:50] 接收不同的三处\n- [05:50-05:51] 有不同的什么\n- [05:51-05:52] 不同效果\n- [05:52-05:55] 但是你告诉多态有个避端\n- [05:55-05:56] 多态有个避端\n- [05:56-05:57] 多态也有避端\n- [05:57-06:01] 多态的避端是什么\n- [06:02-06:04] 无法精准\n- [06:04-06:07] 限定累计了\n- [06:07-06:09] 那将来是不是有可能会出现\n- [06:09-06:10] 我要的是一个动物\n- [06:10-06:11] 结果那你看\n- [06:11-06:12] 咱上几个\n- [06:12-06:13] 就是传了一个汽车进去\n- [06:13-06:15] 他居然也能干结上\n- [06:15-06:16] 他无法受精准的限定\n- [06:16-06:17] 为什么\n- [06:17-06:18] 因为潘斯作了多态\n- [06:18-06:19] 视为多态\n- [06:19-06:21] 但是他们也不为我们不管\n- [06:21-06:22] 就讨论你个问题\n- [06:22-06:23] 潘斯在这里边\n- [06:23-06:25] 能不能解决你技术的\n- [06:25-06:25] 我和性增强\n- [06:25-06:26] 这个问题\n- [06:26-06:27] 能可以\n- [06:27-06:27] 对吧\n- [06:27-06:30] 这是实现了模块之间的一个结果\n- [06:30-06:31] 这样的操作\n- [06:31-06:31] 好了\n- [06:31-06:32] OK\n- [06:32-06:32] 那接下来\n- [06:32-06:34] 我们很重新\n- [06:34-06:35] 就是潘斯的多态\n- [06:35-06:36] 无法精准限定\n- [06:36-06:37] 不是插网\n- [06:37-06:38] 插网的多态\n- [06:38-06:39] 能精准限定\n- [06:39-06:40] 说啥就啥\n- [06:40-06:41] 然后插网\n- [06:41-06:42] 终是可以的\n- [06:42-06:43] 潘斯终不行\n- [06:43-06:44] 而且我还能告诉你\n- [06:44-06:45] 就怕潘斯终\n- [06:45-06:46] 来给你写一下\n- [06:46-06:47] 潘斯终\n- [06:47-06:48] 然后两段网\n- [06:48-06:50] 抽象力\n- [06:50-06:51] 等于解放\n- [06:51-06:52] 一模一样\n- [06:52-06:54] 插网终是不一样的\n- [06:54-06:55] 但是在潘斯终是一样的\n- [06:55-06:56] 所以说\n- [06:56-06:57] 我们可以说\n- [06:57-06:59] 潘斯这个地方的多态\n- [06:59-07:01] 他无法精准限定类型\n- [07:01-07:02] 我把这个内容\n- [07:02-07:03] 我肯着塞一下\n- [07:03-07:05] 我给你们贴到哪里\n- [07:05-07:06] 贴到笔记中\n- [07:06-07:07] 贴到这个位置\n- [07:07-07:08] 就是当然大家可以看\n- [07:08-07:10] 就叫多态内容\n- [07:10-07:11] 写到这儿\n- [07:11-07:12] 大家可以看他这个代码\n- [07:12-07:13] 那么说过来\n- [07:13-07:14] 我们接着\n- [07:14-07:16] 下来讲下个内容\n- [07:16-07:17] 下个操作\n- [07:17-07:18] 就是我们要去写了\n- [07:18-07:20] 对于进程和多态的理解\n- [07:20-07:21] 你可以理解为\n- [07:21-07:22] 进程是还的\n- [07:22-07:24] 可以复用老爹的东西\n- [07:24-07:25] 是不是可以进\n- [07:25-07:26] 进程过来\n- [07:26-07:28] 而多态相当适合老爹的方向\n- [07:28-07:29] 啥意思呢\n- [07:29-07:31] 这不做任何修改的情况下\n- [07:31-07:33] 可以扩展使用后来人写的内容\n- [07:33-07:35] 这就是刚才咱说的那个暂动机\n- [07:35-07:36] 那个对战平台\n- [07:36-07:39] 那比如说后来人写了英雄机三代\n- [07:39-07:39] 迪迪二代\n- [07:39-07:40] 迪迪三代\n- [07:40-07:41] 迪迪四代\n- [07:41-07:43] 那你会发现我的平台依旧无辞\n- [07:43-07:45] 就拿网上主要来讲\n- [07:45-07:46] 他的对战机制\n- [07:46-07:48] 包括小兵防御谈\n- [07:48-07:50] 机制已经写好了\n- [07:50-07:52] 以后无论出什么吸引型\n- [07:52-07:53] 那你进来以后\n- [07:53-07:54] 是不是都要按照我这套规则\n- [07:54-07:55] 往下来走\n- [07:55-07:56] 是一样的\n- [07:56-07:57] OK\n- [07:57-07:58] 那进来\n- [07:58-07:59] 往下再来看\n- [07:59-07:59] 下一个\n- [07:59-08:01] 为个更好的使用多态\n- [08:01-08:02] 这个特性\n- [08:02-08:04] 行业专家们又提出来\n- [08:04-08:05] 来捏一下这两段\n- [08:05-08:05] 叫什么\n- [08:05-08:06] 抽向内容\n- [08:06-08:07] 什么\n- [08:07-08:08] 抽向接口\n- [08:08-08:09] 其实\n- [08:09-08:09] 简称\n- [08:09-08:10] 这叫接口\n- [08:10-08:11] 都干净\n- [08:11-08:12] 那么来看\n- [08:12-08:13] 细心的同学会发现\n- [08:13-08:15] 之前我们互联\n- [08:15-08:16] Animal的\n- [08:16-08:16] 4p个方法\n- [08:16-08:18] 他是一个空使劲\n- [08:18-08:19] 我说了\n- [08:19-08:21] 有空使劲的这个罕处\n- [08:21-08:23] 称之为叫什么来着\n- [08:23-08:25] 抽向方法\n- [08:25-08:26] 对吧\n- [08:26-08:27] 没有方法体的方法\n- [08:27-08:28] 叫抽向方法\n- [08:28-08:30] 那么有抽向方法的类\n- [08:30-08:33] 被称之为叫抽向内\n- [08:33-08:34] 抽向内\n- [08:34-08:36] 那有人就会问我的问题\n- [08:36-08:37] 你塞他问我什么\n- [08:37-08:38] 老师\n- [08:38-08:40] 如果这个类里边没有抽向方法呢\n- [08:40-08:40] 你看\n- [08:40-08:41] 这个类里边\n- [08:41-08:42] 比如拿下位的一个\n- [08:42-08:43] 这个类\n- [08:43-08:44] 貓类\n- [08:44-08:45] 还有这个狗类\n- [08:45-08:46] 他里边是不是都没有\n- [08:46-08:48] 抽向方法啊\n- [08:48-08:48] 那这样的类\n- [08:48-08:49] 你告诉我\n- [08:49-08:50] 他叫什么类\n- [08:50-08:52] 他就叫普通类\n- [08:52-08:53] 就是普通人的那个普通\n- [08:53-08:54] 就叫普通类\n- [08:54-08:56] 然后有抽向方法的类\n- [08:56-08:57] 被称之为叫抽向内\n- [08:57-08:58] 所以来了\n- [08:58-09:00] 这种设计含义就是\n- [09:00-09:01] 负类\n- [09:01-09:02] 用来确定\n- [09:02-09:03] 用哪些方法\n- [09:03-09:05] 就是制定接口标准\n- [09:05-09:06] 具体的实现\n- [09:06-09:07] 由此类来实现\n- [09:07-09:08] 只是此类\n- [09:08-09:10] 基于标准来称成\n- [09:10-09:11] 那么这种写法呢\n- [09:11-09:12] 就称之为叫抽向类\n- [09:12-09:13] 你看\n- [09:13-09:14] 后边有决法\n- [09:14-09:16] 也可以称之为叫什么\n- [09:16-09:17] 写法\n- [09:17-09:17] 这决法\n- [09:17-09:19] 就是在拍摄中一回事\n- [09:19-09:20] 扎完中两回事\n- [09:20-09:22] 拍摄中一模一样\n- [09:22-09:24] 所以有抽向方法的类\n- [09:24-09:25] 就是抽向类\n- [09:25-09:27] 然后抽向方法不是\n- [09:27-09:29] 方法体是空时间的方法\n- [09:29-09:32] 就成之为叫抽向方法\n- [09:32-09:32] 好\n- [09:32-09:33] 那我告诉你一句话\n- [09:33-09:35] 抽向类有什么用\n- [09:35-09:35] 抽向类\n- [09:35-09:38] 倒于标准\n- [09:38-09:39] 抽向类\n- [09:39-09:40] 倒于两字\n- [09:40-09:42] 找标准\n- [09:42-09:42] 好\n- [09:42-09:43] 抽向类\n- [09:43-09:44] 一般干嘛呢\n- [09:44-09:46] 一般充当什么类用\n- [09:46-09:46] 充当\n- [09:46-09:47] 复类用\n- [09:47-09:48] 然后干嘛呢\n- [09:48-09:49] 设定标准\n- [09:49-09:50] 我问你个问题啊\n- [09:50-09:51] 大家知道\n- [09:51-09:52] 现在咱们国家\n- [09:52-09:53] 就是那个汽车\n- [09:53-09:54] 油车\n- [09:54-09:55] 那个汽车的\n- [09:55-09:57] 北气什么什么排放量\n- [09:57-09:58] 现在我们在执行的是一个什么标准\n- [09:58-10:00] 国务还是国六的\n- [10:00-10:02] 是不是现在到了国六的表准\n- [10:02-10:02] 好\n- [10:02-10:02] 你看\n- [10:02-10:04] 国家就发布一个政策\n- [10:04-10:04] 你的汽车\n- [10:04-10:06] 北气必须达到国六的标准\n- [10:06-10:08] 你才能上路\n- [10:08-10:08] 怎么能量\n- [10:08-10:11] 国家发布完这个政策以后\n- [10:11-10:14] 请问各个汽车的制造商\n- [10:14-10:16] 他们要做的事情就是\n- [10:16-10:17] 想办法\n- [10:17-10:19] 让他们汽车的北气排放\n- [10:19-10:20] 主要符合这个标准就行了\n- [10:20-10:22] 那我问你个问题啊\n- [10:22-10:22] 去吧\n- [10:22-10:24] 那各个厂商\n- [10:24-10:26] 他们的手段和方式是一样的吗\n- [10:26-10:27] 比如说\n- [10:27-10:28] 用什么方式打到这个\n- [10:28-10:30] 那必须这个汽车\n- [10:30-10:31] 也用什么方式打到这个\n- [10:31-10:32] 是一样的吗\n- [10:32-10:33] 不是\n- [10:33-10:34] 你会发现各个厂商\n- [10:34-10:36] 有点像什么\n- [10:36-10:37] 发现过害\n- [10:37-10:38] 个性沈通\n- [10:38-10:40] 最终是不是能符合这个标准就行了\n- [10:40-10:42] 那我想表达意思就是\n- [10:42-10:43] 那这个充向类\n- [10:43-10:44] 就可以理解为\n- [10:44-10:46] 这是国家制定的标准\n- [10:46-10:47] 好\n- [10:47-10:47] 那比如说\n- [10:47-10:48] 再来\n- [10:48-10:49] 国家对定\n- [10:49-10:50] 你的空调\n- [10:50-10:52] 必须用哪些充能\n- [10:52-10:53] 那就可以发现\n- [10:53-10:55] 比如像目前的隔地啊\n- [10:55-10:56] 小米啊\n- [10:56-10:57] 小米啊\n- [10:57-10:58] 包括那个美地啊\n- [10:58-10:59] 等等等等\n- [10:59-11:01] 是不是有很多很多的空调制造商\n- [11:01-11:03] 每家空调制造商\n- [11:03-11:05] 看制冷和制热的技术\n- [11:05-11:06] 是不是都不一样\n- [11:06-11:07] 但是最终只要能实现\n- [11:07-11:09] 一个效果是就可以了\n- [11:09-11:09] 对\n- [11:09-11:09] 是一样的\n- [11:09-11:10] 那再来\n- [11:10-11:12] 手机大家发现\n- [11:12-11:13] 目前手机品牌\n- [11:13-11:14] 是不是也是众多的呀\n- [11:14-11:15] 好\n- [11:15-11:16] 那么国家规定\n- [11:16-11:17] 你看啊\n- [11:17-11:20] 你如果想去发布一款手机\n- [11:20-11:21] 你必须具备什么什么什么什么\n- [11:21-11:22] 什么功能\n- [11:22-11:23] 你才能发布\n- [11:23-11:25] 那么你的手机\n- [11:25-11:25] 啊\n- [11:25-11:26] 你的手机\n- [11:26-11:27] 如果不符合这个规范\n- [11:27-11:29] 那是不是就不可以啊\n- [11:29-11:30] 所以那对于我们来讲\n- [11:30-11:31] 挺好啊\n- [11:31-11:34] 充向类一般是充当后立的制定标准\n- [11:34-11:36] 那什么时候用处向类呢\n- [11:36-11:37] 大家看一下\n- [11:37-11:38] 这个一看就知道了\n- [11:38-11:40] 他是一个什么空调\n- [11:40-11:41] 那国家规定了\n- [11:41-11:43] 空调制定标准\n- [11:43-11:43] 你看\n- [11:43-11:45] 所以早的时候\n- [11:45-11:47] 那些空调制楼\n- [11:47-11:49] 大多数的空调都采用什么\n- [11:49-11:50] 那个加服\n- [11:50-11:52] 就那个福利亚那个技术\n- [11:52-11:54] 是不是通过那个玩意制楼\n- [11:54-11:56] 但是后来发现福利亚会干嘛\n- [11:56-11:57] 不欢迎的抽样层\n- [11:57-11:58] 说那个欧三\n- [11:58-12:00] 然后导致那个死外线这么增强了\n- [12:00-12:02] 然后人就永远得皮不病\n- [12:02-12:02] 或者怎么样\n- [12:02-12:03] 等等等等\n- [12:03-12:05] 后来国家是不是卸地\n- [12:05-12:06] 然后空调制造商\n- [12:06-12:07] 制楼的技术\n- [12:07-12:09] 是不是不能采用加服这个小桌呀\n- [12:09-12:10] 那好\n- [12:10-12:12] 那国家出台这个政策以后\n- [12:12-12:14] 那剩下的具体怎么实现\n- [12:14-12:16] 怎么解决这个事\n- [12:16-12:18] 是不是各个生产超\n- [12:18-12:21] 他们自己是来找自己的一个方案去实现\n- [12:21-12:21] 对吧\n- [12:21-12:23] 那就发现过海了\n- [12:23-12:24] 随着你会发现\n- [12:24-12:26] 抽象类一般充当所类\n- [12:26-12:27] 就干嘛制定标准\n- [12:27-12:28] 我空实现\n- [12:28-12:29] 我就告诉你\n- [12:29-12:30] 必须有这个\n- [12:30-12:32] 但是怎么做\n- [12:32-12:33] 你自己想办法还做\n- [12:33-12:33] 我不管\n- [12:33-12:35] 当然你得实现它\n- [12:35-12:35] 好\n- [12:35-12:36] 那就跟国家规定\n- [12:36-12:38] 气息说现在凯蒙利亚\n- [12:38-12:39] 必须是达到国六标准\n- [12:39-12:41] 如果你这款新车\n- [12:41-12:42] 你达不到\n- [12:42-12:43] 那我就不允许你上市\n- [12:43-12:45] 我就不允许你卖\n- [12:45-12:45] 好\n- [12:45-12:47] 那么怎么达到这个事\n- [12:47-12:48] 你自己想办法\n- [12:48-12:49] 自管去策\n- [12:49-12:50] 改进你的发脱机\n- [12:50-12:51] 还是干嘛\n- [12:51-12:52] 还是干嘛\n- [12:52-12:53] 发生最终我只要结果\n- [12:53-12:54] 那所以说\n- [12:54-12:57] 我们说抽象类一般是充当货类\n- [12:57-12:57] 制定标准\n- [12:57-12:59] 那比如说空调的制定标准\n- [12:59-13:00] 不是\n- [13:00-13:01] 可以制装\n- [13:01-13:01] 可以制装\n- [13:01-13:02] 可以左右摆锋\n- [13:02-13:05] 那至于你将来这个空调\n- [13:05-13:06] 你怎么制装的\n- [13:06-13:06] 我不管\n- [13:06-13:07] 你怎么制装的\n- [13:07-13:08] 我不管\n- [13:08-13:09] 你怎么左右摆锋的\n- [13:09-13:09] 我不管\n- [13:09-13:11] 哪怕\n- [13:11-13:13] 你给每个埋你空调的人\n- [13:13-13:14] 去配备一个什么的\n- [13:14-13:16] 专属服务员\n- [13:16-13:17] 晚上人家再来睡觉\n- [13:17-13:18] 然后你让这个人\n- [13:18-13:19] 服务人家的空调\n- [13:19-13:20] 再来左右摆锋\n- [13:20-13:21] 都行\n- [13:21-13:21] 对吧\n- [13:21-13:23] 但前提有人埋你这个产品的性\n- [13:23-13:25] 所以我想表了意思\n- [13:25-13:25] 就是\n- [13:25-13:26] 上边\n- [13:26-13:27] 只管制定标准\n- [13:27-13:29] 怎么实现各个厂商\n- [13:29-13:30] 发现过海\n- [13:30-13:31] 你自己来错\n- [13:31-13:31] 所以呢\n- [13:31-13:32] 接下来\n- [13:32-13:33] 我们来看一下这个动作\n- [13:33-13:34] 就变成了\n- [13:34-13:35] 大团化解释\n- [13:35-13:37] 国家或者行业提出标准后\n- [13:37-13:38] 不同的厂商\n- [13:38-13:41] 各自实现标准的要求\n- [13:41-13:42] 问你个问题\n- [13:42-13:44] 现在你在网上买的所有的优盘\n- [13:44-13:45] 所有的硬盘\n- [13:45-13:47] 所有的鼠标\n- [13:47-13:48] 所有的键盘等等等等\n- [13:48-13:50] 那个USB接口\n- [13:50-13:52] 大家有一个发现是全球终于的呀\n- [13:52-13:53] 对吧\n- [13:53-13:55] 你们有没有用过那种鼠标\n- [13:55-13:57] 就是这个可能就不要老了\n- [13:57-13:58] 灵灵后不一定用过\n- [13:58-13:59] 就是对对对\n- [13:59-14:00] 原的\n- [14:00-14:01] 原的\n- [14:01-14:03] 就是在台积上边\n- [14:03-14:05] 在最早有一类鼠标\n- [14:05-14:07] 它那个头不是USB的\n- [14:07-14:08] 是个圆头\n- [14:08-14:09] 里边我真讲\n- [14:09-14:10] 就是那种点点点点那盘\n- [14:10-14:11] 插上去\n- [14:11-14:13] 就是台积专用的那种鼠标\n- [14:13-14:14] 用过那种的吗\n- [14:14-14:15] 用过\n- [14:15-14:16] 台积我\n- [14:16-14:17] 有的可能灵灵后的微定\n- [14:17-14:18] 用过那个\n- [14:18-14:20] 就是大家接触可能比较多的\n- [14:20-14:21] 就是USB那种\n- [14:21-14:22] 是吧\n- [14:22-14:23] 那像我们小时候\n- [14:23-14:24] 还有一种圆头的\n- [14:24-14:26] 这那个圆头是对到主席上那种\n- [14:26-14:28] 你看圆头现在是毕竟没了\n- [14:28-14:29] 被淘汰了\n- [14:29-14:29] 为什么\n- [14:29-14:30] 你买个圆头\n- [14:30-14:31] 你用不了\n- [14:31-14:31] 你用哪\n- [14:31-14:34] 现在好像台积的那个接口\n- [14:34-14:36] 好像大多数都不支持那个圆头了\n- [14:36-14:38] 然后以前都是支持的\n- [14:38-14:41] 还有我上那个小学的时候\n- [14:41-14:43] 我们上那个危机课\n- [14:43-14:44] 你们用过大头机吗\n- [14:44-14:45] 跟那个显示器\n- [14:45-14:46] 大头显示器\n- [14:46-14:47] 用过吧\n- [14:47-14:49] 然后还有什么的\n- [14:49-14:50] 还要得一鼠标\n- [14:50-14:52] 那个鼠标夏娃娜的一个果图儿\n- [14:52-14:53] 是一个石球的那个\n- [14:53-14:55] 用过这个\n- [14:55-14:56] 还真用过\n- [14:56-14:57] 不了了\n- [14:57-14:59] 再聊三年纪就差不多了\n- [14:59-15:00] 然后说\n- [15:00-15:00] 说来\n- [15:00-15:01] 接着唱歌\n- [15:01-15:02] 所以\n- [15:02-15:03] 来啊\n- [15:03-15:04] 就是国家或者行业\n- [15:04-15:06] 提过我们标准\n- [15:06-15:07] 厂商各鞋神宗\n- [15:07-15:09] 然后去实现就可以了\n- [15:09-15:10] 好了\n- [15:10-15:10] 那接了\n- [15:10-15:11] 我们来看看具体\n- [15:11-15:13] 总在做一个落地的案例\n- [15:13-15:15] 那比如说这里边有叫AC\n- [15:15-15:16] AC市场呢\n- [15:16-15:17] 就是空调嘛\n- [15:17-15:18] 然后呢\n- [15:18-15:19] 检生嘛\n- [15:19-15:20] 然后里边有场呢\n- [15:20-15:20] 第一个\n- [15:20-15:22] Core面的\n- [15:22-15:23] 第一个叫Core面的\n- [15:23-15:24] 什么\n- [15:24-15:24] 冷风\n- [15:24-15:25] 在那个Core面的\n- [15:25-15:26] 就是热风\n- [15:26-15:27] Diamson\n- [15:27-15:28] Lafter\n- [15:28-15:28] I'm the right\n- [15:28-15:30] 直播我简单写了一个L\n- [15:30-15:31] 一个R\n- [15:31-15:32] 就是左右百风\n- [15:32-15:32] 好\n- [15:32-15:33] 那你看\n- [15:33-15:34] 这就是我所说的\n- [15:34-15:35] 这个叫啥\n- [15:35-15:37] 这是一个抽象的呀\n- [15:37-15:37] 那就是\n- [15:37-15:38] 国家制定的标准\n- [15:38-15:40] 比如说你想收场空调\n- [15:40-15:41] 你必须要制作\n- [15:41-15:42] 还要制作\n- [15:42-15:44] 还要会左右百风\n- [15:44-15:45] 那至于各个厂商\n- [15:45-15:46] 你是怎么弄的\n- [15:46-15:47] 我就不管\n- [15:47-15:47] 你看\n- [15:47-15:48] 美丁\n- [15:48-15:50] 他可以有自己的制作技术\n- [15:50-15:51] 那格力\n- [15:51-15:53] 是可以有自己的制作技术啊\n- [15:53-15:54] 各个公司\n- [15:54-15:55] 你各鞋神宗\n- [15:55-15:57] 只要符合我这个规范\n- [15:57-15:58] 我就让你消失\n- [15:58-16:00] 那么不符合这个规范\n- [16:00-16:01] 那你回去再研究\n- [16:01-16:02] 就可以了\n- [16:02-16:03] 所以你会发现\n- [16:03-16:04] 抽象类就是服务类\n- [16:04-16:07] 充当标准具体的实现\n- [16:07-16:08] 交给什么\n- [16:08-16:09] 此类待外成\n- [16:09-16:10] 就可以了\n- [16:10-16:10] 好了\n- [16:10-16:11] 那接下来呢\n- [16:11-16:13] 在往下面就是抽象类的\n- [16:13-16:14] 服务类征定\n- [16:14-16:15] 就是设定标准\n- [16:15-16:16] 具体的此类实现\n- [16:16-16:17] 就是实现标准\n- [16:17-16:17] 就可以了\n- [16:17-16:19] 下面就是这个丢弭\n- [16:19-16:19] 好了\n- [16:19-16:21] 那接下来就是总体了\n- [16:21-16:21] 总体之前\n- [16:21-16:23] 再把这个弹码去写一下\n- [16:23-16:24] 来\n- [16:24-16:25] 这个地方是我们讲的\n- [16:25-16:26] 08\n- [16:26-16:28] 08这个地方我比较讲啥呀\n- [16:30-16:32] 空调\n- [16:32-16:33] 抽象类解释啊\n- [16:33-16:33] 好\n- [16:33-16:34] 那么来\n- [16:34-16:35] 案例叫\n- [16:35-16:38] 也是抽象类的一个用法\n- [16:38-16:38] 好\n- [16:38-16:39] 首先\n- [16:39-16:40] 抽象类解释\n- [16:40-16:41] 我们刚才说了\n- [16:41-16:43] 抽象类在什么交怎么样\n- [16:43-16:44] 在开始中\n- [16:44-16:45] 然后呢\n- [16:45-16:46] 抽象类是\n- [16:46-16:46] 盗鱼\n- [16:46-16:47] 结果的\n- [16:47-16:48] 一回事啊\n- [16:48-16:49] 一回事\n- [16:49-16:49] 然后接下来\n- [16:49-16:50] 叫技\n- [16:50-16:51] 干嘛呢\n- [16:51-16:53] 有抽象方法的类\n- [16:53-16:54] 就是\n- [16:54-16:54] 哎\n- [16:54-16:55] 抽象类\n- [16:55-16:56] 也叫\n- [16:56-16:57] 也叫什么\n- [16:57-16:57] 结果\n- [16:57-16:58] 一模一\n- [16:58-16:59] 第二句话\n- [16:59-17:00] 第二句话\n- [17:00-17:01] 然后呢\n- [17:01-17:03] 抽象方法的鱼\n- [17:03-17:04] 没有方法\n- [17:05-17:06] 技\n- [17:06-17:07] 方法\n- [17:07-17:07] 技\n- [17:07-17:08] 是\n- [17:08-17:09] 是什么\n- [17:09-17:10] 是谁凶释的\n- [17:10-17:11] 怕死\n- [17:11-17:12] 就是\n- [17:12-17:13] 来了\n- [17:13-17:13] 那接下来\n- [17:13-17:16] 他能作用和目的一般大大的呢\n- [17:16-17:16] 就是\n- [17:16-17:17] 抽象类\n- [17:17-17:18] 一般\n- [17:18-17:19] 充当\n- [17:19-17:20] 傅类\n- [17:20-17:20] 啊\n- [17:20-17:21] 用于\n- [17:21-17:22] 啊\n- [17:22-17:22] 用于制定\n- [17:22-17:23] 制定什么\n- [17:23-17:24] 行业\n- [17:24-17:25] 规范\n- [17:25-17:26] 或者说\n- [17:26-17:27] 这个叫准则\n- [17:27-17:28] 啊\n- [17:28-17:29] 具体的实现\n- [17:29-17:30] 销油\n- [17:30-17:31] 具体的实现\n- [17:31-17:32] 实现\n- [17:32-17:33] 销油\n- [17:33-17:34] 销油谁呢\n- [17:34-17:35] 此类\n- [17:35-17:36] 来完成\n- [17:36-17:36] 就是此类\n- [17:36-17:37] 八千锅海\n- [17:37-17:38] 和写成功\n- [17:38-17:39] 还是那句话\n- [17:39-17:39] 比如空调\n- [17:39-17:40] 国家规定\n- [17:40-17:41] 啊\n- [17:41-17:41] 必须\n- [17:41-17:42] 怎么怎么样\n- [17:42-17:42] 你的\n- [17:42-17:43] 委屈排放量\n- [17:43-17:44] 达到什么什么\n- [17:44-17:45] 等等等等\n- [17:45-17:46] 这种情况\n- [17:46-17:46] 你达到了\n- [17:46-17:47] 就让你们\n- [17:47-17:48] 达不到就不让你们\n- [17:48-17:49] 那再来\n- [17:49-17:50] 比如说生产气说\n- [17:50-17:50] 那你必须\n- [17:50-17:51] 委屈\n- [17:51-17:52] 符合国六\n- [17:52-17:53] 那不符合国六\n- [17:53-17:53] 那\n- [17:53-17:54] 这个气说\n- [17:54-17:55] 就不让你少吃\n- [17:55-17:56] 怎么\n- [17:56-17:56] 那老师\n- [17:56-17:57] 已经有了\n- [17:57-17:57] 国三\n- [17:57-17:58] 或者国四的气收\n- [17:58-17:59] 怎么办\n- [17:59-17:59] 好\n- [17:59-18:00] 他有一定的年线\n- [18:00-18:02] 慢慢的那个国三\n- [18:02-18:02] 国四的气收\n- [18:02-18:04] 是不是也就不让上商务了\n- [18:04-18:05] 那比如说像\n- [18:05-18:07] 你看去一些机场啊\n- [18:07-18:08] 或者说去一些\n- [18:08-18:09] 这个别的地方\n- [18:09-18:10] 像正中那边\n- [18:10-18:12] 比如我经常往那个机场那边跑\n- [18:12-18:15] 像那边到那个路口那边\n- [18:15-18:17] 他上面就会挂一个盘\n- [18:17-18:18] 不允许国三的\n- [18:18-18:20] 或者说走这条路\n- [18:20-18:20] 就是走的话\n- [18:20-18:22] 就可能就\n- [18:22-18:22] 就空你前\n- [18:22-18:23] 或者怎么样\n- [18:23-18:24] 就罚款\n- [18:24-18:25] 或者傻力达力\n- [18:25-18:26] 就这种\n- [18:26-18:26] 那一看\n- [18:26-18:28] 现在算是追的是国六\n- [18:28-18:28] 当然在往后\n- [18:28-18:30] 过过五年或者十年\n- [18:30-18:31] 是不是那个时候\n- [18:31-18:31] 可能会国西\n- [18:31-18:32] 国八\n- [18:32-18:34] 国九是不是会有些新的标准啊\n- [18:34-18:35] 也有可能的\n- [18:35-18:35] 好了\n- [18:35-18:36] 那说回来\n- [18:36-18:37] 我们接着往下来地狼\n- [18:37-18:38] 来啦\n- [18:38-18:39] 来啦\n- [18:39-18:40] 第一步就是\n- [18:40-18:41] 第一什么\n- [18:41-18:42] 抽下来\n- [18:42-18:43] 叫空调力\n- [18:43-18:45] 那么设定\n- [18:45-18:46] 设定空调的\n- [18:46-18:49] 空调的一个什么规则\n- [18:49-18:49] 好\n- [18:49-18:50] 那接下来\n- [18:50-18:52] 克拉斯来个AC\n- [18:52-18:52] 骂好\n- [18:52-18:53] 来\n- [18:53-18:53] 1.1\n- [18:53-18:55] 叫赤鲁\n- [18:55-18:56] 叫赤鲁\n- [18:56-18:57] 赤鲁\n- [18:57-18:58] 那怎么赤鲁呢\n- [18:58-18:59] 我管你怎么知道\n- [18:59-18:59] 这个叫\n- [18:59-19:00] core\n- [19:00-19:01] core什么\n- [19:01-19:02] 偶应的\n- [19:02-19:03] 然后地板写上\n- [19:03-19:04] core\n- [19:04-19:05] 来得怕死\n- [19:05-19:05] 又说\n- [19:05-19:06] 这个是赤鲁\n- [19:06-19:07] 怎么赤鲁\n- [19:07-19:07] 我不管\n- [19:07-19:09] 反正你的有赤鲁\n- [19:09-19:09] 好\n- [19:09-19:10] 那接下来\n- [19:10-19:10] 1.2\n- [19:10-19:11] 1.2\n- [19:11-19:11] 是什么\n- [19:11-19:12] 赤鲁\n- [19:12-19:12] 赤鲁\n- [19:12-19:13] 我觉得你会了\n- [19:13-19:14] 或者他\n- [19:14-19:15] 命的\n- [19:15-19:15] 好\n- [19:15-19:15] 再来个\n- [19:15-19:16] 1.3\n- [19:16-19:17] 1.3\n- [19:17-19:18] 是左右\n- [19:18-19:19] 百分\n- [19:19-19:20] 说百分\n- [19:20-19:21] 百分\n- [19:21-19:22] 死命\n- [19:22-19:22] laptop\n- [19:22-19:23] on the right\n- [19:23-19:24] 你看\n- [19:24-19:25] 他说死命命的\n- [19:25-19:26] 那这个叫束缚\n- [19:26-19:27] 是吧\n- [19:27-19:27] 死命\n- [19:27-19:28] 我们来得laptop\n- [19:28-19:29] on the\n- [19:29-19:30] on the right\n- [19:30-19:32] 就是左右百分嘛\n- [19:32-19:32] 好那现在\n- [19:32-19:34] 我是不是已经把规则\n- [19:34-19:35] 也制定了\n- [19:35-19:35] 那接下来\n- [19:35-19:37] 就是你具体的实现\n- [19:37-19:37] 那为你个问题\n- [19:37-19:38] 来\n- [19:38-19:38] 第二\n- [19:38-19:39] 就是\n- [19:39-19:39] 干嘛呀\n- [19:39-19:40] 叫定义\n- [19:40-19:41] 死命\n- [19:41-19:42] 比如说\n- [19:42-19:42] 例如\n- [19:42-19:44] 再来个合力吧\n- [19:44-19:44] 他说有小命\n- [19:44-19:45] 那就小命吧\n- [19:45-19:46] 小命\n- [19:46-19:46] 命条\n- [19:46-19:47] 实现\n- [19:47-19:47] 腹肋\n- [19:47-19:48] 所有的出现方法\n- [19:48-19:49] 你看\n- [19:49-19:50] 我先来个\n- [19:50-19:50] class\n- [19:50-19:51] 销销\n- [19:51-19:52] 摸一笔\n- [19:52-19:53] 然后来个ac\n- [19:53-19:54] 但是这个地方\n- [19:54-19:55] 你看\n- [19:55-19:56] 我现在啥也不行\n- [19:56-19:58] 就会发现这件事\n- [19:58-19:59] 捏写都怕死\n- [19:59-20:00] 那么把肚子了\n- [20:00-20:01] 是不是说明\n- [20:01-20:01] 腹肋\n- [20:01-20:02] 提供了标准\n- [20:02-20:03] 紫薇\n- [20:03-20:05] 是不是没有提供距离实现啊\n- [20:05-20:05] 那么行\n- [20:05-20:06] 你是不是得实现他\n- [20:06-20:08] 怎么实现的\n- [20:08-20:08] 接下来\n- [20:08-20:09] 在这里边\n- [20:09-20:10] 我们来看一下\n- [20:10-20:11] 整个这个大阳\n- [20:11-20:12] 你可以把削盘\n- [20:12-20:14] 那几个动作给的到吧\n- [20:14-20:15] 可能谁全拉过来\n- [20:15-20:16] 拉过来以后\n- [20:16-20:16] 2.1\n- [20:16-20:17] 2.2\n- [20:17-20:18] 2.3\n- [20:18-20:19] 你是不是要把所有的功能\n- [20:19-20:20] 都得实现了\n- [20:20-20:21] 来吧\n- [20:21-20:22] 第一个\n- [20:22-20:24] 小命怎么知道啊\n- [20:25-20:25] 小冰块\n- [20:25-20:26] 提出的\n- [20:28-20:29] 核心\n- [20:29-20:31] 制作技术\n- [20:31-20:32] 好\n- [20:32-20:32] 那再来\n- [20:32-20:33] 那\n- [20:33-20:34] 否则\n- [20:34-20:35] 小命什么\n- [20:35-20:36] 制作啊\n- [20:36-20:38] 核心制作技术\n- [20:38-20:39] 你不能说家伙\n- [20:39-20:41] 家什么的不合适啊\n- [20:41-20:42] 小命\n- [20:42-20:43] 小命干嘛呢\n- [20:44-20:45] 核心\n- [20:45-20:46] 百合技术\n- [20:46-20:47] 百合又啥核心的\n- [20:47-20:48] 不就加个电机\n- [20:48-20:49] 摇头不就行了吗\n- [20:49-20:50] 对吧\n- [20:50-20:50] 核心\n- [20:50-20:51] 这\n- [20:51-20:52] 小命\n- [20:52-20:54] 小命共跳\n- [20:54-20:55] 到这个\n- [20:55-20:56] 会\n- [20:56-20:57] 对\n- [20:57-20:59] 静音\n- [20:59-21:00] 静音\n- [21:00-21:01] 左右\n- [21:01-21:02] 百封\n- [21:02-21:04] 百封\n- [21:04-21:05] 百合技术\n- [21:05-21:05] 行了\n- [21:05-21:06] 那接下来\n- [21:06-21:07] 我们开始\n- [21:07-21:08] 往下来聊了\n- [21:08-21:09] 那除了有小命一般\n- [21:09-21:10] 是不是还有什么美地啊\n- [21:10-21:11] 隔地啊\n- [21:11-21:11] 等等\n- [21:11-21:12] 除了有很多啊\n- [21:12-21:13] 那接下来呢\n- [21:13-21:14] 你看\n- [21:14-21:15] 这地方\n- [21:15-21:15] 我们来个\n- [21:15-21:16] 三\n- [21:16-21:17] 来个三\n- [21:17-21:17] 好\n- [21:17-21:18] 那么\n- [21:18-21:19] 这地方就是第一次\n- [21:19-21:19] 来\n- [21:19-21:20] 你看花围\n- [21:20-21:21] 花围有功能吗\n- [21:23-21:23] 这个\n- [21:23-21:24] 这个不清楚啊\n- [21:24-21:24] 来\n- [21:24-21:25] 我们看一下\n- [21:29-21:29] 行动\n- [21:30-21:32] 花围真有吗\n- [21:34-21:35] 好像\n- [21:35-21:35] 完出来就有\n- [21:35-21:37] 哎呦\n- [21:37-21:38] 哎呦\n- [21:39-21:40] 花围真是\n- [21:41-21:42] 花围几房\n- [21:43-21:44] 兄弟\n- [21:44-21:44] 呦\n- [21:47-21:48] 给服务器用的完成\n- [21:49-21:49] 算了\n- [21:49-21:50] 他换一个\n- [21:50-21:50] 换一个隔地的\n- [21:50-21:51] 不就点那个啊\n- [21:51-21:52] 隔地\n- [21:53-21:53] 隔地是\n- [21:53-21:54] 隔地\n- [21:54-21:55] 对吧\n- [21:55-21:55] 我们叫\n- [21:55-21:56] 鸡啊\n- [21:56-21:56] 一\n- [21:56-21:57] 啊\n- [21:57-21:57] 隔地吗\n- [21:57-21:58] 隔地\n- [21:58-21:59] 隔地\n- [21:59-21:59] 这\n- [21:59-22:01] 你叫隔地也行\n- [22:01-22:02] 那上面叫小命\n- [22:02-22:03] 前面叫隔地\n- [22:03-22:03] 那来了\n- [22:03-22:04] 这就是\n- [22:04-22:05] 三\n- [22:05-22:06] 三\n- [22:06-22:06] 三\n- [22:06-22:07] 来\n- [22:07-22:08] 三二一\n- [22:08-22:08] 不是小命\n- [22:08-22:09] 隔地之道\n- [22:09-22:10] 而是什么\n- [22:11-22:12] 隔地和隔地之道\n- [22:12-22:12] 啊\n- [22:12-22:13] 隔地和隔地\n- [22:13-22:14] 他的他\n- [22:14-22:15] 隔地同胶\n- [22:15-22:16] 然后干嘛\n- [22:17-22:18] 什么做百奉\n- [22:20-22:21] 不能这么干啊\n- [22:21-22:22] 都是咱过咱们盘子\n- [22:22-22:23] 低频吗\n- [22:23-22:25] 把低频所有百奉进行\n- [22:25-22:25] 好\n- [22:25-22:26] 那么\n- [22:26-22:26] 说话\n- [22:26-22:27] 那近代\n- [22:27-22:28] 请问\n- [22:28-22:28] 第四部\n- [22:28-22:29] 是不是有开始\n- [22:29-22:30] 我们那个叫他了\n- [22:30-22:31] 找测试\n- [22:31-22:32] 怎么测试啊\n- [22:32-22:33] 我们\n- [22:33-22:33] 来\n- [22:33-22:34] 首先\n- [22:34-22:34] 第1个\n- [22:34-22:35] 4.1\n- [22:35-22:35] 啊\n- [22:35-22:36] 4.1\n- [22:36-22:36] 干嘛呢\n- [22:36-22:38] 叫小命同胶\n- [22:39-22:39] 不像\n- [22:39-22:40] 啊\n- [22:40-22:40] 怎么\n- [22:40-22:41] 怎么写啊\n- [22:41-22:42] 相比等约\n- [22:42-22:43] 相比\n- [22:43-22:44] 要相比点什么\n- [22:44-22:45] 点扩不冷风\n- [22:45-22:47] 相比点扩等着风\n- [22:47-22:48] 相比点\n- [22:48-22:49] 此为左百方\n- [22:49-22:50] 不能的减号成立\n- [22:50-22:52] 暂时二三几\n- [22:52-22:52] 是吧\n- [22:52-22:53] 来\n- [22:53-22:54] 4.2\n- [22:55-22:55] 来\n- [22:55-22:56] 隔地同胶\n- [22:56-22:58] 后面的代码什么都是一样的\n- [22:58-22:59] 我快解剑了\n- [22:59-23:00] 那设成了\n- [23:00-23:01] 所以你会发现啊\n- [23:01-23:02] 整个的这对\n- [23:02-23:04] 那不同的成商\n- [23:04-23:06] 用的技术是不是不一样\n- [23:06-23:07] 但是你的成商\n- [23:07-23:08] 是不是必须得符合\n- [23:08-23:10] 我这边的规范啊\n- [23:10-23:11] 有人说老师我丢一个\n- [23:11-23:12] 那丢一个丢一个为\n- [23:12-23:14] 比如说小命在这里边\n- [23:14-23:15] 不会左右百风\n- [23:15-23:16] 你看\n- [23:16-23:16] 小命在这儿\n- [23:16-23:18] 是不是不会左右百风啊\n- [23:18-23:19] 又见这形了\n- [23:19-23:20] 这个跟你讲的不一样\n- [23:20-23:21] 因为你会发现\n- [23:21-23:22] 这个地方不报错\n- [23:22-23:23] 是不是正常运行啊\n- [23:23-23:25] 啥吧是报错\n- [23:25-23:26] 啥吧是啥呢\n- [23:26-23:28] 就是负类有几个规则\n- [23:28-23:29] 你的此类\n- [23:29-23:30] 必须有几个规则\n- [23:30-23:31] 比如说负类你看\n- [23:31-23:32] 在这里面\n- [23:32-23:33] 负类是为有三规则\n- [23:33-23:34] 那你的此类\n- [23:34-23:36] 必须有几个规则\n- [23:36-23:37] 必须有三\n- [23:37-23:38] 如果没有就报错\n- [23:38-23:39] 当然开始就没有\n- [23:39-23:41] 他只是说\n- [23:41-23:42] 你要这么去划\n- [23:42-23:43] 而你这个动作\n- [23:43-23:44] 要注意\n- [23:44-23:45] 随着我把你工作\n- [23:45-23:46] 在这里打开\n- [23:46-23:47] 那你的此类呢\n- [23:47-23:49] 要去处够付类\n- [23:49-23:51] 所有的这些抽象方法\n- [23:51-23:53] 那么这就是关于抽下类的\n- [23:53-23:54] 一个用法\n- [23:54-23:55] 大家有用发现\n- [23:55-23:57] 就是充当什么来用的\n- [23:57-23:59] 充当付类充当\n- [23:59-24:01] 规则来用的\n- [24:01-24:01] 具体的实现\n- [24:01-24:02] 是不是交给此类的\n- [24:02-24:03] 什么\n- [24:03-24:04] 这个了解下去\n- [24:04-24:05] 所以就没发现\n- [24:05-24:06] 面向对上部分\n- [24:06-24:08] 我虽然写了很多代码\n- [24:08-24:10] 但是绝大多数都不需要理念\n- [24:10-24:12] 你只需要把这个代码\n- [24:12-24:14] 比较大的含义和它的一个用法\n- [24:14-24:16] 是不是理解就可以了\n- [24:16-24:17] 很多都不需要你去写的\n- [24:17-24:19] 随这个要注意\n- [24:19-24:21] 来落地了\n- [24:21-24:22] 找同学\n- [24:22-24:23] 找同学打得好\n- [24:23-24:24] 再就结束\n- [24:24-24:24] 打得不好\n- [24:24-24:28] 我就再讲一个\n- [24:28-24:29] 有没有似到分用了\n- [24:29-24:31] 就是不是几个了\n- [24:33-24:34] 12.9分\n- [24:34-24:35] 第九层的\n- [24:35-24:35] 第九层没有\n- [24:35-24:36] 第十层\n- [24:36-24:36] 来\n- [24:36-24:42] 6号\n- [24:42-24:47] 什么是多态\n- [24:47-24:53] 声稍微大点\n- [24:53-24:56] 拍战中真的是同一个喊数\n- [24:56-24:57] 接收不同的\n- [24:57-24:58] 对象\n- [24:58-25:00] 然后有不同的效果\n- [25:00-25:01] 大改化就是\n- [25:01-25:02] 同一个事务\n- [25:02-25:02] 不同环境\n- [25:02-25:03] 有不同效果\n- [25:03-25:04] 有点水\n- [25:04-25:05] 高温低温\n- [25:05-25:06] 长温挺好的\n- [25:06-25:06] OK\n- [25:06-25:08] 那接下来在我身边\n- [25:08-25:14] 实现多态的条件\n- [25:14-25:18] 有基层\n- [25:18-25:19] 有重写\n- [25:19-25:20] 如果没有重写的话\n- [25:20-25:21] 就直接玩复类的\n- [25:21-25:22] 就没有意义了\n- [25:22-25:26] 第三个是有\n- [25:26-25:27] Animo\n- [25:27-25:28] An等于开谈\n- [25:28-25:30] 这个是撒牌上来着\n- [25:30-25:31] 有复类应用\n- [25:31-25:32] 指向\n- [25:32-25:32] 指向\n- [25:32-25:34] 但是如果大家\n- [25:34-25:35] 真正意义上\n- [25:35-25:36] 你去扣这个点\n- [25:36-25:37] 你会发现一件事\n- [25:37-25:38] 这三条件\n- [25:38-25:40] 哪怕我一个都不把足\n- [25:40-25:41] 那个喊数\n- [25:41-25:43] 是不是也能时间咱要的效果\n- [25:43-25:43] 喊那句话\n- [25:43-25:44] 拍战中\n- [25:44-25:45] 它是萎多态\n- [25:45-25:47] 因为它是弱类型的\n- [25:47-25:47] 它做不了\n- [25:47-25:48] 像炸网那样\n- [25:48-25:49] 精准的限定\n- [25:49-25:50] 正细节戴上主义\n- [25:50-25:51] 来\n- [25:51-25:53] 多态的好处\n- [25:53-25:54] 这就两个\n- [25:57-25:57] 节奏好\n- [25:57-25:58] 节奏好处\n- [25:58-25:59] 多大的好处\n- [25:59-26:00] 其实就是实现\n- [26:00-26:01] 节奏合的\n- [26:01-26:02] 实现软件的扩展\n- [26:02-26:03] 那下来\n- [26:03-26:04] 同样的一个平台\n- [26:04-26:06] 你是不是传入什么对象\n- [26:06-26:06] 我是说\n- [26:06-26:08] 下来就会有对应的一个效果\n- [26:08-26:10] 这个知道就行了\n- [26:10-26:10] 好 OK\n- [26:10-26:11] 那关于多态\n- [26:11-26:13] 我们就聊到这个地方\n- [26:13-26:13] 好\n- [26:13-26:15] 那么我把这个支点\n- [26:15-26:16] 往前一下",
      "resultMd": "## 目录\n\n- [核心概念摘要 (Key Takeaways)](#核心概念摘要-key-takeaways)\n- [知识点详解 (Detailed Notes)](#知识点详解-detailed-notes)\n  - [1. 面向对象三大特性的演进关系](#1-面向对象三大特性的演进关系)\n  - [2. 多态的核心价值：解耦与扩展性](#2-多态的核心价值解耦与扩展性)\n  - [3. 抽象类：制定行业标准的设计模式](#3-抽象类制定行业标准的设计模式)\n- [代码实战 / 算法推导 (Code & Math)](#代码实战--算法推导-code--math)\n  - [空调抽象类案例完整实现](#空调抽象类案例完整实现)\n- [避坑指南 & 常见QA (Tips & QA)](#避坑指南--常见qa-tips--qa)\n\n---\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **多态的本质**：同一接口接收不同对象，产生不同行为效果，实现\"机制不变，内容可变\"的扩展架构\n2. **三大特性演进链**：封装（解决安全）→ 继承（解决复用）→ 多态（解决扩展），层层递进又各有限制\n3. **抽象类的定位**：充当父类制定标准，强制子类实现规范，类比\"国家标准/行业规范\"\n4. **Python多态特性**：弱类型语言，无需严格满足三条件（继承、重写、父类引用指向子类对象）即可实现多态效果，但无法像Java那样精准限定类型\n5. **设计模式启示**：游戏对战平台、USB接口等案例体现\"顶层定规则，底层自由实现\"的工程思想\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 1. 面向对象三大特性的演进关系\n\n| 特性 | 核心解决的问题 | 带来的好处 | 存在的弊端 |\n|:---|:---|:---|:---|\n| **封装** | 数据安全性 | 安全性（私有属性）、复用性（getter/setter） | 代码量增加 |\n| **继承** | 代码重复问题 | 提高代码复用性，子类直接复用父类属性和方法 | 耦合性增强（父类功能强制传递给子类） |\n| **多态** | 系统扩展性问题 | 解耦、可扩展性（平台不变，内容可变） | Python中无法精准限定类型 |\n\n> **讲师比喻**：继承是\"还老爹的东西\"，多态是\"给老爹长脸\"——在不修改原有平台的情况下，让新加入的内容按统一规则运行。\n\n### 2. 多态的核心价值：解耦与扩展性\n\n**王者荣耀案例解析**：\n- 地图机制（刷野、防御塔、兵线）= **固定平台/父类框架**\n- 100+英雄 = **不同的子类实现**\n- 新英雄上线（如\"元流之子\"）= **新增子类，无需改动地图代码**\n\n关键洞察：**机制是一套，至于传过来的是谁，机制不会随内容改变而改变**。这正是软件设计中\"对扩展开放，对修改封闭\"（开闭原则）的体现。\n\n**多态的三条件（理论层面）**：\n1. 存在继承关系\n2. 子类重写父类方法\n3. 父类引用指向子类对象：`Animal a = new Dog()`\n\n> **Python特殊性**：作为弱类型语言，即使不满足上述三条件，也能实现多态效果，但牺牲了类型安全性。\n\n### 3. 抽象类：制定行业标准的设计模式\n\n**核心定义**：\n- **抽象方法**：没有方法体的方法（`pass`占位）\n- **抽象类**：包含抽象方法的类\n- **普通类**：不包含抽象方法的类\n\n**设计哲学类比**：\n\n| 现实场景 | 抽象类角色 | 子类角色 |\n|:---|:---|:---|\n| 汽车排放标准 | 国家制定的\"国六标准\" | 各车企的具体技术方案 |\n| 空调行业规范 | 必须具备制冷/制热/摆风功能 | 格力、小米、美的各自的专利技术 |\n| USB接口协议 | 统一的物理尺寸和数据规范 | 鼠标、键盘、U盘等具体设备 |\n\n**关键约束**：子类必须实现父类**所有**抽象方法，否则报错。这保证了\"符合规范才能上市\"的强制性。\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### 空调抽象类案例完整实现\n\n```python\nfrom abc import ABC, abstractmethod\n\n\n# ==================== 第一步：定义抽象类（制定行业标准）====================\nclass AC(ABC):\n    \"\"\"\n    空调抽象类 - 充当父类，设定行业规范\n    类比：国家规定的空调必须满足的功能标准\n    \"\"\"\n    \n    @abstractmethod\n    def cool_wind(self):\n        \"\"\"制冷功能 - 抽象方法，无具体实现\"\"\"\n        pass\n    \n    @abstractmethod\n    def hot_wind(self):\n        \"\"\"制热功能 - 抽象方法，无具体实现\"\"\"\n        pass\n    \n    @abstractmethod\n    def swing_l_r(self):\n        \"\"\"左右摆风功能 - 抽象方法，无具体实现\"\"\"\n        pass\n\n\n# ==================== 第二步：具体厂商实现（各自的技术方案）====================\nclass XiaoMi(AC):\n    \"\"\"\n    小米空调 - 具体实现类\n    采用\"小冰块\"核心制冷技术、\"静音百合\"摆风技术\n    \"\"\"\n    \n    def cool_wind(self):\n        # 小米特有的制冷技术实现\n        print(\"小米空调：小冰块核心制冷技术启动\")\n    \n    def hot_wind(self):\n        print(\"小米空调：核心制热技术启动\")\n    \n    def swing_l_r(self):\n        # 静音电机摇头方案\n        print(\"小米空调：静音百合左右摆风\")\n\n\nclass Gree(AC):\n    \"\"\"\n    格力空调 - 具体实现类\n    采用\"低频\"相关技术（讲师提及但未详细展开）\n    \"\"\"\n    \n    def cool_wind(self):\n        print(\"格力空调：核心制冷技术启动\")\n    \n    def hot_wind(self):\n        print(\"格力空调：核心制热技术启动\")\n    \n    def swing_l_r(self):\n        print(\"格力空调：左右摆风功能启动\")\n\n\n# ==================== 第三步：测试验证 ====================\ndef test_ac(ac: AC):\n    \"\"\"\n    统一测试接口 - 体现多态性\n    同一函数，传入不同品牌空调，执行各自实现\n    \"\"\"\n    print(f\"\\n{'='*20} 测试 {type(ac).__name__} {'='*20}\")\n    ac.cool_wind()\n    ac.hot_wind()\n    ac.swing_l_r()\n\n\nif __name__ == \"__main__\":\n    # 4.1 测试小米空调\n    xiaomi = XiaoMi()\n    test_ac(xiaomi)\n    \n    # 4.2 测试格力空调\n    gree = Gree()\n    test_ac(gree)\n    \n    # 错误示范：若子类未实现全部抽象方法，实例化时报错\n    # class BadAC(AC):\n    #     def cool_wind(self):\n    #         pass\n    #     # 缺少 hot_wind 和 swing_l_r 的实现\n    # \n    # bad = BadAC()  # TypeError: Can't instantiate abstract class\n```\n\n**输出结果**：\n```\n==================== 测试 XiaoMi ====================\n小米空调：小冰块核心制冷技术启动\n小米空调：核心制热技术启动\n小米空调：静音百合左右摆风\n\n==================== 测试 Gree ====================\n格力空调：核心制冷技术启动\n格力空调：核心制热技术启动\n格力空调：左右摆风功能启动\n```\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n| 问题 | 解答 |\n|:---|:---|\n| **Q：抽象类和普通类的本质区别？** | 抽象类包含抽象方法（无方法体），不能直接实例化；普通类可以实例化。有抽象方法的类必须是抽象类。 |\n| **Q：子类可以只实现部分抽象方法吗？** | ❌ 不可以。Python会强制要求实现**所有**抽象方法，否则会抛出`TypeError`。 |\n| **Q：Python的多态为什么叫\"伪多态\"？** | Python是弱类型语言，不需要显式声明类型，运行时动态绑定。这带来灵活性，但也导致无法像Java那样在编译期精准限定类型（如传入`Car`对象到期望`Animal`的位置不会提前报错）。 |\n| **Q：抽象类 vs 接口的区别？** | 在Python中二者合一（通过`abc`模块实现）。Java/C++中区分明显：抽象类可有成员变量和具体方法，接口早期只能有抽象方法（Java 8+支持默认方法）。 |\n| **易错点：忘记导入`abc`模块** | 需`from abc import ABC, abstractmethod`，并令抽象类继承`ABC`。 |\n| **易错点：抽象方法必须有`@abstractmethod`装饰器** | 仅写`pass`不构成抽象方法，子类不强制重写。 |\n| **设计建议：何时使用抽象类？** | 当需要\"强制规范 + 灵活实现\"的场景：框架设计、插件系统、硬件驱动接口等。 |",
      "resolvedTitle": "第二章-面向对象高级-08.抽象类案例_空调案例",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_7a45f9396dad",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1MT411x7GH?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=3",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T16:16:41.943Z",
      "updatedAt": "2026-02-15T16:19:37.055Z",
      "resultMd": "```markdown\n# Kubernetes 实战进阶、运维管理及 DevOps 课程模块解读\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [一、集群搭建方案对比](#一集群搭建方案对比)\n  - [二、Pod 深入理解](#二pod-深入理解)\n  - [三、资源调度核心组件](#三资源调度核心组件)\n  - [四、服务发布与网络](#四服务发布与网络)\n  - [五、存储与配置管理](#五存储与配置管理)\n  - [六、高级调度与认证授权](#六高级调度与认证授权)\n  - [七、运维管理篇](#七运维管理篇)\n  - [八、DevOps 实战项目](#八devops-实战项目)\n- [避坑指南 & 常见QA](#避坑指南--常见qa)\n\n---\n\n## 核心概念摘要\n\n1. **K8s 集群搭建的\"学习悖论\"**：二进制安装最繁琐但最利于理解架构，命令行工具最便捷但隐藏底层细节——建议初学者先难后易。\n2. **Pod 是 K8s 的最小调度单元**，探针（Probe）和生命周期钩子是其高可用设计的核心机制。\n3. **Label + Selector 是 K8s 的\"导航系统\"**，Deployment/StatefulSet/DaemonSet 分别对应无状态、有状态、守护进程三类工作负载。\n4. **Service + Ingress 解决了容器网络的两大难题**：集群内部通信与外部流量入口。\n5. **Helm 是 K8s 的包管理器**，类比 Maven/NPM/YUM，实现应用的快速部署与版本管理。\n\n---\n\n## 知识点详解\n\n### 一、集群搭建方案对比\n\n| 方案 | 特点 | 适用场景 | 生产环境建议 |\n|:---|:---|:---|:---|\n| **二进制安装** | 手动编译组件，深度理解架构 | 学习、定制化需求 | ⭐⭐⭐ 推荐（熟悉后） |\n| **kubeadm** | 官方半自动化工具 | 快速搭建测试环境 | ⭐⭐☆ 谨慎使用 |\n| **命令行工具**（如 kind/minikube） | 一键安装，极简操作 | 本地开发、CI/CD | ⭐☆☆ 不推荐生产 |\n| **云厂商托管**（ACK/EKS/GKE） | 全托管，免运维 | 企业生产环境 | ⭐⭐⭐ 强烈推荐 |\n\n> **讲师观点**：二进制安装的\"麻烦\"恰恰是价值所在——亲手经历 etcd、apiserver、scheduler、controller-manager 的部署过程，才能建立对 K8s 架构的系统性认知。遇到问题时，你能直接定位到具体组件而非盲目排查。\n\n**API 与 kubectl 的关系**：\n```\n用户命令 (kubectl) → REST API 调用 → etcd 状态变更 → 控制器协调 → 实际执行\n```\n\n---\n\n### 二、Pod 深入理解\n\n#### 2.1 探针（Probe）—— 容器的\"健康监护仪\"\n\n| 探针类型 | 作用时机 | 典型用途 |\n|:---|:---|:---|\n| `LivenessProbe` | 容器运行期间持续检测 | 应用卡死时自动重启容器 |\n| `ReadinessProbe` | 容器启动后检测 | 确保流量只打到就绪实例 |\n| `StartupProbe` | 容器启动初期检测 | 保护慢启动应用不被误杀 |\n\n**核心价值**：实现故障自愈——服务异常时自动重启，无需人工介入。\n\n#### 2.2 生命周期钩子（Lifecycle Hooks）\n\n```yaml\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n    - name: app\n      lifecycle:\n        postStart:      # 容器创建后立即执行\n          exec:\n            command: [\"/bin/sh\", \"-c\", \"echo 'Init done' >> /var/log/app.log\"]\n        preStop:        # 容器终止前执行（优雅停机）\n          exec:\n            command: [\"/bin/sh\", \"-c\", \"sleep 10 && kill -SIGTERM 1\"]\n```\n\n**应用场景**：\n- `postStart`：预热缓存、注册服务发现\n- `preStop`：完成当前请求、注销服务、释放连接池\n\n---\n\n### 三、资源调度核心组件\n\n#### 3.1 Label 与 Selector —— \"标签-选择器\"机制\n\n类比：给行李箱贴标签（Label），通过标签快速筛选（Selector）。\n\n```yaml\n# 打标签\nmetadata:\n  labels:\n    app: nginx\n    tier: frontend\n    env: production\n\n# 选择器匹配\nselector:\n  matchLabels:\n    app: nginx\n    env: production\n```\n\n#### 3.2 工作负载控制器对比\n\n| 控制器 | 适用场景 | 关键特性 |\n|:---|:---|:---|\n| **Deployment** | 无状态应用（Web服务、API） | 滚动更新、回滚、扩缩容、暂停/恢复 |\n| **StatefulSet** | 有状态应用（MySQL、Redis、Kafka） | 稳定网络标识、有序部署、持久化存储绑定 |\n| **DaemonSet** | 节点级守护进程（日志采集、监控Agent） | 每节点一个副本，随节点自动扩缩 |\n| **HPA** | 自动弹性伸缩 | 基于 CPU/内存/自定义指标自动调整副本数 |\n\n> **有状态 vs 无状态**：MySQL 数据不能丢、需要固定身份（Pod 名、网络标识）→ StatefulSet；Nginx 配置相同、随时可替换 → Deployment。\n\n---\n\n### 四、服务发布与网络\n\n#### 4.1 两大核心技术解决网络问题\n\n| 层级 | 组件 | 解决问题 |\n|:---|:---|:---|\n| **集群内部** | Service（ClusterIP/NodePort/LoadBalancer） | Pod IP 动态变化时的服务发现与负载均衡 |\n| **集群外部** | Ingress | HTTP/HTTPS 路由、域名解析、SSL 终止、限流熔断 |\n\n**演进路径**：\n```\nDocker 时代：容器间通信困难 → 端口映射混乱\n    ↓\nK8s Service：抽象出稳定的虚拟 IP（ClusterIP），自动后端端点维护\n    ↓\nIngress：七层路由，类似 Nginx 的配置即代码\n```\n\n---\n\n### 五、存储与配置管理\n\n#### 5.1 ConfigMap / Secret —— 动态配置注入\n\n痛点解决：修改配置文件无需重新构建镜像、滚动重启 Pod。\n\n```yaml\n# 配置与镜像解耦\nenvFrom:\n  - configMapRef:\n      name: app-config    # 热更新需配合 volume 挂载\nvolumeMounts:\n  - name: config-vol\n    mountPath: /etc/config\nvolumes:\n  - name: config-vol\n    configMap:\n      name: app-config    # 文件变更后自动同步（约 ~1min 延迟）\n```\n\n#### 5.2 持久化存储（PV/PVC/StorageClass）\n\n**核心链条**：\n```\n应用声明需求 (PVC) → 动态供应 (StorageClass + Provisioner) → 后端存储 (NFS/Ceph/云盘)\n         ↑___________________________________________________________|\n                              回收策略控制\n```\n\n**关键原则**：有状态应用的数据必须持久化，MySQL 数据丢失 = 服务不可用。\n\n---\n\n### 六、高级调度与认证授权\n\n#### 6.1 高级调度特性\n\n- **亲和性/反亲和性**：将 Pod 调度到指定节点或分散部署\n- **污点与容忍**：专用节点隔离（如 GPU 节点、SSD 节点）\n- **优先级与抢占**：资源紧张时保障核心服务\n\n**规模化运维价值**：数百台节点的应用部署，一条命令完成全局配置。\n\n#### 6.2 RBAC 认证授权\n\n| 概念 | 说明 |\n|:---|:---|\n| Role/ClusterRole | 定义权限规则（对哪些资源做什么操作） |\n| RoleBinding/ClusterRoleBinding | 将权限绑定到用户/组/ServiceAccount |\n| ServiceAccount | Pod 访问 API Server 的身份凭证 |\n\n> 控制台界面操作时必须理解 RBAC，否则易出现权限泄露或功能受限。\n\n---\n\n### 七、运维管理篇\n\n| 模块 | 工具/方案 | 核心能力 |\n|:---|:---|:---|\n| **包管理器** | Helm | Chart 打包、版本管理、依赖解析、一键安装/升级/回滚 |\n| **集群监控** | Prometheus + Grafana | 多维度指标采集、告警规则、可视化大盘 |\n| **日志管理** | EFK/Loki | 统一收集、检索、分析分布式日志 |\n| **可视化界面** | Dashboard/Rancher/KubeSphere | Web UI 替代命令行，降低运维门槛 |\n\n**学习路径建议**：先精通 kubectl 命令，再使用控制台——\"先难后易\"才能真正理解 K8s 在做什么。\n\n---\n\n### 八、DevOps 实战项目\n\n基于阿里云 ACK 的微服务完整流水线：\n\n```\n代码提交 (GitLab) \n    ↓\n代码质量检查 (SonarQube) —— 漏洞扫描、覆盖率、规范审查\n    ↓\n镜像构建与推送 (Docker Registry)\n    ↓\n流水线编排 (Jenkins/GitLab CI) —— 自动化测试、环境部署\n    ↓\nK8s 集群部署 (Helm/原生 YAML)\n    ↓\n服务网格治理 (可选：Istio)\n```\n\n**涉及技术栈**：Docker、GitLab、SonarQube、Jenkins、Helm、ACK。\n\n---\n\n## 避坑指南 & 常见QA\n\n| 问题 | 解答/建议 |\n|:---|:---|\n| **kubeadm 能否用于生产？** | 官方不建议。可能存在稳定性隐患，小规模可用，大规模推荐二进制或云托管。 |\n| **为什么先学命令行而不是直接用 Dashboard？** | 控制台隐藏了底层逻辑，遇到问题无法排查。\"先难后易\"才能建立真正的系统认知。 |\n| **StatefulSet 的 Pod 能随便删除吗？** | 可以，但会按序号重建。需注意：① 持久卷不会删除 ② 网络标识保持不变。 |\n| **ConfigMap 修改后为何不立即生效？** | 环境变量方式注入的不会热更新；Volume 挂载方式有约 1 分钟延迟。 |\n| **HPA 扩容不及时怎么办？** | 考虑自定义指标（如 QPS）、降低采样周期、或配合 Cluster Autoscaler 扩节点。 |\n| **Helm 与直接 apply YAML 的区别？** | Helm 提供版本管理、参数化模板、依赖管理，适合复杂应用的生命周期管理。 |\n```",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_3ca5bd6a7d95",
      "status": "cancelled",
      "stage": "local_transcribe",
      "progress": 15,
      "message": "任务已取消",
      "sourceUrl": "https://www.bilibili.com/video/BV1MT411x7GH?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=3",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T16:15:42.869Z",
      "updatedAt": "2026-02-15T16:15:54.394Z",
      "cancelReason": "user_cancelled"
    },
    {
      "id": "task_00031573b855",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html",
      "sourceType": "web",
      "promptId": "prompt-mll-template-detailed",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T06:51:46.520Z",
      "updatedAt": "2026-02-15T06:53:07.872Z",
      "preparedMd": "# 网页抓取内容汇总\n\n## 网页来源 1\n\n原始链接：https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html\n网页标题：4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation\n\n4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation\n===============\n\n__\n\n[4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)_navigate\\_next_[4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n\n_search_ \n\n Quick search \n\n[_code_](https://zh.d2l.ai/_sources/chapter_multilayer-perceptrons/backprop.rst.txt)\n\n Show Source \n\n[MXNet](https://zh-v2.d2l.ai/d2l-zh.pdf)[PyTorch](https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf)[Jupyter 记事本](https://zh-v2.d2l.ai/d2l-zh.zip)[课程](https://courses.d2l.ai/zh-v2/)[GitHub](https://github.com/d2l-ai/d2l-zh)[English](https://d2l.ai/)\n\n[![Image 2: 动手学深度学习](https://zh.d2l.ai/_static/logo-with-text.png)](https://zh.d2l.ai/index.html)\n\nTable Of Contents\n*   [前言](https://zh.d2l.ai/chapter_preface/index.html)\n*   [安装](https://zh.d2l.ai/chapter_installation/index.html)\n*   [符号](https://zh.d2l.ai/chapter_notation/index.html)\n\n*   [1. 引言](https://zh.d2l.ai/chapter_introduction/index.html)\n*   [2. 预备知识](https://zh.d2l.ai/chapter_preliminaries/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [2.1. 数据操作](https://zh.d2l.ai/chapter_preliminaries/ndarray.html)\n    *   [2.2. 数据预处理](https://zh.d2l.ai/chapter_preliminaries/pandas.html)\n    *   [2.3. 线性代数](https://zh.d2l.ai/chapter_preliminaries/linear-algebra.html)\n    *   [2.4. 微积分](https://zh.d2l.ai/chapter_preliminaries/calculus.html)\n    *   [2.5. 自动微分](https://zh.d2l.ai/chapter_preliminaries/autograd.html)\n    *   [2.6. 概率](https://zh.d2l.ai/chapter_preliminaries/probability.html)\n    *   [2.7. 查阅文档](https://zh.d2l.ai/chapter_preliminaries/lookup-api.html)\n\n*   [3. 线性神经网络](https://zh.d2l.ai/chapter_linear-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [3.1. 线性回归](https://zh.d2l.ai/chapter_linear-networks/linear-regression.html)\n    *   [3.2. 线性回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)\n    *   [3.3. 线性回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html)\n    *   [3.4. softmax回归](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html)\n    *   [3.5. 图像分类数据集](https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html)\n    *   [3.6. softmax回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)\n    *   [3.7. softmax回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html)\n\n*   [4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [4.1. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html)\n    *   [4.2. 多层感知机的从零开始实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html)\n    *   [4.3. 多层感知机的简洁实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html)\n    *   [4.4. 模型选择、欠拟合和过拟合](https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html)\n    *   [4.5. 权重衰减](https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html)\n    *   [4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)\n    *   [4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)\n    *   [4.9. 环境和分布偏移](https://zh.d2l.ai/chapter_multilayer-perceptrons/environment.html)\n    *   [4.10. 实战Kaggle比赛：预测房价](https://zh.d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html)\n\n*   [5. 深度学习计算](https://zh.d2l.ai/chapter_deep-learning-computation/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [5.1. 层和块](https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html)\n    *   [5.2. 参数管理](https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html)\n    *   [5.3. 延后初始化](https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html)\n    *   [5.4. 自定义层](https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html)\n    *   [5.5. 读写文件](https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html)\n    *   [5.6. GPU](https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html)\n\n*   [6. 卷积神经网络](https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [6.1. 从全连接层到卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html)\n    *   [6.2. 图像卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)\n    *   [6.3. 填充和步幅](https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)\n    *   [6.4. 多输入多输出通道](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)\n    *   [6.5. 汇聚层](https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html)\n    *   [6.6. 卷积神经网络（LeNet）](https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html)\n\n*   [7. 现代卷积神经网络](https://zh.d2l.ai/chapter_convolutional-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [7.1. 深度卷积神经网络（AlexNet）](https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html)\n    *   [7.2. 使用块的网络（VGG）](https://zh.d2l.ai/chapter_convolutional-modern/vgg.html)\n    *   [7.3. 网络中的网络（NiN）](https://zh.d2l.ai/chapter_convolutional-modern/nin.html)\n    *   [7.4. 含并行连结的网络（GoogLeNet）](https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html)\n    *   [7.5. 批量规范化](https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html)\n    *   [7.6. 残差网络（ResNet）](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html)\n    *   [7.7. 稠密连接网络（DenseNet）](https://zh.d2l.ai/chapter_convolutional-modern/densenet.html)\n\n*   [8. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [8.1. 序列模型](https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html)\n    *   [8.2. 文本预处理](https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)\n    *   [8.3. 语言模型和数据集](https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)\n    *   [8.4. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n    *   [8.5. 循环神经网络的从零开始实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)\n    *   [8.6. 循环神经网络的简洁实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html)\n    *   [8.7. 通过时间反向传播](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)\n\n*   [9. 现代循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [9.1. 门控循环单元（GRU）](https://zh.d2l.ai/chapter_recurrent-modern/gru.html)\n    *   [9.2. 长短期记忆网络（LSTM）](https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)\n    *   [9.3. 深度循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/deep-rnn.html)\n    *   [9.4. 双向循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/bi-rnn.html)\n    *   [9.5. 机器翻译与数据集](https://zh.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html)\n    *   [9.6. 编码器-解码器架构](https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html)\n    *   [9.7. 序列到序列学习（seq2seq）](https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html)\n    *   [9.8. 束搜索](https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html)\n\n*   [10. 注意力机制](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [10.1. 注意力提示](https://zh.d2l.ai/chapter_attention-mechanisms/attention-cues.html)\n    *   [10.2. 注意力汇聚：Nadaraya-Watson 核回归](https://zh.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)\n    *   [10.3. 注意力评分函数](https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)\n    *   [10.4. Bahdanau 注意力](https://zh.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)\n    *   [10.5. 多头注意力](https://zh.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)\n    *   [10.6. 自注意力和位置编码](https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)\n    *   [10.7. Transformer](https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html)\n\n*   [11. 优化算法](https://zh.d2l.ai/chapter_optimization/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [11.1. 优化和深度学习](https://zh.d2l.ai/chapter_optimization/optimization-intro.html)\n    *   [11.2. 凸性](https://zh.d2l.ai/chapter_optimization/convexity.html)\n    *   [11.3. 梯度下降](https://zh.d2l.ai/chapter_optimization/gd.html)\n    *   [11.4. 随机梯度下降](https://zh.d2l.ai/chapter_optimization/sgd.html)\n    *   [11.5. 小批量随机梯度下降](https://zh.d2l.ai/chapter_optimization/minibatch-sgd.html)\n    *   [11.6. 动量法](https://zh.d2l.ai/chapter_optimization/momentum.html)\n    *   [11.7. AdaGrad算法](https://zh.d2l.ai/chapter_optimization/adagrad.html)\n    *   [11.8. RMSProp算法](https://zh.d2l.ai/chapter_optimization/rmsprop.html)\n    *   [11.9. Adadelta](https://zh.d2l.ai/chapter_optimization/adadelta.html)\n    *   [11.10. Adam算法](https://zh.d2l.ai/chapter_optimization/adam.html)\n    *   [11.11. 学习率调度器](https://zh.d2l.ai/chapter_optimization/lr-scheduler.html)\n\n*   [12. 计算性能](https://zh.d2l.ai/chapter_computational-performance/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [12.1. 编译器和解释器](https://zh.d2l.ai/chapter_computational-performance/hybridize.html)\n    *   [12.2. 异步计算](https://zh.d2l.ai/chapter_computational-performance/async-computation.html)\n    *   [12.3. 自动并行](https://zh.d2l.ai/chapter_computational-performance/auto-parallelism.html)\n    *   [12.4. 硬件](https://zh.d2l.ai/chapter_computational-performance/hardware.html)\n    *   [12.5. 多GPU训练](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus.html)\n    *   [12.6. 多GPU的简洁实现](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus-concise.html)\n    *   [12.7. 参数服务器](https://zh.d2l.ai/chapter_computational-performance/parameterserver.html)\n\n*   [13. 计算机视觉](https://zh.d2l.ai/chapter_computer-vision/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [13.1. 图像增广](https://zh.d2l.ai/chapter_computer-vision/image-augmentation.html)\n    *   [13.2. 微调](https://zh.d2l.ai/chapter_computer-vision/fine-tuning.html)\n    *   [13.3. 目标检测和边界框](https://zh.d2l.ai/chapter_computer-vision/bounding-box.html)\n    *   [13.4. 锚框](https://zh.d2l.ai/chapter_computer-vision/anchor.html)\n    *   [13.5. 多尺度目标检测](https://zh.d2l.ai/chapter_computer-vision/multiscale-object-detection.html)\n    *   [13.6. 目标检测数据集](https://zh.d2l.ai/chapter_computer-vision/object-detection-dataset.html)\n    *   [13.7. 单发多框检测（SSD）](https://zh.d2l.ai/chapter_computer-vision/ssd.html)\n    *   [13.8. 区域卷积神经网络（R-CNN）系列](https://zh.d2l.ai/chapter_computer-vision/rcnn.html)\n    *   [13.9. 语义分割和数据集](https://zh.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html)\n    *   [13.10. 转置卷积](https://zh.d2l.ai/chapter_computer-vision/transposed-conv.html)\n    *   [13.11. 全卷积网络](https://zh.d2l.ai/chapter_computer-vision/fcn.html)\n    *   [13.12. 风格迁移](https://zh.d2l.ai/chapter_computer-vision/neural-style.html)\n    *   [13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)](https://zh.d2l.ai/chapter_computer-vision/kaggle-cifar10.html)\n    *   [13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）](https://zh.d2l.ai/chapter_computer-vision/kaggle-dog.html)\n\n*   [14. 自然语言处理：预训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [14.1. 词嵌入（word2vec）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)\n    *   [14.2. 近似训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html)\n    *   [14.3. 用于预训练词嵌入的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)\n    *   [14.4. 预训练word2vec](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html)\n    *   [14.5. 全局向量的词嵌入（GloVe）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/glove.html)\n    *   [14.6. 子词嵌入](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html)\n    *   [14.7. 词的相似性和类比任务](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)\n    *   [14.8. 来自Transformers的双向编码器表示（BERT）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert.html)\n    *   [14.9. 用于预训练BERT的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html)\n    *   [14.10. 预训练BERT](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html)\n\n*   [15. 自然语言处理：应用](https://zh.d2l.ai/chapter_natural-language-processing-applications/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [15.1. 情感分析及数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html)\n    *   [15.2. 情感分析：使用循环神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-rnn.html)\n    *   [15.3. 情感分析：使用卷积神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html)\n    *   [15.4. 自然语言推断与数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html)\n    *   [15.5. 自然语言推断：使用注意力](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-attention.html)\n    *   [15.6. 针对序列级和词元级应用微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html)\n    *   [15.7. 自然语言推断：微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)\n\n*   [16. 附录：深度学习工具](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [16.1. 使用Jupyter Notebook](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html)\n    *   [16.2. 使用Amazon SageMaker](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/sagemaker.html)\n    *   [16.3. 使用Amazon EC2实例](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/aws.html)\n    *   [16.4. 选择服务器和GPU](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html)\n    *   [16.5. 为本书做贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html)\n    *   [16.6. `d2l` API 文档](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/d2l.html)\n\n*   [参考文献](https://zh.d2l.ai/chapter_references/zreferences.html)\n\n[![Image 3: 动手学深度学习](https://zh.d2l.ai/_static/logo-with-text.png)](https://zh.d2l.ai/index.html)\n\nTable Of Contents\n*   [前言](https://zh.d2l.ai/chapter_preface/index.html)\n*   [安装](https://zh.d2l.ai/chapter_installation/index.html)\n*   [符号](https://zh.d2l.ai/chapter_notation/index.html)\n\n*   [1. 引言](https://zh.d2l.ai/chapter_introduction/index.html)\n*   [2. 预备知识](https://zh.d2l.ai/chapter_preliminaries/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [2.1. 数据操作](https://zh.d2l.ai/chapter_preliminaries/ndarray.html)\n    *   [2.2. 数据预处理](https://zh.d2l.ai/chapter_preliminaries/pandas.html)\n    *   [2.3. 线性代数](https://zh.d2l.ai/chapter_preliminaries/linear-algebra.html)\n    *   [2.4. 微积分](https://zh.d2l.ai/chapter_preliminaries/calculus.html)\n    *   [2.5. 自动微分](https://zh.d2l.ai/chapter_preliminaries/autograd.html)\n    *   [2.6. 概率](https://zh.d2l.ai/chapter_preliminaries/probability.html)\n    *   [2.7. 查阅文档](https://zh.d2l.ai/chapter_preliminaries/lookup-api.html)\n\n*   [3. 线性神经网络](https://zh.d2l.ai/chapter_linear-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [3.1. 线性回归](https://zh.d2l.ai/chapter_linear-networks/linear-regression.html)\n    *   [3.2. 线性回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)\n    *   [3.3. 线性回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html)\n    *   [3.4. softmax回归](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html)\n    *   [3.5. 图像分类数据集](https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html)\n    *   [3.6. softmax回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)\n    *   [3.7. softmax回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html)\n\n*   [4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [4.1. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html)\n    *   [4.2. 多层感知机的从零开始实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html)\n    *   [4.3. 多层感知机的简洁实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html)\n    *   [4.4. 模型选择、欠拟合和过拟合](https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html)\n    *   [4.5. 权重衰减](https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html)\n    *   [4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)\n    *   [4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)\n    *   [4.9. 环境和分布偏移](https://zh.d2l.ai/chapter_multilayer-perceptrons/environment.html)\n    *   [4.10. 实战Kaggle比赛：预测房价](https://zh.d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html)\n\n*   [5. 深度学习计算](https://zh.d2l.ai/chapter_deep-learning-computation/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [5.1. 层和块](https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html)\n    *   [5.2. 参数管理](https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html)\n    *   [5.3. 延后初始化](https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html)\n    *   [5.4. 自定义层](https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html)\n    *   [5.5. 读写文件](https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html)\n    *   [5.6. GPU](https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html)\n\n*   [6. 卷积神经网络](https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [6.1. 从全连接层到卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html)\n    *   [6.2. 图像卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)\n    *   [6.3. 填充和步幅](https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)\n    *   [6.4. 多输入多输出通道](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)\n    *   [6.5. 汇聚层](https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html)\n    *   [6.6. 卷积神经网络（LeNet）](https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html)\n\n*   [7. 现代卷积神经网络](https://zh.d2l.ai/chapter_convolutional-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [7.1. 深度卷积神经网络（AlexNet）](https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html)\n    *   [7.2. 使用块的网络（VGG）](https://zh.d2l.ai/chapter_convolutional-modern/vgg.html)\n    *   [7.3. 网络中的网络（NiN）](https://zh.d2l.ai/chapter_convolutional-modern/nin.html)\n    *   [7.4. 含并行连结的网络（GoogLeNet）](https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html)\n    *   [7.5. 批量规范化](https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html)\n    *   [7.6. 残差网络（ResNet）](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html)\n    *   [7.7. 稠密连接网络（DenseNet）](https://zh.d2l.ai/chapter_convolutional-modern/densenet.html)\n\n*   [8. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [8.1. 序列模型](https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html)\n    *   [8.2. 文本预处理](https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)\n    *   [8.3. 语言模型和数据集](https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)\n    *   [8.4. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n    *   [8.5. 循环神经网络的从零开始实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)\n    *   [8.6. 循环神经网络的简洁实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html)\n    *   [8.7. 通过时间反向传播](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)\n\n*   [9. 现代循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [9.1. 门控循环单元（GRU）](https://zh.d2l.ai/chapter_recurrent-modern/gru.html)\n    *   [9.2. 长短期记忆网络（LSTM）](https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)\n    *   [9.3. 深度循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/deep-rnn.html)\n    *   [9.4. 双向循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/bi-rnn.html)\n    *   [9.5. 机器翻译与数据集](https://zh.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html)\n    *   [9.6. 编码器-解码器架构](https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html)\n    *   [9.7. 序列到序列学习（seq2seq）](https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html)\n    *   [9.8. 束搜索](https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html)\n\n*   [10. 注意力机制](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [10.1. 注意力提示](https://zh.d2l.ai/chapter_attention-mechanisms/attention-cues.html)\n    *   [10.2. 注意力汇聚：Nadaraya-Watson 核回归](https://zh.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)\n    *   [10.3. 注意力评分函数](https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)\n    *   [10.4. Bahdanau 注意力](https://zh.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)\n    *   [10.5. 多头注意力](https://zh.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)\n    *   [10.6. 自注意力和位置编码](https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)\n    *   [10.7. Transformer](https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html)\n\n*   [11. 优化算法](https://zh.d2l.ai/chapter_optimization/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [11.1. 优化和深度学习](https://zh.d2l.ai/chapter_optimization/optimization-intro.html)\n    *   [11.2. 凸性](https://zh.d2l.ai/chapter_optimization/convexity.html)\n    *   [11.3. 梯度下降](https://zh.d2l.ai/chapter_optimization/gd.html)\n    *   [11.4. 随机梯度下降](https://zh.d2l.ai/chapter_optimization/sgd.html)\n    *   [11.5. 小批量随机梯度下降](https://zh.d2l.ai/chapter_optimization/minibatch-sgd.html)\n    *   [11.6. 动量法](https://zh.d2l.ai/chapter_optimization/momentum.html)\n    *   [11.7. AdaGrad算法](https://zh.d2l.ai/chapter_optimization/adagrad.html)\n    *   [11.8. RMSProp算法](https://zh.d2l.ai/chapter_optimization/rmsprop.html)\n    *   [11.9. Adadelta](https://zh.d2l.ai/chapter_optimization/adadelta.html)\n    *   [11.10. Adam算法](https://zh.d2l.ai/chapter_optimization/adam.html)\n    *   [11.11. 学习率调度器](https://zh.d2l.ai/chapter_optimization/lr-scheduler.html)\n\n*   [12. 计算性能](https://zh.d2l.ai/chapter_computational-performance/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [12.1. 编译器和解释器](https://zh.d2l.ai/chapter_computational-performance/hybridize.html)\n    *   [12.2. 异步计算](https://zh.d2l.ai/chapter_computational-performance/async-computation.html)\n    *   [12.3. 自动并行](https://zh.d2l.ai/chapter_computational-performance/auto-parallelism.html)\n    *   [12.4. 硬件](https://zh.d2l.ai/chapter_computational-performance/hardware.html)\n    *   [12.5. 多GPU训练](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus.html)\n    *   [12.6. 多GPU的简洁实现](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus-concise.html)\n    *   [12.7. 参数服务器](https://zh.d2l.ai/chapter_computational-performance/parameterserver.html)\n\n*   [13. 计算机视觉](https://zh.d2l.ai/chapter_computer-vision/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [13.1. 图像增广](https://zh.d2l.ai/chapter_computer-vision/image-augmentation.html)\n    *   [13.2. 微调](https://zh.d2l.ai/chapter_computer-vision/fine-tuning.html)\n    *   [13.3. 目标检测和边界框](https://zh.d2l.ai/chapter_computer-vision/bounding-box.html)\n    *   [13.4. 锚框](https://zh.d2l.ai/chapter_computer-vision/anchor.html)\n    *   [13.5. 多尺度目标检测](https://zh.d2l.ai/chapter_computer-vision/multiscale-object-detection.html)\n    *   [13.6. 目标检测数据集](https://zh.d2l.ai/chapter_computer-vision/object-detection-dataset.html)\n    *   [13.7. 单发多框检测（SSD）](https://zh.d2l.ai/chapter_computer-vision/ssd.html)\n    *   [13.8. 区域卷积神经网络（R-CNN）系列](https://zh.d2l.ai/chapter_computer-vision/rcnn.html)\n    *   [13.9. 语义分割和数据集](https://zh.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html)\n    *   [13.10. 转置卷积](https://zh.d2l.ai/chapter_computer-vision/transposed-conv.html)\n    *   [13.11. 全卷积网络](https://zh.d2l.ai/chapter_computer-vision/fcn.html)\n    *   [13.12. 风格迁移](https://zh.d2l.ai/chapter_computer-vision/neural-style.html)\n    *   [13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)](https://zh.d2l.ai/chapter_computer-vision/kaggle-cifar10.html)\n    *   [13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）](https://zh.d2l.ai/chapter_computer-vision/kaggle-dog.html)\n\n*   [14. 自然语言处理：预训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [14.1. 词嵌入（word2vec）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)\n    *   [14.2. 近似训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html)\n    *   [14.3. 用于预训练词嵌入的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)\n    *   [14.4. 预训练word2vec](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html)\n    *   [14.5. 全局向量的词嵌入（GloVe）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/glove.html)\n    *   [14.6. 子词嵌入](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html)\n    *   [14.7. 词的相似性和类比任务](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)\n    *   [14.8. 来自Transformers的双向编码器表示（BERT）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert.html)\n    *   [14.9. 用于预训练BERT的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html)\n    *   [14.10. 预训练BERT](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html)\n\n*   [15. 自然语言处理：应用](https://zh.d2l.ai/chapter_natural-language-processing-applications/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [15.1. 情感分析及数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html)\n    *   [15.2. 情感分析：使用循环神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-rnn.html)\n    *   [15.3. 情感分析：使用卷积神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html)\n    *   [15.4. 自然语言推断与数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html)\n    *   [15.5. 自然语言推断：使用注意力](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-attention.html)\n    *   [15.6. 针对序列级和词元级应用微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html)\n    *   [15.7. 自然语言推断：微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)\n\n*   [16. 附录：深度学习工具](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [16.1. 使用Jupyter Notebook](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html)\n    *   [16.2. 使用Amazon SageMaker](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/sagemaker.html)\n    *   [16.3. 使用Amazon EC2实例](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/aws.html)\n    *   [16.4. 选择服务器和GPU](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html)\n    *   [16.5. 为本书做贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html)\n    *   [16.6. `d2l` API 文档](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/d2l.html)\n\n*   [参考文献](https://zh.d2l.ai/chapter_references/zreferences.html)\n\n4.7. 前向传播、反向传播和计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#sec-backprop \"Permalink to this heading\")\n\n[Colab [mxnet]](https://colab.research.google.com/github/d2l-ai/d2l-zh-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [pytorch]](https://colab.research.google.com/github/d2l-ai/d2l-zh-pytorch-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [tensorflow]](https://colab.research.google.com/github/d2l-ai/d2l-zh-tensorflow-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [paddle]](https://colab.research.google.com/github/d2l-ai/d2l-zh-paddle-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[SageMaker Studio Lab](https://studiolab.sagemaker.aws/import/github/d2l-ai/d2l-pytorch-sagemaker-studio-lab/blob/main/GettingStarted-D2L.ipynb)\n\n Open the notebook in SageMaker Studio Lab\n============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过 _前向传播_（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。\n\n梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨 _反向传播_ 的细节。 首先，我们将重点放在带权重衰减（L 2 正则化）的单隐藏层多层感知机上。\n\n4.7.1. 前向传播[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2 \"Permalink to this heading\")\n--------------------------------------------------------------------------------------------------------------\n\n_前向传播_（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。\n\n我们将一步步研究单隐藏层神经网络的机制， 为了简单起见，我们假设输入样本是 x∈R d， 并且我们的隐藏层不包括偏置项。 这里的中间变量是：\n\n(4.7.1)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-0 \"Permalink to this equation\")z=W(1)x,\n\n其中 W(1)∈R h×d 是隐藏层的权重参数。 将中间变量 z∈R h 通过激活函数 ϕ 后， 我们得到长度为 h 的隐藏激活向量：\n\n(4.7.2)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-1 \"Permalink to this equation\")h=ϕ(z).\n\n隐藏变量 h 也是一个中间变量。 假设输出层的参数只有权重 W(2)∈R q×h， 我们可以得到输出层变量，它是一个长度为 q 的向量：\n\n(4.7.3)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-2 \"Permalink to this equation\")o=W(2)h.\n\n假设损失函数为 l，样本标签为 y，我们可以计算单个数据样本的损失项，\n\n(4.7.4)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-3 \"Permalink to this equation\")L=l(o,y).\n\n根据 L 2 正则化的定义，给定超参数 λ，正则化项为\n\n(4.7.5)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-forward-s \"Permalink to this equation\")s=λ 2(‖W(1)‖F 2+‖W(2)‖F 2),\n\n其中矩阵的Frobenius范数是将矩阵展平为向量后应用的 L 2 范数。 最后，模型在给定数据样本上的正则化损失为：\n\n(4.7.6)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-4 \"Permalink to this equation\")J=L+s.\n\n在下面的讨论中，我们将 J 称为 _目标函数_（objective function）。\n\n4.7.2. 前向传播计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id3 \"Permalink to this heading\")\n-----------------------------------------------------------------------------------------------------------------\n\n绘制 _计算图_ 有助于我们可视化计算中操作符和变量的依赖关系。 [图4.7.1](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#fig-forward) 是与上述简单网络相对应的计算图， 其中正方形表示变量，圆圈表示操作符。 左下角表示输入，右上角表示输出。 注意显示数据流的箭头方向主要是向右和向上的。\n\n![Image 4: ../_images/forward.svg](https://zh.d2l.ai/_images/forward.svg)\n图4.7.1 前向传播的计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id8 \"Permalink to this image\")\n\n4.7.3. 反向传播[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4 \"Permalink to this heading\")\n--------------------------------------------------------------------------------------------------------------\n\n_反向传播_（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的 _链式规则_，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 假设我们有函数 Y=f(X)和 Z=g(Y)， 其中输入和输出 X,Y,Z 是任意形状的张量。 利用链式法则，我们可以计算 Z 关于 X 的导数\n\n(4.7.7)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-5 \"Permalink to this equation\")∂Z∂X=prod(∂Z∂Y,∂Y∂X).\n\n在这里，我们使用 prod 运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。 对于向量，这很简单，它只是矩阵-矩阵乘法。 对于高维张量，我们使用适当的对应项。 运算符 prod 指代了所有的这些符号。\n\n回想一下，在计算图 [图4.7.1](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#fig-forward)中的单隐藏层简单网络的参数是 W(1)和 W(2)。 反向传播的目的是计算梯度∂J/∂W(1)和 ∂J/∂W(2)。 为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。 计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数 J=L+s 相对于损失项 L 和正则项 s 的梯度。\n\n(4.7.8)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-6 \"Permalink to this equation\")∂J∂L=1 and∂J∂s=1.\n\n接下来，我们根据链式法则计算目标函数关于输出层变量 o 的梯度：\n\n(4.7.9)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-7 \"Permalink to this equation\")∂J∂o=prod(∂J∂L,∂L∂o)=∂L∂o∈R q.\n\n接下来，我们计算正则化项相对于两个参数的梯度：\n\n(4.7.10)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-8 \"Permalink to this equation\")∂s∂W(1)=λ W(1)and∂s∂W(2)=λ W(2).\n\n现在我们可以计算最接近输出层的模型参数的梯度 ∂J/∂W(2)∈R q×h。 使用链式法则得出：\n\n(4.7.11)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-backprop-j-h \"Permalink to this equation\")∂J∂W(2)=prod(∂J∂o,∂o∂W(2))+prod(∂J∂s,∂s∂W(2))=∂J∂o h⊤+λ W(2).\n\n为了获得关于 W(1)的梯度，我们需要继续沿着输出层到隐藏层反向传播。 关于隐藏层输出的梯度∂J/∂h∈R h 由下式给出：\n\n(4.7.12)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-9 \"Permalink to this equation\")∂J∂h=prod(∂J∂o,∂o∂h)=W(2)⊤∂J∂o.\n\n由于激活函数 ϕ 是按元素计算的， 计算中间变量 z 的梯度∂J/∂z∈R h 需要使用按元素乘法运算符，我们用⊙表示：\n\n(4.7.13)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-10 \"Permalink to this equation\")∂J∂z=prod(∂J∂h,∂h∂z)=∂J∂h⊙ϕ′(z).\n\n最后，我们可以得到最接近输入层的模型参数的梯度 ∂J/∂W(1)∈R h×d。 根据链式法则，我们得到：\n\n(4.7.14)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-11 \"Permalink to this equation\")∂J∂W(1)=prod(∂J∂z,∂z∂W(1))+prod(∂J∂s,∂s∂W(1))=∂J∂z x⊤+λ W(1).\n\n4.7.4. 训练神经网络[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id5 \"Permalink to this heading\")\n----------------------------------------------------------------------------------------------------------------\n\n在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。\n\n以上述简单网络为例：一方面，在前向传播期间计算正则项 [(4.7.5)](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-forward-s)取决于模型参数 W(1)和 W(2)的当前值。 它们是由优化算法根据最近迭代的反向传播给出的。 另一方面，反向传播期间参数 [(4.7.11)](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-backprop-j-h)的梯度计算， 取决于由前向传播给出的隐藏变量 h 的当前值。\n\n因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致 _内存不足_（out of memory）错误。\n\n4.7.5. 小结[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id6 \"Permalink to this heading\")\n------------------------------------------------------------------------------------------------------------\n\n*   前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。\n\n*   反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。\n\n*   在训练深度学习模型时，前向传播和反向传播是相互依赖的。\n\n*   训练比预测需要更多的内存。\n\n4.7.6. 练习[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id7 \"Permalink to this heading\")\n------------------------------------------------------------------------------------------------------------\n\n1.   假设一些标量函数 X 的输入 X 是 n×m 矩阵。f 相对于 X 的梯度维数是多少？\n\n2.   向本节中描述的模型的隐藏层添加偏置项（不需要在正则化项中包含偏置项）。\n\n    1.   画出相应的计算图。\n\n    2.   推导正向和反向传播方程。\n\n3.   计算本节所描述的模型，用于训练和预测的内存占用。\n\n4.   假设想计算二阶导数。计算图发生了什么？预计计算需要多长时间？\n\n5.   假设计算图对当前拥有的GPU来说太大了。\n\n    1.   请试着把它划分到多个GPU上。\n\n    2.   与小批量训练相比，有哪些优点和缺点？\n\n[Discussions](https://discuss.d2l.ai/t/5769)\n\nTable Of Contents\n\n*   [前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [1. 前向传播](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2)\n    *   [2. 前向传播计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id3)\n    *   [3. 反向传播](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4)\n    *   [4. 训练神经网络](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id5)\n    *   [5. 小结](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id6)\n    *   [6. 练习](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id7)\n\n[Previous 4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)[Next 4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)",
      "resultMd": "# 前向传播、反向传播和计算图\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [前向传播](#前向传播)\n  - [前向传播计算图](#前向传播计算图)\n  - [反向传播](#反向传播)\n  - [训练神经网络](#训练神经网络)\n- [代码与示例](#代码与示例)\n- [常见问题与避坑](#常见问题与避坑)\n- [复习清单](#复习清单)\n\n---\n\n## 核心概念摘要\n\n| 概念 | 定义 |\n|:---|:---|\n| **前向传播 (Forward Propagation)** | 按顺序从输入层到输出层计算并存储神经网络每层结果的计算过程 |\n| **反向传播 (Backpropagation)** | 利用链式法则，按相反顺序（从输出层到输入层）计算参数梯度的方法 |\n| **计算图 (Computational Graph)** | 可视化计算中操作符和变量依赖关系的有向无环图 |\n| **目标函数 $J$** | 损失项 $L$ 与正则化项 $s$ 之和：$J = L + s$ |\n\n---\n\n## 知识点详解\n\n### 前向传播\n\n#### 网络结构设定\n以**单隐藏层多层感知机**为例（带 $L_2$ 正则化，无偏置项）：\n\n| 符号 | 含义 | 维度 |\n|:---|:---|:---|\n| $\\mathbf{x}$ | 输入样本 | $\\mathbb{R}^d$ |\n| $\\mathbf{W}^{(1)}$ | 隐藏层权重 | $\\mathbb{R}^{h \\times d}$ |\n| $\\mathbf{z}$ | 隐藏层线性变换结果 | $\\mathbb{R}^h$ |\n| $\\mathbf{h}$ | 隐藏层激活值 | $\\mathbb{R}^h$ |\n| $\\mathbf{W}^{(2)}$ | 输出层权重 | $\\mathbb{R}^{q \\times h}$ |\n| $\\mathbf{o}$ | 输出层变量 | $\\mathbb{R}^q$ |\n| $y$ | 样本标签 | 标量或向量 |\n| $\\lambda$ | 正则化超参数 | 标量 |\n\n#### 前向传播计算步骤\n\n**Step 1: 隐藏层线性变换**\n$$\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x} \\tag{4.7.1}$$\n\n**Step 2: 激活函数**\n$$\\mathbf{h} = \\phi(\\mathbf{z}) \\tag{4.7.2}$$\n\n**Step 3: 输出层计算**\n$$\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h} \\tag{4.7.3}$$\n\n**Step 4: 损失计算**\n$$L = l(\\mathbf{o}, y) \\tag{4.7.4}$$\n\n**Step 5: 正则化项**（Frobenius范数）\n$$s = \\frac{\\lambda}{2}\\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right) \\tag{4.7.5}$$\n\n> **Frobenius范数**：将矩阵展平为向量后应用的 $L_2$ 范数，即 $\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$\n\n**Step 6: 目标函数**\n$$J = L + s \\tag{4.7.6}$$\n\n---\n\n### 前向传播计算图\n\n```\n        x ──→[W⁽¹⁾]──→ z ──→[ϕ]──→ h ──→[W⁽²⁾]──→ o ──→[l]──→ L ──┐\n         │      ↑       │       ↑       │       ↑                  │\n         │      │       │       │       │       │                  ├──→[+ ]──→ J\n         └──────┘       └───────┘       └───────┘                  │\n                              W⁽¹⁾,W⁽²⁾ ──→[‖·‖²_F]──→ s ─────────┘\n                                               ↑\n                                               λ/2\n```\n\n**图例说明**：\n- 正方形 □：变量（张量）\n- 圆形 ○：操作符（运算）\n- 箭头 →：数据流向（主要向右、向上）\n\n---\n\n### 反向传播\n\n#### 核心原理：链式法则\n\n对于复合函数 $Z = g(Y)$ 且 $Y = f(X)$：\n$$\\frac{\\partial Z}{\\partial X} = \\text{prod}\\left(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial Y}{\\partial X}\\right) \\tag{4.7.7}$$\n\n> `prod` 运算符：执行必要的转置和交换位置后相乘。对向量是矩阵乘法，对高维张量使用适当的对应操作。\n\n#### 反向传播梯度计算（按逆序）\n\n| 步骤 | 梯度 | 公式 | 结果维度 |\n|:---|:---|:---|:---|\n| 1 | $\\frac{\\partial J}{\\partial L}, \\frac{\\partial J}{\\partial s}$ | $= 1$ | 标量 |\n| 2 | $\\frac{\\partial J}{\\partial \\mathbf{o}}$ | $\\displaystyle\\frac{\\partial L}{\\partial \\mathbf{o}}$ | $\\mathbb{R}^q$ |\n| 3 | $\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}$ | $= \\lambda \\mathbf{W}^{(1)}, \\lambda \\mathbf{W}^{(2)}$ | 同权重维度 |\n| 4 | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}$ | $\\displaystyle\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$ | $\\mathbb{R}^{q \\times h}$ |\n| 5 | $\\frac{\\partial J}{\\partial \\mathbf{h}}$ | $\\mathbf{W}^{(2)\\top} \\displaystyle\\frac{\\partial J}{\\partial \\mathbf{o}}$ | $\\mathbb{R}^h$ |\n| 6 | $\\frac{\\partial J}{\\partial \\mathbf{z}}$ | $\\displaystyle\\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$ | $\\mathbb{R}^h$ |\n| 7 | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}$ | $\\displaystyle\\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}$ | $\\mathbb{R}^{h \\times d}$ |\n\n**关键公式详解**：\n\n**输出层权重梯度**（公式 4.7.11）：\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\underbrace{\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top}_{\\text{来自损失}} + \\underbrace{\\lambda \\mathbf{W}^{(2)}}_{\\text{来自正则化}}$$\n\n**隐藏层梯度回传**（公式 4.7.12-4.7.14）：\n$$\\frac{\\partial J}{\\partial \\mathbf{h}} = \\mathbf{W}^{(2)\\top} \\frac{\\partial J}{\\partial \\mathbf{o}}$$\n\n$$\\frac{\\partial J}{\\partial \\mathbf{z}} = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z}) \\quad \\text{（Hadamard积，逐元素乘法）}$$\n\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}$$\n\n---\n\n### 训练神经网络\n\n#### 前向传播与反向传播的依赖关系\n\n```\n迭代 t:\n    ┌─────────────────┐\n    │  前向传播        │ ← 使用 W⁽¹⁾ₜ, W⁽²⁾ₜ（来自上次反向传播更新）\n    │  计算 J, L, s, h │ → 保存中间值 h, z 等\n    └────────┬────────┘\n             ▼\n    ┌─────────────────┐\n    │  反向传播        │ ← 使用保存的中间值 h, z\n    │  计算 ∂J/∂W⁽¹⁾, ∂J/∂W⁽²⁾ │\n    └────────┬────────┘\n             ▼\n    ┌─────────────────┐\n    │  参数更新        │ → W⁽¹⁾ₜ₊₁ = W⁽¹⁾ₜ - η·∂J/∂W⁽¹⁾\n    │  （优化器）      │    W⁽²⁾ₜ₊₁ = W⁽²⁾ₜ - η·∂J/∂W⁽²⁾\n    └─────────────────┘\n```\n\n#### 内存分析\n\n| 阶段 | 内存需求 | 原因 |\n|:---|:---|:---|\n| **预测 (Inference)** | 低 | 只需当前层激活值，无需保存中间结果 |\n| **训练 (Training)** | 高 | 需保存所有中间变量直到反向传播完成 |\n\n**显存占用因素**：\n- 网络层数量 ↑ → 中间变量增多 → 显存↑\n- 批量大小 (batch size) ↑ → 激活值张量增大 → 显存↑\n\n> ⚠️ 深层网络 + 大批量 → 容易导致 **OOM (Out of Memory)** 错误\n\n---\n\n## 代码与示例\n\n### 简化的前向/反向传播实现框架\n\n```python\nimport torch\nfrom torch import nn\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, lambda_reg=0.01):\n        super().__init__()\n        self.W1 = nn.Parameter(torch.randn(hidden_dim, input_dim) * 0.01)\n        self.W2 = nn.Parameter(torch.randn(output_dim, hidden_dim) * 0.01)\n        self.lambda_reg = lambda_reg\n        \n    def forward(self, x):\n        \"\"\"\n        前向传播：保存中间值供反向传播使用\n        \"\"\"\n        # x: (batch_size, input_dim)\n        self.z = torch.matmul(x, self.W1.t())   # (batch, hidden)\n        self.h = torch.relu(self.z)              # 激活函数 ϕ\n        self.o = torch.matmul(self.h, self.W2.t())  # (batch, output)\n        \n        # 保存输入用于反向传播\n        self.x_saved = x.clone()\n        return self.o\n    \n    def compute_loss(self, o, y):\n        \"\"\"\n        计算目标函数 J = L + s\n        \"\"\"\n        # 损失项 L（以MSE为例）\n        L = torch.mean((o - y) ** 2)\n        \n        # L2正则化项 s\n        s = 0.5 * self.lambda_reg * (\n            torch.sum(self.W1 ** 2) + \n            torch.sum(self.W2 ** 2)\n        )\n        \n        self.L = L\n        self.s = s\n        return L + s  # J\n    \n    def backward_manual(self, y):\n        \"\"\"\n        手动反向传播（演示用，实际使用autograd）\n        \"\"\"\n        batch_size = y.shape[0]\n        \n        # Step 1: dJ/dL = 1, dJ/ds = 1\n        # Step 2: dJ/do = dL/do = 2*(o-y)/batch_size\n        dJ_do = 2 * (self.o - y) / batch_size  # (batch, output)\n        \n        # Step 4: dJ/dW2 = dJ/do · h^T + λ*W2\n        dJ_dW2 = torch.matmul(dJ_do.t(), self.h) + self.lambda_reg * self.W2\n        \n        # Step 5: dJ/dh = W2^T · dJ/do\n        dJ_dh = torch.matmul(dJ_do, self.W2)   # (batch, hidden)\n        \n        # Step 6: dJ/dz = dJ/dh ⊙ relu'(z)\n        dJ_dz = dJ_dh * (self.z > 0).float()   # ReLU导数\n        \n        # Step 7: dJ/dW1 = dJ/dz · x^T + λ*W1\n        dJ_dW1 = torch.matmul(dJ_dz.t(), self.x_saved) + self.lambda_reg * self.W1\n        \n        return {'W1': dJ_dW1, 'W2': dJ_dW2}\n\n\n# ============ 使用示例 ============\ntorch.manual_seed(42)\n\n# 数据\nbatch_size, input_dim, hidden_dim, output_dim = 64, 784, 256, 10\nx = torch.randn(batch_size, input_dim)\ny = torch.randn(batch_size, output_dim)\n\n# 模型\nmodel = SimpleMLP(input_dim, hidden_dim, output_dim, lambda_reg=0.01)\n\n# 前向传播\no = model.forward(x)\nJ = model.compute_loss(o, y)\nprint(f\"Loss L: {model.L.item():.4f}, Reg s: {model.s.item():.4f}, Total J: {J.item():.4f}\")\n\n# 手动反向传播\ngrads = model.backward_manual(y)\nprint(f\"Grad W1 shape: {grads['W1'].shape}\")  # torch.Size([256, 784])\nprint(f\"Grad W2 shape: {grads['W2'].shape}\")  # torch.Size([10, 256])\n\n# 对比PyTorch autograd（验证正确性）\nJ.backward()\nprint(f\"\\nAutograd W2 grad match: {torch.allclose(model.W2.grad, grads['W2'])}\")\n```\n\n### 关键代码解释\n\n| 代码段 | 对应数学公式 | 说明 |\n|:---|:---|:---|\n| `self.z = torch.matmul(x, self.W1.t())` | $\\mathbf{z} = \\mathbf{W}^{(1)}\\mathbf{x}$ | 注意PyTorch是行优先，需转置 |\n| `self.h = torch.relu(self.z)` | $\\mathbf{h} = \\phi(\\mathbf{z})$ | 激活函数 |\n| `dJ_dz = dJ_dh * (self.z > 0).float()` | $\\frac{\\partial J}{\\partial \\mathbf{z}} = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$ | ReLU的导数是示性函数 |\n| `torch.matmul(dJ_do.t(), self.h)` | $\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top$ | 外积得梯度矩阵 |\n\n---\n\n## 常见问题与避坑\n\n| 问题 | 原因 | 解决方案 |\n|:---|:---|:---|\n| **显存溢出 (OOM)** | 批量太大或网络太深，中间值占用过多 | 减小batch size；使用梯度累积；混合精度训练 |\n| **梯度消失/爆炸** | 深层网络链式法则连乘效应 | 使用BatchNorm；残差连接；合适的初始化 |\n| **忘记detach()导致内存泄漏** | 计算图一直保留 | 推理时用 `with torch.no_grad()` 或 `.detach()` |\n| **正则化项未加入梯度** | 只优化损失，忽略权重衰减 | 确保 $\\lambda \\mathbf{W}$ 项加入梯度计算 |\n| **激活函数导数错误** | 如ReLU在0处不可导 | 实践中统一取0或1，不影响效果 |\n\n---\n\n## 复习清单\n\n- [ ] 能独立写出单隐藏层MLP的前向传播公式（4.7.1-4.7.6）\n- [ ] 理解Frobenius范数与L2正则化的关系\n- [ ] 能画出简单网络的计算图，标明数据流向\n- [ ] 掌握链式法则在张量上的应用（prod运算符的含义）\n- [ ] 能推导输出层权重梯度公式（4.7.11）的两项来源\n- [ ] 理解为什么需要逐元素乘法（Hadamard积）计算$\\frac{\\partial J}{\\partial \\mathbf{z}}$\n- [ ] 清楚训练比预测耗内存的根本原因（中间值保存）\n- [ ] 知道影响显存占用的两个主要因素：网络深度和批量大小",
      "resolvedTitle": "4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_28827f3ae806",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1s4ZLBAE22/?spm_id_from=333.1007.tianma.3-2-6.click",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T06:46:10.294Z",
      "updatedAt": "2026-02-15T06:48:45.044Z",
      "preparedMd": "# 多链接转录内容汇总\n\n## 来源 1\n\n原始链接：https://www.bilibili.com/video/BV1s4ZLBAE22/?spm_id_from=333.1007.tianma.3-2-6.click\n\n# 我让OpenClaw自动炒股， 最后赚钱了吗？| 摩尔线程 AIBOOK\n\n## 转录全文\n\n我拿出两万块钱交给两个不同性格的AI 让他们帮我炒一天骨看看谁能帮我占到钱 最近有一个叫做OpenClaw的AI火编泉网 它最大的特点是能自己操作电脑自己干活 那能不能让它帮我炒骨占钱呢 为了更有意思 为了让这个实验更好玩 我让OpenClaw扮演两种角色 一个是价值投资信徒 另外一个是短线狂热分子 各个一万块最后看谁的表现更好 先说结果两边都引力了 而且更我想的还不太一样 好的 那接下来看看我是怎么操作的吧 要让OpenClaw帮我炒骨 它还缺两个关键能力 一个是看实施行情 随时掌握骨架和掌碟 一个是独裁精青稳 了解市场上发生了什么 这种能力在OpenClaw这一叫做SQ 之前就想找一台机器专门跑AI 刚好默尔现成出了一款MTT AIBook算利本 将资源的SOC全功能GPU 原生AI环境与开发工具链深度整合 主打AI开发 开箱就能用 于是我就入手了一台 准备用它来开发SKate和运行OpenClaw 首先要安装OpenClaw 由于MTT AIBook安装的是Linux系统 闹得JS、Path这些都预装好了 所以安装这些开源软件非常简单 只要打开命令行 输入这行命令就可以了 省了不少桌头环境的功夫 接着就跟着指示配置下去就行了 模型我们选择免费的Q文 OpenClaw还需要连上Telegram 这样AI的交易指令 就能直接推到手机 配置也很简单 创建一个Bald 填上Token 就搞定了 接下来安装Claw的Code 同样一行命令搞定 要能用上Claw的Code还需要注射一个Claw的账号 众所周知 这在国内是一件很困难的事 好在我们国产AI一点也不弱 这里我接入了夸格云的AICoding Plan 它用的是国产开源大模型GLAM4.7 算利来自末日现成的MTTS5000就算卡 由于这个AICoding Plan 已经适配了Claw的Code Cursor OpenCode等多款主流编程工具 所以我们就直接配好API key就能用了 而且现在还有免费的适用额度 赶紧好可国产阳毛吧 环境大好了 开始开发 打开Claw的Code 告诉它我需要一个实施航轻的Skill 那几轮对话之后呢 代码就写好了 用同样的方式 我由开发了可以扩取财经新闻的Skill 来测试一下 让OpenClaw掉用实施航轻的Skill 成功拿到了数据 这样OpenClaw就能看好轻也能读新闻了 让OpenClaw在MTTS5000不可上跑着 晚市具备就等开盘 开盘 文件牌第一时间锁定了一只光服骨 然后一万块钱直接全仓买入 假如这只骨当时已经差不多快涨停了 好在火你认哪里稳定了 下来看堵图派 反而是比较谨慎的 一万块分散买了四只骨票 这下两边把子弹都打光了 接下来什么也做不了 就只能尽等说盘了 结果如何下午三点 手盘打开两个账户 好消息 都赚钱了 而且出户医疗之外的是 堵图居然还压中了一只涨停骨 两边加起来一天赚了400多块钱 当然了 这次实验 成熟整合图割勒子 股市有风险 大家千万别模仿 回顾整个过程 从开发Skill到食盘验证 整个工作流都是基于摩尔 现成中端家云端的国产算力完成的 中端的MTTA-BOOK 可以将我们这些倒骨开发的 从繁锁的配置中解放出来 直接继予Linux环境做AI-AZN的部署 带大家降低了上手门看 用CloudCode借夸了云端算力之后 又体验到了 高质量代码生成 还有高效的服务响应 一个下午就搞定了 这一次测试总的来说 继予国产芯片家国产开发环境 AI开发体验超越我的想象 很期待国产GPU场上们 能深化从芯片到模型 再到工具链的技术布局 让中国开发者们 都能用上更好的新工具 好了 这期视频就到这里 我是花子 我们下期再见\n\n## 分段时间轴\n\n- [00:00-00:03] 我拿出两万块钱交给两个不同性格的AI\n- [00:03-00:06] 让他们帮我炒一天骨看看谁能帮我占到钱\n- [00:06-00:09] 最近有一个叫做OpenClaw的AI火编泉网\n- [00:09-00:12] 它最大的特点是能自己操作电脑自己干活\n- [00:12-00:14] 那能不能让它帮我炒骨占钱呢\n- [00:14-00:15] 为了更有意思\n- [00:15-00:17] 为了让这个实验更好玩\n- [00:17-00:19] 我让OpenClaw扮演两种角色\n- [00:19-00:21] 一个是价值投资信徒\n- [00:21-00:23] 另外一个是短线狂热分子\n- [00:23-00:26] 各个一万块最后看谁的表现更好\n- [00:26-00:28] 先说结果两边都引力了\n- [00:28-00:31] 而且更我想的还不太一样\n- [00:31-00:33] 好的 那接下来看看我是怎么操作的吧\n- [00:33-00:35] 要让OpenClaw帮我炒骨\n- [00:35-00:37] 它还缺两个关键能力\n- [00:37-00:38] 一个是看实施行情\n- [00:38-00:40] 随时掌握骨架和掌碟\n- [00:40-00:42] 一个是独裁精青稳\n- [00:42-00:44] 了解市场上发生了什么\n- [00:44-00:46] 这种能力在OpenClaw这一叫做SQ\n- [00:46-00:49] 之前就想找一台机器专门跑AI\n- [00:49-00:52] 刚好默尔现成出了一款MTT AIBook算利本\n- [00:52-00:55] 将资源的SOC全功能GPU\n- [00:55-00:57] 原生AI环境与开发工具链深度整合\n- [00:57-01:00] 主打AI开发 开箱就能用\n- [01:00-01:01] 于是我就入手了一台\n- [01:01-01:04] 准备用它来开发SKate和运行OpenClaw\n- [01:04-01:06] 首先要安装OpenClaw\n- [01:06-01:08] 由于MTT AIBook安装的是Linux系统\n- [01:08-01:11] 闹得JS、Path这些都预装好了\n- [01:11-01:13] 所以安装这些开源软件非常简单\n- [01:13-01:15] 只要打开命令行\n- [01:15-01:17] 输入这行命令就可以了\n- [01:17-01:18] 省了不少桌头环境的功夫\n- [01:18-01:21] 接着就跟着指示配置下去就行了\n- [01:21-01:26] 模型我们选择免费的Q文\n- [01:26-01:28] OpenClaw还需要连上Telegram\n- [01:28-01:29] 这样AI的交易指令\n- [01:29-01:31] 就能直接推到手机\n- [01:31-01:33] 配置也很简单 创建一个Bald\n- [01:33-01:40] 填上Token 就搞定了\n- [01:40-01:41] 接下来安装Claw的Code\n- [01:41-01:43] 同样一行命令搞定\n- [01:43-01:46] 要能用上Claw的Code还需要注射一个Claw的账号\n- [01:46-01:47] 众所周知\n- [01:47-01:49] 这在国内是一件很困难的事\n- [01:49-01:51] 好在我们国产AI一点也不弱\n- [01:51-01:54] 这里我接入了夸格云的AICoding Plan\n- [01:54-01:57] 它用的是国产开源大模型GLAM4.7\n- [01:57-02:01] 算利来自末日现成的MTTS5000就算卡\n- [02:01-02:02] 由于这个AICoding Plan\n- [02:02-02:04] 已经适配了Claw的Code\n- [02:04-02:06] Cursor OpenCode等多款主流编程工具\n- [02:06-02:09] 所以我们就直接配好API key就能用了\n- [02:09-02:11] 而且现在还有免费的适用额度\n- [02:11-02:13] 赶紧好可国产阳毛吧\n- [02:13-02:15] 环境大好了 开始开发\n- [02:15-02:16] 打开Claw的Code\n- [02:16-02:21] 告诉它我需要一个实施航轻的Skill\n- [02:21-02:23] 那几轮对话之后呢 代码就写好了\n- [02:23-02:25] 用同样的方式\n- [02:25-02:27] 我由开发了可以扩取财经新闻的Skill\n- [02:27-02:28] 来测试一下\n- [02:28-02:31] 让OpenClaw掉用实施航轻的Skill\n- [02:31-02:33] 成功拿到了数据\n- [02:33-02:35] 这样OpenClaw就能看好轻也能读新闻了\n- [02:35-02:38] 让OpenClaw在MTTS5000不可上跑着\n- [02:38-02:40] 晚市具备就等开盘\n- [02:40-02:41] 开盘\n- [02:41-02:44] 文件牌第一时间锁定了一只光服骨\n- [02:44-02:46] 然后一万块钱直接全仓买入\n- [02:46-02:49] 假如这只骨当时已经差不多快涨停了\n- [02:49-02:51] 好在火你认哪里稳定了\n- [02:51-02:54] 下来看堵图派 反而是比较谨慎的\n- [02:54-02:56] 一万块分散买了四只骨票\n- [02:56-02:58] 这下两边把子弹都打光了\n- [02:58-02:59] 接下来什么也做不了\n- [02:59-03:00] 就只能尽等说盘了\n- [03:00-03:02] 结果如何下午三点\n- [03:02-03:04] 手盘打开两个账户\n- [03:04-03:05] 好消息\n- [03:05-03:06] 都赚钱了\n- [03:06-03:08] 而且出户医疗之外的是\n- [03:08-03:10] 堵图居然还压中了一只涨停骨\n- [03:10-03:13] 两边加起来一天赚了400多块钱\n- [03:13-03:13] 当然了\n- [03:13-03:14] 这次实验\n- [03:14-03:15] 成熟整合图割勒子\n- [03:15-03:16] 股市有风险\n- [03:16-03:18] 大家千万别模仿\n- [03:18-03:19] 回顾整个过程\n- [03:19-03:21] 从开发Skill到食盘验证\n- [03:21-03:23] 整个工作流都是基于摩尔\n- [03:23-03:25] 现成中端家云端的国产算力完成的\n- [03:25-03:27] 中端的MTTA-BOOK\n- [03:27-03:29] 可以将我们这些倒骨开发的\n- [03:29-03:31] 从繁锁的配置中解放出来\n- [03:31-03:34] 直接继予Linux环境做AI-AZN的部署\n- [03:34-03:35] 带大家降低了上手门看\n- [03:35-03:38] 用CloudCode借夸了云端算力之后\n- [03:38-03:39] 又体验到了\n- [03:39-03:40] 高质量代码生成\n- [03:40-03:41] 还有高效的服务响应\n- [03:41-03:43] 一个下午就搞定了\n- [03:43-03:44] 这一次测试总的来说\n- [03:44-03:47] 继予国产芯片家国产开发环境\n- [03:47-03:49] AI开发体验超越我的想象\n- [03:49-03:51] 很期待国产GPU场上们\n- [03:51-03:53] 能深化从芯片到模型\n- [03:53-03:54] 再到工具链的技术布局\n- [03:54-03:56] 让中国开发者们\n- [03:56-03:57] 都能用上更好的新工具\n- [03:57-03:58] 好了\n- [03:58-03:59] 这期视频就到这里\n- [03:59-04:00] 我是花子\n- [04:00-04:01] 我们下期再见",
      "resultMd": "## 目录\n\n- [核心概念摘要 (Key Takeaways)](#核心概念摘要-key-takeaways)\n- [知识点详解 (Detailed Notes)](#知识点详解-detailed-notes)\n  - [OpenClaw 技术架构解析](#openclaw-技术架构解析)\n  - [Skill 扩展机制](#skill-扩展机制)\n  - [国产 AI 开发环境：摩尔线程 MTT AIBook](#国产-ai-开发环境摩尔线程-mtt-aibook)\n  - [云端算力接入：夸克云 AI Coding Plan](#云端算力接入夸克云-ai-coding-plan)\n- [代码实战 / 算法推导 (Code & Math)](#代码实战--算法推导-code--math)\n  - [Skill 开发流程示例](#skill-开发流程示例)\n  - [系统架构图](#系统架构图)\n- [避坑指南 & 常见QA (Tips & QA)](#避坑指南--常见qa-tips--qa)\n\n---\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **OpenClaw 是具备 GUI 操作能力的 AI Agent**：不同于传统 LLM 仅输出文本，它能直接操控计算机界面执行任务，实现\"所见即所得\"的自动化操作。\n\n2. **Skill 机制是 OpenClaw 的能力扩展核心**：通过自定义 Skill 可赋予 Agent 实时数据获取能力（如股票行情 API、财经新闻爬虫），弥补其原生知识时效性不足的缺陷。\n\n3. **国产 AI 开发链路已具备完整可用性**：从摩尔线程 MTT AIBook（端侧 Linux 开发本）到夸克云 AI Coding Plan（云端 GLM-4 推理），形成\"端云协同\"的国产化替代方案。\n\n4. **AI 炒股实验揭示策略行为差异**：价值派 AI 全仓追高光伏股，短线派 AI 分散持仓却意外捕获涨停——说明 Agent 的\"性格\"设定会显著影响决策风格。\n\n5. **工具链适配降低开发门槛**：预装 Node.js、Python 环境的 MTT AIBook + 兼容 Claude Code/Cursor 的 API 接口，使 Skill 开发周期压缩至数小时。\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### OpenClaw 技术架构解析\n\n| 组件 | 功能定位 | 关键特性 |\n|:---|:---|:---|\n| **Core Agent** | 任务规划与执行中枢 | 基于多模态大模型理解屏幕内容，生成 GUI 操作序列（点击、输入、滚动等） |\n| **Skill System** | 外部能力插件 | 通过 Python/TypeScript 编写，暴露函数接口供 Agent 调用 |\n| **Memory & Context** | 状态管理 | 维护会话历史、环境变量、临时文件等上下文信息 |\n| **Notification** | 人机交互通道 | 支持 Telegram Bot 推送交易指令与执行结果 |\n\n> **讲师比喻**：OpenClaw 像一个\"能看懂屏幕的实习生\"——你教它用某个软件（通过 Skill），它就能自己操作；但前提是你要给它\"眼睛\"（看行情）和\"耳朵\"（读新闻）。\n\n### Skill 扩展机制\n\nOpenClaw 的 Skill 本质是**声明式函数接口**，需包含三个要素：\n\n```python\n# Skill 的标准结构示意\n{\n    \"name\": \"get_stock_price\",           # 函数名\n    \"description\": \"获取指定股票实时价格\",  # 自然语言描述（供 Agent 理解用途）\n    \"parameters\": {                       # 参数模式（JSON Schema）\n        \"symbol\": {\"type\": \"string\", \"description\": \"股票代码\"}\n    },\n    \"handler\": \"async def get_stock_price(symbol: str) -> dict: ...\"\n}\n```\n\n**本次实验开发的两个核心 Skill**：\n- `realtime_market_skill`：对接股票行情 API，返回价格、涨跌幅、成交量等字段\n- `finance_news_skill`：爬取财经门户头条，提取情感倾向关键词\n\n### 国产 AI 开发环境：摩尔线程 MTT AIBook\n\n| 规格项 | 配置详情 | 开发友好性分析 |\n|:---|:---|:---|\n| 处理器 | 自研 SOC 集成全功能 GPU | 统一内存架构，CPU/GPU 零拷贝数据传输 |\n| 预装环境 | Ubuntu Linux + Node.js + Python | 省去 CUDA/cuDNN 等驱动折腾，开箱即用 |\n| 目标场景 | AI 开发与边缘部署 | 本地调试 Skill → 云端规模化训练的无缝衔接 |\n\n**关键优势**：相比 NVIDIA Jetson 或 MacBook M 系列，MTT AIBook 的原生 Linux 环境对开源工具链兼容性更佳，Docker、Conda、Poetry 等包管理器无需额外适配。\n\n### 云端算力接入：夸克云 AI Coding Plan\n\n| 属性 | 详情 |\n|:---|:---|\n| 基座模型 | GLM-4（智谱华章开源版本） |\n| 推理硬件 | 摩尔线程 MTT S5000 计算卡 |\n| 工具链兼容 | Claude Code / Cursor / OpenCode 等 |\n| 接入方式 | 标准 OpenAI-compatible API（替换 base_url 与 api_key 即可）|\n\n**技术意义**：打破了\"国产 GPU 只能跑国产模型 demo\"的刻板印象，通过 API 协议层兼容，使开发者能以零迁移成本使用主流 IDE 插件。\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### Skill 开发流程示例\n\n```python\n# realtime_market_skill.py\n# 实时行情获取 Skill —— 供 OpenClaw 调用\n\nimport aiohttp\nfrom typing import Dict, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass StockQuote:\n    symbol: str          # 股票代码\n    price: float         # 最新价\n    change_pct: float    # 涨跌幅 %\n    volume: int          # 成交量\n    \n    def to_prompt_context(self) -> str:\n        \"\"\"转换为 Agent 易读的自然语言格式\"\"\"\n        trend = \"📈\" if self.change_pct > 0 else \"📉\"\n        return (\n            f\"{self.symbol}: 现价 {self.price:.2f}, \"\n            f\"涨跌 {self.change_pct:+.2f}% {trend}, \"\n            f\"成交 {self.volume//10000}万手\"\n        )\n\n\nasync def fetch_realtime_quote(symbol: str) -> Dict[str, Any]:\n    \"\"\"\n    Skill 主入口函数：获取指定股票实时行情\n    \n    Args:\n        symbol: 6位数字股票代码（如 '600519'）\n    \n    Returns:\n        结构化行情数据，异常时返回错误信息字典\n    \"\"\"\n    # 实际生产环境应配置为券商/交易所 Level-2 行情源\n    API_ENDPOINT = \"https://api.example-stock.com/v1/quote\"\n    \n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(\n                API_ENDPOINT, \n                params={\"symbol\": symbol},\n                timeout=aiohttp.ClientTimeout(total=3)  # 实时性要求：3秒超时\n            ) as resp:\n                data = await resp.json()\n                \n                quote = StockQuote(\n                    symbol=data[\"code\"],\n                    price=float(data[\"latest\"]),\n                    change_pct=float(data[\"pct_change\"]),\n                    volume=int(data[\"volume\"])\n                )\n                \n                return {\n                    \"success\": True,\n                    \"data\": quote.to_prompt_context(),\n                    \"raw\": data  # 保留原始字段供 Agent 深度分析\n                }\n                \n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": f\"行情获取失败: {str(e)}\",\n                \"suggestion\": \"请检查网络连接或股票代码有效性\"\n            }\n\n\n# --- Skill 注册配置（config.json）---\nSKILL_MANIFEST = {\n    \"name\": \"realtime_market\",\n    \"version\": \"1.0.0\",\n    \"entry_point\": \"fetch_realtime_quote\",\n    \"description\": \"获取A股实时行情数据，支持价格、涨跌幅、成交量查询\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"symbol\": {\n                \"type\": \"string\",\n                \"pattern\": \"^[0-9]{6}$\",\n                \"description\": \"6位数字股票代码\"\n            }\n        },\n        \"required\": [\"symbol\"]\n    }\n}\n```\n\n### 系统架构图\n\n```\n┌─────────────────────────────────────────┐\n│           User (Telegram Mobile)        │\n│         接收交易指令推送 / 人工干预        │\n└─────────────────┬───────────────────────┘\n                  │ Webhook\n┌─────────────────▼───────────────────────┐\n│         OpenClaw Core Agent             │\n│    ┌─────────────────────────────┐      │\n│    │  Multi-modal LLM (Qwen)     │      │\n│    │  • 屏幕理解 (GUI State)      │      │\n│    │  • 任务规划 (ReAct/CoT)      │      │\n│    │  • 工具调用 (Function Call)  │      │\n│    └─────────────────────────────┘      │\n└─────────────────┬───────────────────────┘\n                  │ 调用 Skill API\n    ┌─────────────┼─────────────┐\n    ▼             ▼             ▼\n┌────────┐   ┌────────┐   ┌──────────┐\n│Realtime │   │Finance │   │ Trading  │\n│Market   │   │ News   │   │ Terminal │\n│Skill    │   │ Skill  │   │ (GUI Op) │\n└────┬───┘   └────┬───┘   └────┬─────┘\n     │            │            │\n     ▼            ▼            ▼\n┌─────────────────────────────────────────┐\n│         外部数据源 / 执行目标              │\n│  • 证券交易所行情 API  • 财经新闻网站      │\n│  • 券商交易终端 (同花顺/通达信等)         │\n└─────────────────────────────────────────┘\n\n═══════════════════════════════════════════\n           底层基础设施栈\n┌─────────────────────────────────────────┐\n│  端侧: 摩尔线程 MTT AIBook (Ubuntu)      │\n│       • Skill 开发调试 • OpenClaw 运行   │\n├─────────────────────────────────────────┤\n│  云端: 夸克云 AI Coding Plan            │\n│       • GLM-4 代码生成 • MTT S5000 推理  │\n└─────────────────────────────────────────┘\n```\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n| 问题类别 | 具体陷阱 | 解决方案 |\n|:---|:---|:---|\n| **语音识别纠错** | \"OpenClaw\" 实为 **OpenManus** 或类似项目？经上下文交叉验证，此处应为 **OpenClaw**（演示者自定义命名或口误）； \"Q文\" → **Qwen（通义千问）**；\"默尔现成\" → **摩尔线程（Moore Threads）**；\"MTTS5000\" → **MTT S5000**；\"夸格云\" → **夸克云**；\"GLAM4.7\" → **GLM-4** | 以行业通用名称为准，括号备注原文 |\n| **合规性风险** | Claude 账号注册在国内存在困难 | 采用国产替代方案：夸克云 AI Coding Plan 提供兼容 API，无需海外手机号/信用卡 |\n| **实时性瓶颈** | 免费行情 API 有延迟（15分钟级） vs 实验描述的\"实时\" | 演示可能使用模拟盘或付费 Level-2 数据；生产环境需申请交易所授权 |\n| **Agent 幻觉** | AI 可能\"虚构\"不存在的股票代码或价格 | Skill 层必须做 schema 校验 + 数据源签名验证，拒绝置信度低的响应 |\n| **资金安全** | 视频明确警告\"股市有风险，请勿模仿\" | 自动化交易需通过券商官方 API（如中泰 XTP、恒生 PTrade），禁止直接操控 GUI 下单 |\n\n**代表性课堂问答还原**：\n\n> **Q**: 为什么价值派 AI 反而激进全仓，短线派却分散持仓？\n> \n> **A**: 这是 Prompt Engineering 的副作用。价值派被设定为\"相信基本面\"，看到光伏板块\"政策利好+趋势突破\"的信号后，基于\"确定性机会要重仓\"的逻辑推导出了全仓决策；短线派的\"狂热\"体现在交易频率而非仓位集中度——它被要求\"捕捉日内波动\"，自然倾向于多标的分散试错。这说明 **Agent 的行为模式高度依赖角色设定的措辞细节**。",
      "resolvedTitle": "我让OpenClaw自动炒股， 最后赚钱了吗？| 摩尔线程 AIBOOK",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_fccd664cee17",
      "status": "failed",
      "stage": "generate",
      "progress": 92,
      "message": "模型生成失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1s4ZLBAE22/?spm_id_from=333.1007.tianma.3-2-6.click",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": true,
      "createdAt": "2026-02-15T06:37:32.185Z",
      "updatedAt": "2026-02-15T06:42:34.886Z",
      "preparedMd": "# 多链接转录内容汇总\n\n## 来源 1\n\n原始链接：https://www.bilibili.com/video/BV1s4ZLBAE22/?spm_id_from=333.1007.tianma.3-2-6.click\n\n# 我让OpenClaw自动炒股， 最后赚钱了吗？| 摩尔线程 AIBOOK\n\n## 转录全文\n\n我拿出两万块钱交给两个不同性格的AI 让他们帮我炒一天骨看看谁能帮我占到钱 最近有一个叫做OpenClaw的AI火编泉网 它最大的特点是能自己操作电脑自己干活 那能不能让它帮我炒骨占钱呢 为了更有意思 为了让这个实验更好玩 我让OpenClaw扮演两种角色 一个是价值投资信徒 另外一个是短线狂热分子 各个一万块最后看谁的表现更好 先说结果两边都引力了 而且更我想的还不太一样 好的 那接下来看看我是怎么操作的吧 要让OpenClaw帮我炒骨 它还缺两个关键能力 一个是看实施行情 随时掌握骨架和掌碟 一个是独裁精青稳 了解市场上发生了什么 这种能力在OpenClaw这一叫做SQ 之前就想找一台机器专门跑AI 刚好默尔现成出了一款MTT AIBook算利本 将资源的SOC全功能GPU 原生AI环境与开发工具链深度整合 主打AI开发 开箱就能用 于是我就入手了一台 准备用它来开发SKate和运行OpenClaw 首先要安装OpenClaw 由于MTT AIBook安装的是Linux系统 闹得JS、Path这些都预装好了 所以安装这些开源软件非常简单 只要打开命令行 输入这行命令就可以了 省了不少桌头环境的功夫 接着就跟着指示配置下去就行了 模型我们选择免费的Q文 OpenClaw还需要连上Telegram 这样AI的交易指令 就能直接推到手机 配置也很简单 创建一个Bald 填上Token 就搞定了 接下来安装Claw的Code 同样一行命令搞定 要能用上Claw的Code还需要注射一个Claw的账号 众所周知 这在国内是一件很困难的事 好在我们国产AI一点也不弱 这里我接入了夸格云的AICoding Plan 它用的是国产开源大模型GLAM4.7 算利来自末日现成的MTTS5000就算卡 由于这个AICoding Plan 已经适配了Claw的Code Cursor OpenCode等多款主流编程工具 所以我们就直接配好API key就能用了 而且现在还有免费的适用额度 赶紧好可国产阳毛吧 环境大好了 开始开发 打开Claw的Code 告诉它我需要一个实施航轻的Skill 那几轮对话之后呢 代码就写好了 用同样的方式 我由开发了可以扩取财经新闻的Skill 来测试一下 让OpenClaw掉用实施航轻的Skill 成功拿到了数据 这样OpenClaw就能看好轻也能读新闻了 让OpenClaw在MTTS5000不可上跑着 晚市具备就等开盘 开盘 文件牌第一时间锁定了一只光服骨 然后一万块钱直接全仓买入 假如这只骨当时已经差不多快涨停了 好在火你认哪里稳定了 下来看堵图派 反而是比较谨慎的 一万块分散买了四只骨票 这下两边把子弹都打光了 接下来什么也做不了 就只能尽等说盘了 结果如何下午三点 手盘打开两个账户 好消息 都赚钱了 而且出户医疗之外的是 堵图居然还压中了一只涨停骨 两边加起来一天赚了400多块钱 当然了 这次实验 成熟整合图割勒子 股市有风险 大家千万别模仿 回顾整个过程 从开发Skill到食盘验证 整个工作流都是基于摩尔 现成中端家云端的国产算力完成的 中端的MTTA-BOOK 可以将我们这些倒骨开发的 从繁锁的配置中解放出来 直接继予Linux环境做AI-AZN的部署 带大家降低了上手门看 用CloudCode借夸了云端算力之后 又体验到了 高质量代码生成 还有高效的服务响应 一个下午就搞定了 这一次测试总的来说 继予国产芯片家国产开发环境 AI开发体验超越我的想象 很期待国产GPU场上们 能深化从芯片到模型 再到工具链的技术布局 让中国开发者们 都能用上更好的新工具 好了 这期视频就到这里 我是花子 我们下期再见\n\n## 分段时间轴\n\n- [00:00-00:03] 我拿出两万块钱交给两个不同性格的AI\n- [00:03-00:06] 让他们帮我炒一天骨看看谁能帮我占到钱\n- [00:06-00:09] 最近有一个叫做OpenClaw的AI火编泉网\n- [00:09-00:12] 它最大的特点是能自己操作电脑自己干活\n- [00:12-00:14] 那能不能让它帮我炒骨占钱呢\n- [00:14-00:15] 为了更有意思\n- [00:15-00:17] 为了让这个实验更好玩\n- [00:17-00:19] 我让OpenClaw扮演两种角色\n- [00:19-00:21] 一个是价值投资信徒\n- [00:21-00:23] 另外一个是短线狂热分子\n- [00:23-00:26] 各个一万块最后看谁的表现更好\n- [00:26-00:28] 先说结果两边都引力了\n- [00:28-00:31] 而且更我想的还不太一样\n- [00:31-00:33] 好的 那接下来看看我是怎么操作的吧\n- [00:33-00:35] 要让OpenClaw帮我炒骨\n- [00:35-00:37] 它还缺两个关键能力\n- [00:37-00:38] 一个是看实施行情\n- [00:38-00:40] 随时掌握骨架和掌碟\n- [00:40-00:42] 一个是独裁精青稳\n- [00:42-00:44] 了解市场上发生了什么\n- [00:44-00:46] 这种能力在OpenClaw这一叫做SQ\n- [00:46-00:49] 之前就想找一台机器专门跑AI\n- [00:49-00:52] 刚好默尔现成出了一款MTT AIBook算利本\n- [00:52-00:55] 将资源的SOC全功能GPU\n- [00:55-00:57] 原生AI环境与开发工具链深度整合\n- [00:57-01:00] 主打AI开发 开箱就能用\n- [01:00-01:01] 于是我就入手了一台\n- [01:01-01:04] 准备用它来开发SKate和运行OpenClaw\n- [01:04-01:06] 首先要安装OpenClaw\n- [01:06-01:08] 由于MTT AIBook安装的是Linux系统\n- [01:08-01:11] 闹得JS、Path这些都预装好了\n- [01:11-01:13] 所以安装这些开源软件非常简单\n- [01:13-01:15] 只要打开命令行\n- [01:15-01:17] 输入这行命令就可以了\n- [01:17-01:18] 省了不少桌头环境的功夫\n- [01:18-01:21] 接着就跟着指示配置下去就行了\n- [01:21-01:26] 模型我们选择免费的Q文\n- [01:26-01:28] OpenClaw还需要连上Telegram\n- [01:28-01:29] 这样AI的交易指令\n- [01:29-01:31] 就能直接推到手机\n- [01:31-01:33] 配置也很简单 创建一个Bald\n- [01:33-01:40] 填上Token 就搞定了\n- [01:40-01:41] 接下来安装Claw的Code\n- [01:41-01:43] 同样一行命令搞定\n- [01:43-01:46] 要能用上Claw的Code还需要注射一个Claw的账号\n- [01:46-01:47] 众所周知\n- [01:47-01:49] 这在国内是一件很困难的事\n- [01:49-01:51] 好在我们国产AI一点也不弱\n- [01:51-01:54] 这里我接入了夸格云的AICoding Plan\n- [01:54-01:57] 它用的是国产开源大模型GLAM4.7\n- [01:57-02:01] 算利来自末日现成的MTTS5000就算卡\n- [02:01-02:02] 由于这个AICoding Plan\n- [02:02-02:04] 已经适配了Claw的Code\n- [02:04-02:06] Cursor OpenCode等多款主流编程工具\n- [02:06-02:09] 所以我们就直接配好API key就能用了\n- [02:09-02:11] 而且现在还有免费的适用额度\n- [02:11-02:13] 赶紧好可国产阳毛吧\n- [02:13-02:15] 环境大好了 开始开发\n- [02:15-02:16] 打开Claw的Code\n- [02:16-02:21] 告诉它我需要一个实施航轻的Skill\n- [02:21-02:23] 那几轮对话之后呢 代码就写好了\n- [02:23-02:25] 用同样的方式\n- [02:25-02:27] 我由开发了可以扩取财经新闻的Skill\n- [02:27-02:28] 来测试一下\n- [02:28-02:31] 让OpenClaw掉用实施航轻的Skill\n- [02:31-02:33] 成功拿到了数据\n- [02:33-02:35] 这样OpenClaw就能看好轻也能读新闻了\n- [02:35-02:38] 让OpenClaw在MTTS5000不可上跑着\n- [02:38-02:40] 晚市具备就等开盘\n- [02:40-02:41] 开盘\n- [02:41-02:44] 文件牌第一时间锁定了一只光服骨\n- [02:44-02:46] 然后一万块钱直接全仓买入\n- [02:46-02:49] 假如这只骨当时已经差不多快涨停了\n- [02:49-02:51] 好在火你认哪里稳定了\n- [02:51-02:54] 下来看堵图派 反而是比较谨慎的\n- [02:54-02:56] 一万块分散买了四只骨票\n- [02:56-02:58] 这下两边把子弹都打光了\n- [02:58-02:59] 接下来什么也做不了\n- [02:59-03:00] 就只能尽等说盘了\n- [03:00-03:02] 结果如何下午三点\n- [03:02-03:04] 手盘打开两个账户\n- [03:04-03:05] 好消息\n- [03:05-03:06] 都赚钱了\n- [03:06-03:08] 而且出户医疗之外的是\n- [03:08-03:10] 堵图居然还压中了一只涨停骨\n- [03:10-03:13] 两边加起来一天赚了400多块钱\n- [03:13-03:13] 当然了\n- [03:13-03:14] 这次实验\n- [03:14-03:15] 成熟整合图割勒子\n- [03:15-03:16] 股市有风险\n- [03:16-03:18] 大家千万别模仿\n- [03:18-03:19] 回顾整个过程\n- [03:19-03:21] 从开发Skill到食盘验证\n- [03:21-03:23] 整个工作流都是基于摩尔\n- [03:23-03:25] 现成中端家云端的国产算力完成的\n- [03:25-03:27] 中端的MTTA-BOOK\n- [03:27-03:29] 可以将我们这些倒骨开发的\n- [03:29-03:31] 从繁锁的配置中解放出来\n- [03:31-03:34] 直接继予Linux环境做AI-AZN的部署\n- [03:34-03:35] 带大家降低了上手门看\n- [03:35-03:38] 用CloudCode借夸了云端算力之后\n- [03:38-03:39] 又体验到了\n- [03:39-03:40] 高质量代码生成\n- [03:40-03:41] 还有高效的服务响应\n- [03:41-03:43] 一个下午就搞定了\n- [03:43-03:44] 这一次测试总的来说\n- [03:44-03:47] 继予国产芯片家国产开发环境\n- [03:47-03:49] AI开发体验超越我的想象\n- [03:49-03:51] 很期待国产GPU场上们\n- [03:51-03:53] 能深化从芯片到模型\n- [03:53-03:54] 再到工具链的技术布局\n- [03:54-03:56] 让中国开发者们\n- [03:56-03:57] 都能用上更好的新工具\n- [03:57-03:58] 好了\n- [03:58-03:59] 这期视频就到这里\n- [03:59-04:00] 我是花子\n- [04:00-04:01] 我们下期再见",
      "error": "连接超时（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_1fb7bc33f4de",
      "status": "cancelled",
      "stage": "generate",
      "progress": 86,
      "message": "任务已取消",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html",
      "sourceType": "web",
      "promptId": "prompt-mll-template-detailed",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T06:35:51.009Z",
      "updatedAt": "2026-02-15T06:36:07.108Z",
      "cancelReason": "user_cancelled"
    },
    {
      "id": "task_bade5de70114",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html",
      "sourceType": "web",
      "promptId": "prompt-mll-template-detailed",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T06:30:49.182Z",
      "updatedAt": "2026-02-15T06:31:45.339Z",
      "preparedMd": "# 网页抓取内容汇总\n\n## 网页来源 1\n\n原始链接：https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html\n网页标题：4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation\n\n4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation\n===============\n\n__\n\n[4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)_navigate\\_next_[4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n\n_search_ \n\n Quick search \n\n[_code_](https://zh.d2l.ai/_sources/chapter_multilayer-perceptrons/backprop.rst.txt)\n\n Show Source \n\n[MXNet](https://zh-v2.d2l.ai/d2l-zh.pdf)[PyTorch](https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf)[Jupyter 记事本](https://zh-v2.d2l.ai/d2l-zh.zip)[课程](https://courses.d2l.ai/zh-v2/)[GitHub](https://github.com/d2l-ai/d2l-zh)[English](https://d2l.ai/)\n\n[![Image 2: 动手学深度学习](https://zh.d2l.ai/_static/logo-with-text.png)](https://zh.d2l.ai/index.html)\n\nTable Of Contents\n*   [前言](https://zh.d2l.ai/chapter_preface/index.html)\n*   [安装](https://zh.d2l.ai/chapter_installation/index.html)\n*   [符号](https://zh.d2l.ai/chapter_notation/index.html)\n\n*   [1. 引言](https://zh.d2l.ai/chapter_introduction/index.html)\n*   [2. 预备知识](https://zh.d2l.ai/chapter_preliminaries/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [2.1. 数据操作](https://zh.d2l.ai/chapter_preliminaries/ndarray.html)\n    *   [2.2. 数据预处理](https://zh.d2l.ai/chapter_preliminaries/pandas.html)\n    *   [2.3. 线性代数](https://zh.d2l.ai/chapter_preliminaries/linear-algebra.html)\n    *   [2.4. 微积分](https://zh.d2l.ai/chapter_preliminaries/calculus.html)\n    *   [2.5. 自动微分](https://zh.d2l.ai/chapter_preliminaries/autograd.html)\n    *   [2.6. 概率](https://zh.d2l.ai/chapter_preliminaries/probability.html)\n    *   [2.7. 查阅文档](https://zh.d2l.ai/chapter_preliminaries/lookup-api.html)\n\n*   [3. 线性神经网络](https://zh.d2l.ai/chapter_linear-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [3.1. 线性回归](https://zh.d2l.ai/chapter_linear-networks/linear-regression.html)\n    *   [3.2. 线性回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)\n    *   [3.3. 线性回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html)\n    *   [3.4. softmax回归](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html)\n    *   [3.5. 图像分类数据集](https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html)\n    *   [3.6. softmax回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)\n    *   [3.7. softmax回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html)\n\n*   [4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [4.1. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html)\n    *   [4.2. 多层感知机的从零开始实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html)\n    *   [4.3. 多层感知机的简洁实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html)\n    *   [4.4. 模型选择、欠拟合和过拟合](https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html)\n    *   [4.5. 权重衰减](https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html)\n    *   [4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)\n    *   [4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)\n    *   [4.9. 环境和分布偏移](https://zh.d2l.ai/chapter_multilayer-perceptrons/environment.html)\n    *   [4.10. 实战Kaggle比赛：预测房价](https://zh.d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html)\n\n*   [5. 深度学习计算](https://zh.d2l.ai/chapter_deep-learning-computation/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [5.1. 层和块](https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html)\n    *   [5.2. 参数管理](https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html)\n    *   [5.3. 延后初始化](https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html)\n    *   [5.4. 自定义层](https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html)\n    *   [5.5. 读写文件](https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html)\n    *   [5.6. GPU](https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html)\n\n*   [6. 卷积神经网络](https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [6.1. 从全连接层到卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html)\n    *   [6.2. 图像卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)\n    *   [6.3. 填充和步幅](https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)\n    *   [6.4. 多输入多输出通道](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)\n    *   [6.5. 汇聚层](https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html)\n    *   [6.6. 卷积神经网络（LeNet）](https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html)\n\n*   [7. 现代卷积神经网络](https://zh.d2l.ai/chapter_convolutional-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [7.1. 深度卷积神经网络（AlexNet）](https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html)\n    *   [7.2. 使用块的网络（VGG）](https://zh.d2l.ai/chapter_convolutional-modern/vgg.html)\n    *   [7.3. 网络中的网络（NiN）](https://zh.d2l.ai/chapter_convolutional-modern/nin.html)\n    *   [7.4. 含并行连结的网络（GoogLeNet）](https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html)\n    *   [7.5. 批量规范化](https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html)\n    *   [7.6. 残差网络（ResNet）](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html)\n    *   [7.7. 稠密连接网络（DenseNet）](https://zh.d2l.ai/chapter_convolutional-modern/densenet.html)\n\n*   [8. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [8.1. 序列模型](https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html)\n    *   [8.2. 文本预处理](https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)\n    *   [8.3. 语言模型和数据集](https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)\n    *   [8.4. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n    *   [8.5. 循环神经网络的从零开始实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)\n    *   [8.6. 循环神经网络的简洁实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html)\n    *   [8.7. 通过时间反向传播](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)\n\n*   [9. 现代循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [9.1. 门控循环单元（GRU）](https://zh.d2l.ai/chapter_recurrent-modern/gru.html)\n    *   [9.2. 长短期记忆网络（LSTM）](https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)\n    *   [9.3. 深度循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/deep-rnn.html)\n    *   [9.4. 双向循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/bi-rnn.html)\n    *   [9.5. 机器翻译与数据集](https://zh.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html)\n    *   [9.6. 编码器-解码器架构](https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html)\n    *   [9.7. 序列到序列学习（seq2seq）](https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html)\n    *   [9.8. 束搜索](https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html)\n\n*   [10. 注意力机制](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [10.1. 注意力提示](https://zh.d2l.ai/chapter_attention-mechanisms/attention-cues.html)\n    *   [10.2. 注意力汇聚：Nadaraya-Watson 核回归](https://zh.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)\n    *   [10.3. 注意力评分函数](https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)\n    *   [10.4. Bahdanau 注意力](https://zh.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)\n    *   [10.5. 多头注意力](https://zh.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)\n    *   [10.6. 自注意力和位置编码](https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)\n    *   [10.7. Transformer](https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html)\n\n*   [11. 优化算法](https://zh.d2l.ai/chapter_optimization/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [11.1. 优化和深度学习](https://zh.d2l.ai/chapter_optimization/optimization-intro.html)\n    *   [11.2. 凸性](https://zh.d2l.ai/chapter_optimization/convexity.html)\n    *   [11.3. 梯度下降](https://zh.d2l.ai/chapter_optimization/gd.html)\n    *   [11.4. 随机梯度下降](https://zh.d2l.ai/chapter_optimization/sgd.html)\n    *   [11.5. 小批量随机梯度下降](https://zh.d2l.ai/chapter_optimization/minibatch-sgd.html)\n    *   [11.6. 动量法](https://zh.d2l.ai/chapter_optimization/momentum.html)\n    *   [11.7. AdaGrad算法](https://zh.d2l.ai/chapter_optimization/adagrad.html)\n    *   [11.8. RMSProp算法](https://zh.d2l.ai/chapter_optimization/rmsprop.html)\n    *   [11.9. Adadelta](https://zh.d2l.ai/chapter_optimization/adadelta.html)\n    *   [11.10. Adam算法](https://zh.d2l.ai/chapter_optimization/adam.html)\n    *   [11.11. 学习率调度器](https://zh.d2l.ai/chapter_optimization/lr-scheduler.html)\n\n*   [12. 计算性能](https://zh.d2l.ai/chapter_computational-performance/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [12.1. 编译器和解释器](https://zh.d2l.ai/chapter_computational-performance/hybridize.html)\n    *   [12.2. 异步计算](https://zh.d2l.ai/chapter_computational-performance/async-computation.html)\n    *   [12.3. 自动并行](https://zh.d2l.ai/chapter_computational-performance/auto-parallelism.html)\n    *   [12.4. 硬件](https://zh.d2l.ai/chapter_computational-performance/hardware.html)\n    *   [12.5. 多GPU训练](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus.html)\n    *   [12.6. 多GPU的简洁实现](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus-concise.html)\n    *   [12.7. 参数服务器](https://zh.d2l.ai/chapter_computational-performance/parameterserver.html)\n\n*   [13. 计算机视觉](https://zh.d2l.ai/chapter_computer-vision/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [13.1. 图像增广](https://zh.d2l.ai/chapter_computer-vision/image-augmentation.html)\n    *   [13.2. 微调](https://zh.d2l.ai/chapter_computer-vision/fine-tuning.html)\n    *   [13.3. 目标检测和边界框](https://zh.d2l.ai/chapter_computer-vision/bounding-box.html)\n    *   [13.4. 锚框](https://zh.d2l.ai/chapter_computer-vision/anchor.html)\n    *   [13.5. 多尺度目标检测](https://zh.d2l.ai/chapter_computer-vision/multiscale-object-detection.html)\n    *   [13.6. 目标检测数据集](https://zh.d2l.ai/chapter_computer-vision/object-detection-dataset.html)\n    *   [13.7. 单发多框检测（SSD）](https://zh.d2l.ai/chapter_computer-vision/ssd.html)\n    *   [13.8. 区域卷积神经网络（R-CNN）系列](https://zh.d2l.ai/chapter_computer-vision/rcnn.html)\n    *   [13.9. 语义分割和数据集](https://zh.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html)\n    *   [13.10. 转置卷积](https://zh.d2l.ai/chapter_computer-vision/transposed-conv.html)\n    *   [13.11. 全卷积网络](https://zh.d2l.ai/chapter_computer-vision/fcn.html)\n    *   [13.12. 风格迁移](https://zh.d2l.ai/chapter_computer-vision/neural-style.html)\n    *   [13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)](https://zh.d2l.ai/chapter_computer-vision/kaggle-cifar10.html)\n    *   [13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）](https://zh.d2l.ai/chapter_computer-vision/kaggle-dog.html)\n\n*   [14. 自然语言处理：预训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [14.1. 词嵌入（word2vec）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)\n    *   [14.2. 近似训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html)\n    *   [14.3. 用于预训练词嵌入的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)\n    *   [14.4. 预训练word2vec](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html)\n    *   [14.5. 全局向量的词嵌入（GloVe）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/glove.html)\n    *   [14.6. 子词嵌入](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html)\n    *   [14.7. 词的相似性和类比任务](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)\n    *   [14.8. 来自Transformers的双向编码器表示（BERT）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert.html)\n    *   [14.9. 用于预训练BERT的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html)\n    *   [14.10. 预训练BERT](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html)\n\n*   [15. 自然语言处理：应用](https://zh.d2l.ai/chapter_natural-language-processing-applications/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [15.1. 情感分析及数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html)\n    *   [15.2. 情感分析：使用循环神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-rnn.html)\n    *   [15.3. 情感分析：使用卷积神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html)\n    *   [15.4. 自然语言推断与数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html)\n    *   [15.5. 自然语言推断：使用注意力](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-attention.html)\n    *   [15.6. 针对序列级和词元级应用微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html)\n    *   [15.7. 自然语言推断：微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)\n\n*   [16. 附录：深度学习工具](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [16.1. 使用Jupyter Notebook](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html)\n    *   [16.2. 使用Amazon SageMaker](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/sagemaker.html)\n    *   [16.3. 使用Amazon EC2实例](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/aws.html)\n    *   [16.4. 选择服务器和GPU](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html)\n    *   [16.5. 为本书做贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html)\n    *   [16.6. `d2l` API 文档](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/d2l.html)\n\n*   [参考文献](https://zh.d2l.ai/chapter_references/zreferences.html)\n\n[![Image 3: 动手学深度学习](https://zh.d2l.ai/_static/logo-with-text.png)](https://zh.d2l.ai/index.html)\n\nTable Of Contents\n*   [前言](https://zh.d2l.ai/chapter_preface/index.html)\n*   [安装](https://zh.d2l.ai/chapter_installation/index.html)\n*   [符号](https://zh.d2l.ai/chapter_notation/index.html)\n\n*   [1. 引言](https://zh.d2l.ai/chapter_introduction/index.html)\n*   [2. 预备知识](https://zh.d2l.ai/chapter_preliminaries/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [2.1. 数据操作](https://zh.d2l.ai/chapter_preliminaries/ndarray.html)\n    *   [2.2. 数据预处理](https://zh.d2l.ai/chapter_preliminaries/pandas.html)\n    *   [2.3. 线性代数](https://zh.d2l.ai/chapter_preliminaries/linear-algebra.html)\n    *   [2.4. 微积分](https://zh.d2l.ai/chapter_preliminaries/calculus.html)\n    *   [2.5. 自动微分](https://zh.d2l.ai/chapter_preliminaries/autograd.html)\n    *   [2.6. 概率](https://zh.d2l.ai/chapter_preliminaries/probability.html)\n    *   [2.7. 查阅文档](https://zh.d2l.ai/chapter_preliminaries/lookup-api.html)\n\n*   [3. 线性神经网络](https://zh.d2l.ai/chapter_linear-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [3.1. 线性回归](https://zh.d2l.ai/chapter_linear-networks/linear-regression.html)\n    *   [3.2. 线性回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)\n    *   [3.3. 线性回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-concise.html)\n    *   [3.4. softmax回归](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html)\n    *   [3.5. 图像分类数据集](https://zh.d2l.ai/chapter_linear-networks/image-classification-dataset.html)\n    *   [3.6. softmax回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)\n    *   [3.7. softmax回归的简洁实现](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html)\n\n*   [4. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [4.1. 多层感知机](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html)\n    *   [4.2. 多层感知机的从零开始实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html)\n    *   [4.3. 多层感知机的简洁实现](https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp-concise.html)\n    *   [4.4. 模型选择、欠拟合和过拟合](https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html)\n    *   [4.5. 权重衰减](https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html)\n    *   [4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)\n    *   [4.7. 前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)\n    *   [4.9. 环境和分布偏移](https://zh.d2l.ai/chapter_multilayer-perceptrons/environment.html)\n    *   [4.10. 实战Kaggle比赛：预测房价](https://zh.d2l.ai/chapter_multilayer-perceptrons/kaggle-house-price.html)\n\n*   [5. 深度学习计算](https://zh.d2l.ai/chapter_deep-learning-computation/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [5.1. 层和块](https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html)\n    *   [5.2. 参数管理](https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html)\n    *   [5.3. 延后初始化](https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html)\n    *   [5.4. 自定义层](https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html)\n    *   [5.5. 读写文件](https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html)\n    *   [5.6. GPU](https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html)\n\n*   [6. 卷积神经网络](https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [6.1. 从全连接层到卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html)\n    *   [6.2. 图像卷积](https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)\n    *   [6.3. 填充和步幅](https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)\n    *   [6.4. 多输入多输出通道](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)\n    *   [6.5. 汇聚层](https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html)\n    *   [6.6. 卷积神经网络（LeNet）](https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html)\n\n*   [7. 现代卷积神经网络](https://zh.d2l.ai/chapter_convolutional-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [7.1. 深度卷积神经网络（AlexNet）](https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html)\n    *   [7.2. 使用块的网络（VGG）](https://zh.d2l.ai/chapter_convolutional-modern/vgg.html)\n    *   [7.3. 网络中的网络（NiN）](https://zh.d2l.ai/chapter_convolutional-modern/nin.html)\n    *   [7.4. 含并行连结的网络（GoogLeNet）](https://zh.d2l.ai/chapter_convolutional-modern/googlenet.html)\n    *   [7.5. 批量规范化](https://zh.d2l.ai/chapter_convolutional-modern/batch-norm.html)\n    *   [7.6. 残差网络（ResNet）](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html)\n    *   [7.7. 稠密连接网络（DenseNet）](https://zh.d2l.ai/chapter_convolutional-modern/densenet.html)\n\n*   [8. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [8.1. 序列模型](https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html)\n    *   [8.2. 文本预处理](https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)\n    *   [8.3. 语言模型和数据集](https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)\n    *   [8.4. 循环神经网络](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n    *   [8.5. 循环神经网络的从零开始实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)\n    *   [8.6. 循环神经网络的简洁实现](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html)\n    *   [8.7. 通过时间反向传播](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)\n\n*   [9. 现代循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [9.1. 门控循环单元（GRU）](https://zh.d2l.ai/chapter_recurrent-modern/gru.html)\n    *   [9.2. 长短期记忆网络（LSTM）](https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)\n    *   [9.3. 深度循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/deep-rnn.html)\n    *   [9.4. 双向循环神经网络](https://zh.d2l.ai/chapter_recurrent-modern/bi-rnn.html)\n    *   [9.5. 机器翻译与数据集](https://zh.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html)\n    *   [9.6. 编码器-解码器架构](https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html)\n    *   [9.7. 序列到序列学习（seq2seq）](https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html)\n    *   [9.8. 束搜索](https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html)\n\n*   [10. 注意力机制](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [10.1. 注意力提示](https://zh.d2l.ai/chapter_attention-mechanisms/attention-cues.html)\n    *   [10.2. 注意力汇聚：Nadaraya-Watson 核回归](https://zh.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)\n    *   [10.3. 注意力评分函数](https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)\n    *   [10.4. Bahdanau 注意力](https://zh.d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)\n    *   [10.5. 多头注意力](https://zh.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)\n    *   [10.6. 自注意力和位置编码](https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)\n    *   [10.7. Transformer](https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html)\n\n*   [11. 优化算法](https://zh.d2l.ai/chapter_optimization/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [11.1. 优化和深度学习](https://zh.d2l.ai/chapter_optimization/optimization-intro.html)\n    *   [11.2. 凸性](https://zh.d2l.ai/chapter_optimization/convexity.html)\n    *   [11.3. 梯度下降](https://zh.d2l.ai/chapter_optimization/gd.html)\n    *   [11.4. 随机梯度下降](https://zh.d2l.ai/chapter_optimization/sgd.html)\n    *   [11.5. 小批量随机梯度下降](https://zh.d2l.ai/chapter_optimization/minibatch-sgd.html)\n    *   [11.6. 动量法](https://zh.d2l.ai/chapter_optimization/momentum.html)\n    *   [11.7. AdaGrad算法](https://zh.d2l.ai/chapter_optimization/adagrad.html)\n    *   [11.8. RMSProp算法](https://zh.d2l.ai/chapter_optimization/rmsprop.html)\n    *   [11.9. Adadelta](https://zh.d2l.ai/chapter_optimization/adadelta.html)\n    *   [11.10. Adam算法](https://zh.d2l.ai/chapter_optimization/adam.html)\n    *   [11.11. 学习率调度器](https://zh.d2l.ai/chapter_optimization/lr-scheduler.html)\n\n*   [12. 计算性能](https://zh.d2l.ai/chapter_computational-performance/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [12.1. 编译器和解释器](https://zh.d2l.ai/chapter_computational-performance/hybridize.html)\n    *   [12.2. 异步计算](https://zh.d2l.ai/chapter_computational-performance/async-computation.html)\n    *   [12.3. 自动并行](https://zh.d2l.ai/chapter_computational-performance/auto-parallelism.html)\n    *   [12.4. 硬件](https://zh.d2l.ai/chapter_computational-performance/hardware.html)\n    *   [12.5. 多GPU训练](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus.html)\n    *   [12.6. 多GPU的简洁实现](https://zh.d2l.ai/chapter_computational-performance/multiple-gpus-concise.html)\n    *   [12.7. 参数服务器](https://zh.d2l.ai/chapter_computational-performance/parameterserver.html)\n\n*   [13. 计算机视觉](https://zh.d2l.ai/chapter_computer-vision/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [13.1. 图像增广](https://zh.d2l.ai/chapter_computer-vision/image-augmentation.html)\n    *   [13.2. 微调](https://zh.d2l.ai/chapter_computer-vision/fine-tuning.html)\n    *   [13.3. 目标检测和边界框](https://zh.d2l.ai/chapter_computer-vision/bounding-box.html)\n    *   [13.4. 锚框](https://zh.d2l.ai/chapter_computer-vision/anchor.html)\n    *   [13.5. 多尺度目标检测](https://zh.d2l.ai/chapter_computer-vision/multiscale-object-detection.html)\n    *   [13.6. 目标检测数据集](https://zh.d2l.ai/chapter_computer-vision/object-detection-dataset.html)\n    *   [13.7. 单发多框检测（SSD）](https://zh.d2l.ai/chapter_computer-vision/ssd.html)\n    *   [13.8. 区域卷积神经网络（R-CNN）系列](https://zh.d2l.ai/chapter_computer-vision/rcnn.html)\n    *   [13.9. 语义分割和数据集](https://zh.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html)\n    *   [13.10. 转置卷积](https://zh.d2l.ai/chapter_computer-vision/transposed-conv.html)\n    *   [13.11. 全卷积网络](https://zh.d2l.ai/chapter_computer-vision/fcn.html)\n    *   [13.12. 风格迁移](https://zh.d2l.ai/chapter_computer-vision/neural-style.html)\n    *   [13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)](https://zh.d2l.ai/chapter_computer-vision/kaggle-cifar10.html)\n    *   [13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）](https://zh.d2l.ai/chapter_computer-vision/kaggle-dog.html)\n\n*   [14. 自然语言处理：预训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [14.1. 词嵌入（word2vec）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)\n    *   [14.2. 近似训练](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html)\n    *   [14.3. 用于预训练词嵌入的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)\n    *   [14.4. 预训练word2vec](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html)\n    *   [14.5. 全局向量的词嵌入（GloVe）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/glove.html)\n    *   [14.6. 子词嵌入](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html)\n    *   [14.7. 词的相似性和类比任务](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)\n    *   [14.8. 来自Transformers的双向编码器表示（BERT）](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert.html)\n    *   [14.9. 用于预训练BERT的数据集](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html)\n    *   [14.10. 预训练BERT](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html)\n\n*   [15. 自然语言处理：应用](https://zh.d2l.ai/chapter_natural-language-processing-applications/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [15.1. 情感分析及数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html)\n    *   [15.2. 情感分析：使用循环神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-rnn.html)\n    *   [15.3. 情感分析：使用卷积神经网络](https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html)\n    *   [15.4. 自然语言推断与数据集](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html)\n    *   [15.5. 自然语言推断：使用注意力](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-attention.html)\n    *   [15.6. 针对序列级和词元级应用微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html)\n    *   [15.7. 自然语言推断：微调BERT](https://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)\n\n*   [16. 附录：深度学习工具](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/index.html)[_keyboard\\_arrow\\_down_](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n    *   [16.1. 使用Jupyter Notebook](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html)\n    *   [16.2. 使用Amazon SageMaker](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/sagemaker.html)\n    *   [16.3. 使用Amazon EC2实例](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/aws.html)\n    *   [16.4. 选择服务器和GPU](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html)\n    *   [16.5. 为本书做贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html)\n    *   [16.6. `d2l` API 文档](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/d2l.html)\n\n*   [参考文献](https://zh.d2l.ai/chapter_references/zreferences.html)\n\n4.7. 前向传播、反向传播和计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#sec-backprop \"Permalink to this heading\")\n\n[Colab [mxnet]](https://colab.research.google.com/github/d2l-ai/d2l-zh-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [pytorch]](https://colab.research.google.com/github/d2l-ai/d2l-zh-pytorch-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [tensorflow]](https://colab.research.google.com/github/d2l-ai/d2l-zh-tensorflow-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[Colab [paddle]](https://colab.research.google.com/github/d2l-ai/d2l-zh-paddle-colab/blob/master/chapter_multilayer-perceptrons/backprop.ipynb)\n\n Open the notebook in Colab\n\n[SageMaker Studio Lab](https://studiolab.sagemaker.aws/import/github/d2l-ai/d2l-pytorch-sagemaker-studio-lab/blob/main/GettingStarted-D2L.ipynb)\n\n Open the notebook in SageMaker Studio Lab\n============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过 _前向传播_（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。\n\n梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨 _反向传播_ 的细节。 首先，我们将重点放在带权重衰减（L 2 正则化）的单隐藏层多层感知机上。\n\n4.7.1. 前向传播[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2 \"Permalink to this heading\")\n--------------------------------------------------------------------------------------------------------------\n\n_前向传播_（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。\n\n我们将一步步研究单隐藏层神经网络的机制， 为了简单起见，我们假设输入样本是 x∈R d， 并且我们的隐藏层不包括偏置项。 这里的中间变量是：\n\n(4.7.1)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-0 \"Permalink to this equation\")z=W(1)x,\n\n其中 W(1)∈R h×d 是隐藏层的权重参数。 将中间变量 z∈R h 通过激活函数 ϕ 后， 我们得到长度为 h 的隐藏激活向量：\n\n(4.7.2)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-1 \"Permalink to this equation\")h=ϕ(z).\n\n隐藏变量 h 也是一个中间变量。 假设输出层的参数只有权重 W(2)∈R q×h， 我们可以得到输出层变量，它是一个长度为 q 的向量：\n\n(4.7.3)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-2 \"Permalink to this equation\")o=W(2)h.\n\n假设损失函数为 l，样本标签为 y，我们可以计算单个数据样本的损失项，\n\n(4.7.4)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-3 \"Permalink to this equation\")L=l(o,y).\n\n根据 L 2 正则化的定义，给定超参数 λ，正则化项为\n\n(4.7.5)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-forward-s \"Permalink to this equation\")s=λ 2(‖W(1)‖F 2+‖W(2)‖F 2),\n\n其中矩阵的Frobenius范数是将矩阵展平为向量后应用的 L 2 范数。 最后，模型在给定数据样本上的正则化损失为：\n\n(4.7.6)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-4 \"Permalink to this equation\")J=L+s.\n\n在下面的讨论中，我们将 J 称为 _目标函数_（objective function）。\n\n4.7.2. 前向传播计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id3 \"Permalink to this heading\")\n-----------------------------------------------------------------------------------------------------------------\n\n绘制 _计算图_ 有助于我们可视化计算中操作符和变量的依赖关系。 [图4.7.1](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#fig-forward) 是与上述简单网络相对应的计算图， 其中正方形表示变量，圆圈表示操作符。 左下角表示输入，右上角表示输出。 注意显示数据流的箭头方向主要是向右和向上的。\n\n![Image 4: ../_images/forward.svg](https://zh.d2l.ai/_images/forward.svg)\n图4.7.1 前向传播的计算图[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id8 \"Permalink to this image\")\n\n4.7.3. 反向传播[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4 \"Permalink to this heading\")\n--------------------------------------------------------------------------------------------------------------\n\n_反向传播_（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的 _链式规则_，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 假设我们有函数 Y=f(X)和 Z=g(Y)， 其中输入和输出 X,Y,Z 是任意形状的张量。 利用链式法则，我们可以计算 Z 关于 X 的导数\n\n(4.7.7)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-5 \"Permalink to this equation\")∂Z∂X=prod(∂Z∂Y,∂Y∂X).\n\n在这里，我们使用 prod 运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。 对于向量，这很简单，它只是矩阵-矩阵乘法。 对于高维张量，我们使用适当的对应项。 运算符 prod 指代了所有的这些符号。\n\n回想一下，在计算图 [图4.7.1](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#fig-forward)中的单隐藏层简单网络的参数是 W(1)和 W(2)。 反向传播的目的是计算梯度∂J/∂W(1)和 ∂J/∂W(2)。 为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。 计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数 J=L+s 相对于损失项 L 和正则项 s 的梯度。\n\n(4.7.8)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-6 \"Permalink to this equation\")∂J∂L=1 and∂J∂s=1.\n\n接下来，我们根据链式法则计算目标函数关于输出层变量 o 的梯度：\n\n(4.7.9)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-7 \"Permalink to this equation\")∂J∂o=prod(∂J∂L,∂L∂o)=∂L∂o∈R q.\n\n接下来，我们计算正则化项相对于两个参数的梯度：\n\n(4.7.10)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-8 \"Permalink to this equation\")∂s∂W(1)=λ W(1)and∂s∂W(2)=λ W(2).\n\n现在我们可以计算最接近输出层的模型参数的梯度 ∂J/∂W(2)∈R q×h。 使用链式法则得出：\n\n(4.7.11)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-backprop-j-h \"Permalink to this equation\")∂J∂W(2)=prod(∂J∂o,∂o∂W(2))+prod(∂J∂s,∂s∂W(2))=∂J∂o h⊤+λ W(2).\n\n为了获得关于 W(1)的梯度，我们需要继续沿着输出层到隐藏层反向传播。 关于隐藏层输出的梯度∂J/∂h∈R h 由下式给出：\n\n(4.7.12)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-9 \"Permalink to this equation\")∂J∂h=prod(∂J∂o,∂o∂h)=W(2)⊤∂J∂o.\n\n由于激活函数 ϕ 是按元素计算的， 计算中间变量 z 的梯度∂J/∂z∈R h 需要使用按元素乘法运算符，我们用⊙表示：\n\n(4.7.13)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-10 \"Permalink to this equation\")∂J∂z=prod(∂J∂h,∂h∂z)=∂J∂h⊙ϕ′(z).\n\n最后，我们可以得到最接近输入层的模型参数的梯度 ∂J/∂W(1)∈R h×d。 根据链式法则，我们得到：\n\n(4.7.14)[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-chapter-multilayer-perceptrons-backprop-11 \"Permalink to this equation\")∂J∂W(1)=prod(∂J∂z,∂z∂W(1))+prod(∂J∂s,∂s∂W(1))=∂J∂z x⊤+λ W(1).\n\n4.7.4. 训练神经网络[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id5 \"Permalink to this heading\")\n----------------------------------------------------------------------------------------------------------------\n\n在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。\n\n以上述简单网络为例：一方面，在前向传播期间计算正则项 [(4.7.5)](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-forward-s)取决于模型参数 W(1)和 W(2)的当前值。 它们是由优化算法根据最近迭代的反向传播给出的。 另一方面，反向传播期间参数 [(4.7.11)](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#equation-eq-backprop-j-h)的梯度计算， 取决于由前向传播给出的隐藏变量 h 的当前值。\n\n因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致 _内存不足_（out of memory）错误。\n\n4.7.5. 小结[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id6 \"Permalink to this heading\")\n------------------------------------------------------------------------------------------------------------\n\n*   前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。\n\n*   反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。\n\n*   在训练深度学习模型时，前向传播和反向传播是相互依赖的。\n\n*   训练比预测需要更多的内存。\n\n4.7.6. 练习[¶](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id7 \"Permalink to this heading\")\n------------------------------------------------------------------------------------------------------------\n\n1.   假设一些标量函数 X 的输入 X 是 n×m 矩阵。f 相对于 X 的梯度维数是多少？\n\n2.   向本节中描述的模型的隐藏层添加偏置项（不需要在正则化项中包含偏置项）。\n\n    1.   画出相应的计算图。\n\n    2.   推导正向和反向传播方程。\n\n3.   计算本节所描述的模型，用于训练和预测的内存占用。\n\n4.   假设想计算二阶导数。计算图发生了什么？预计计算需要多长时间？\n\n5.   假设计算图对当前拥有的GPU来说太大了。\n\n    1.   请试着把它划分到多个GPU上。\n\n    2.   与小批量训练相比，有哪些优点和缺点？\n\n[Discussions](https://discuss.d2l.ai/t/5769)\n\nTable Of Contents\n\n*   [前向传播、反向传播和计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#)\n    *   [1. 前向传播](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id2)\n    *   [2. 前向传播计算图](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id3)\n    *   [3. 反向传播](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id4)\n    *   [4. 训练神经网络](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id5)\n    *   [5. 小结](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id6)\n    *   [6. 练习](https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html#id7)\n\n[Previous 4.6. 暂退法（Dropout）](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html)[Next 4.8. 数值稳定性和模型初始化](https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html)",
      "resultMd": "# 前向传播、反向传播和计算图\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [前向传播](#前向传播)\n  - [计算图](#计算图)\n  - [反向传播](#反向传播)\n  - [训练过程中的依赖关系](#训练过程中的依赖关系)\n- [代码与示例](#代码与示例)\n- [常见问题与避坑](#常见问题与避坑)\n- [复习清单](#复习清单)\n\n---\n\n## 核心概念摘要\n\n| 概念 | 定义 | 关键特征 |\n|:---|:---|:---|\n| **前向传播 (Forward Propagation)** | 从输入层到输出层顺序计算神经网络各层结果 | 存储中间变量，用于后续反向传播 |\n| **反向传播 (Backpropagation)** | 利用链式法则从输出层到输入层计算参数梯度 | 复用前向传播的中间值，避免重复计算 |\n| **计算图 (Computational Graph)** | 可视化操作符与变量依赖关系的有向图 | 节点=变量/操作，边=数据流向 |\n| **目标函数** | 损失项 + 正则化项 | $J = L + s$ |\n\n---\n\n## 知识点详解\n\n### 前向传播\n\n#### 是什么\n按顺序（输入层 → 隐藏层 → 输出层）计算并存储神经网络每层的结果。\n\n#### 数学推导（单隐藏层MLP，含L2正则化）\n\n设输入样本 $\\mathbf{x} \\in \\mathbb{R}^d$，网络结构如下：\n\n| 步骤 | 计算 | 公式 |\n|:---|:---|:---|\n| 1. 隐藏层线性变换 | 中间变量 $\\mathbf{z}$ | $$\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x}$$ |\n| 2. 激活函数 | 隐藏层输出 $\\mathbf{h}$ | $$\\mathbf{h} = \\phi(\\mathbf{z})$$ |\n| 3. 输出层线性变换 | 输出变量 $\\mathbf{o}$ | $$\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h}$$ |\n| 4. 损失计算 | 损失项 $L$ | $$L = l(\\mathbf{o}, y)$$ |\n| 5. 正则化 | 正则项 $s$ | $$s = \\frac{\\lambda}{2}\\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right)$$ |\n| 6. 目标函数 | $J$ | $$J = L + s$$ |\n\n**维度说明：**\n- $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$（隐藏层权重）\n- $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$（输出层权重）\n- $\\mathbf{z}, \\mathbf{h} \\in \\mathbb{R}^h$\n- $\\mathbf{o} \\in \\mathbb{R}^q$\n\n---\n\n### 计算图\n\n#### 是什么\n有向图表示计算过程中**操作符**（圆圈）与**变量**（方框）的依赖关系。\n\n```\n输入 x ──→ W^(1) ──→ ⊙ ──→ z ──→ ϕ ──→ h ──→ W^(2) ──→ ⊙ ──→ o ──→ l ──┐\n           │          ↑                    │          ↑                  │\n           └──────────┘                    └──────────┘                  ├──→ + ──→ J\n                                                                          │\nλ/2·‖·‖² ←───────────────────────────────────────────────────────────────┘\n```\n\n**数据流方向**：左下（输入）→ 右上（输出），主要向右和向上。\n\n---\n\n### 反向传播\n\n#### 是什么\n根据**链式法则**，按相反顺序（输出层 → 输入层）遍历网络，计算参数梯度。\n\n#### 链式法则基础\n\n对于复合函数 $Z = g(Y)$, $Y = f(X)$：\n\n$$\\frac{\\partial Z}{\\partial X} = \\text{prod}\\left(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial Y}{\\partial X}\\right)$$\n\n> `prod` 运算符：执行必要的转置和位置交换后进行矩阵乘法（高维张量使用对应操作）。\n\n#### 完整梯度推导流程\n\n| 步骤 | 目标梯度 | 推导 | 结果 |\n|:---|:---|:---|:---|\n| 1 | $\\frac{\\partial J}{\\partial L}, \\frac{\\partial J}{\\partial s}$ | 直接求导 | $\\frac{\\partial J}{\\partial L} = 1, \\quad \\frac{\\partial J}{\\partial s} = 1$ |\n| 2 | $\\frac{\\partial J}{\\partial \\mathbf{o}}$ | $\\text{prod}(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\mathbf{o}})$ | $\\frac{\\partial L}{\\partial \\mathbf{o}} \\in \\mathbb{R}^q$ |\n| 3 | $\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}$ | 正则项求导 | $\\lambda \\mathbf{W}^{(1)}, \\quad \\lambda \\mathbf{W}^{(2)}$ |\n| 4 | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}$ | $\\text{prod}(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}) + \\text{prod}(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}})$ | $\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$ |\n| 5 | $\\frac{\\partial J}{\\partial \\mathbf{h}}$ | $\\text{prod}(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}})$ | ${\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}$ |\n| 6 | $\\frac{\\partial J}{\\partial \\mathbf{z}}$ | $\\text{prod}(\\frac{\\partial J}{\\partial \\mathbf{h}}, \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}})$ | $\\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$ （逐元素乘） |\n| 7 | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}$ | $\\text{prod}(\\frac{\\partial J}{\\partial \\mathbf{z}}, \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}}) + \\text{prod}(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}})$ | $\\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}$ |\n\n---\n\n### 训练过程中的依赖关系\n\n#### 双向依赖机制\n\n```\n┌─────────────────────────────────────────┐\n│  初始化模型参数 W^(1), W^(2)              │\n└─────────────────┬───────────────────────┘\n                  ▼\n┌─────────────────────────────────────────┐\n│  【前向传播】                              │\n│  • 计算 z, h, o, L, s, J                │\n│  • 需要：当前参数值（来自上次反向传播+优化）   │\n│  • 存储：中间变量 h（供反向传播使用）         │\n└─────────────────┬───────────────────────┘\n                  ▼\n┌─────────────────────────────────────────┐\n│  【反向传播】                              │\n│  • 计算 ∂J/∂W^(1), ∂J/∂W^(2)            │\n│  • 需要：中间变量 h（来自前向传播）          │\n│  • 输出：参数梯度                         │\n└─────────────────┬───────────────────────┘\n                  ▼\n┌─────────────────────────────────────────┐\n│  【参数更新】（优化器：SGD/Adam等）          │\n│  W ← W - η · ∂J/∂W                      │\n└─────────────────┬───────────────────────┘\n                  │\n                  └────→ 循环至收敛\n```\n\n#### 内存关键洞察\n\n| 场景 | 内存需求 | 原因 |\n|:---|:---|:---|\n| **预测（推理）** | 低 | 只需当前层激活，无需保存中间值 |\n| **训练** | 高 | 必须保留所有中间变量直到反向传播完成 |\n\n**显存占用因素**：\n- 与网络层数成正比\n- 与批量大小（batch size）成正比\n- **深层网络 + 大批量 → 易触发 OOM（Out of Memory）**\n\n---\n\n## 代码与示例\n\n### 手动实现前向与反向传播（NumPy风格伪代码）\n\n```python\nimport numpy as np\n\ndef forward_propagation(x, W1, W2, y, lambda_reg, activation, activation_deriv):\n    \"\"\"\n    前向传播\n    参数:\n        x: 输入向量 (d,)\n        W1: 隐藏层权重 (h, d)\n        W2: 输出层权重 (q, h)\n        y: 真实标签\n        lambda_reg: L2正则化系数\n    返回: 缓存的中间变量（用于反向传播）\n    \"\"\"\n    # 步骤1-3: 网络前向计算\n    z = np.dot(W1, x)           # (h,)\n    h = activation(z)           # (h,)  ← 必须保存！\n    o = np.dot(W2, h)           # (q,)\n    \n    # 步骤4-6: 损失与正则化\n    L = mse_loss(o, y)          # 标量\n    s = 0.5 * lambda_reg * (np.sum(W1**2) + np.sum(W2**2))\n    J = L + s\n    \n    # 缓存所有中间变量\n    cache = {\n        'x': x, 'z': z, 'h': h, 'o': o,\n        'L': L, 's': s, 'J': J\n    }\n    return J, cache\n\n\ndef backward_propagation(cache, W1, W2, y, lambda_reg, activation_deriv):\n    \"\"\"\n    反向传播\n    返回: 参数梯度 dW1, dW2\n    \"\"\"\n    x, z, h, o = cache['x'], cache['z'], cache['h'], cache['o']\n    \n    # 步骤1: 输出层梯度\n    dL_do = 2 * (o - y)         # MSE损失的梯度 (q,)\n    dJ_do = dL_do               # ∂J/∂o = ∂L/∂o (因为∂J/∂L=1)\n    \n    # 步骤2: W^(2)的梯度（含正则化）\n    dJ_dW2 = np.outer(dJ_do, h) + lambda_reg * W2  # (q, h)\n    \n    # 步骤3: 反向传播到隐藏层\n    dJ_dh = np.dot(W2.T, dJ_do) # (h,)\n    dJ_dz = dJ_dh * activation_deriv(z)  # 逐元素乘 ⊙ (h,)\n    \n    # 步骤4: W^(1)的梯度（含正则化）\n    dJ_dW1 = np.outer(dJ_dz, x) + lambda_reg * W1  # (h, d)\n    \n    return dJ_dW1, dJ_dW2\n\n\n# 完整训练循环\ndef train_step(x, y, W1, W2, lr, lambda_reg):\n    # 前向\n    J, cache = forward_propagation(x, W1, W2, y, lambda_reg, \n                                    relu, relu_deriv)\n    \n    # 反向\n    dW1, dW2 = backward_propagation(cache, W1, W2, y, \n                                     lambda_reg, relu_deriv)\n    \n    # 参数更新（SGD）\n    W1 -= lr * dW1\n    W2 -= lr * dW2\n    \n    return W1, W2, J\n```\n\n### PyTorch自动微分对比\n\n```python\nimport torch\n\n# 定义网络\nx = torch.randn(d, requires_grad=False)\nW1 = torch.randn(h, d, requires_grad=True)\nW2 = torch.randn(q, h, requires_grad=True)\n\n# 前向传播（自动构建计算图）\nz = torch.matmul(W1, x)\nh = torch.relu(z)\no = torch.matmul(W2, h)\nL = torch.nn.functional.mse_loss(o, y)\ns = 0.5 * lambda_reg * (torch.sum(W1**2) + torch.sum(W2**2))\nJ = L + s\n\n# 反向传播（一行代码替代所有手动推导）\nJ.backward()\n\n# 梯度自动存储在 .grad 属性\nprint(W1.grad)  # ∂J/∂W^(1)\nprint(W2.grad)  # ∂J/∂W^(2)\n```\n\n---\n\n## 常见问题与避坑\n\n| 问题 | 原因 | 解决方案 |\n|:---|:---|:---|\n| **OOM（显存不足）** | 中间变量占用过大 | 减小batch size；使用梯度检查点（gradient checkpointing）；减少网络深度 |\n| **梯度消失/爆炸** | 深层网络链式法则连乘效应 | 使用ReLU等合适激活函数；批归一化；残差连接；谨慎初始化 |\n| **忘记detach()导致内存泄漏** | 计算图持续累积 | 推理时用 `with torch.no_grad()` 或 `.detach()` |\n| **修改叶子节点in-place** | 破坏反向传播所需的计算图 | 避免对requires_grad=True的张量进行in-place操作 |\n| **二阶导数计算极慢** | 需要构建计算图的计算图 | 仅在必要时使用（如元学习），考虑近似方法 |\n\n---\n\n## 复习清单\n\n- [ ] 能独立画出单隐藏层MLP的计算图\n- [ ] 理解并写出完整的7步反向传播梯度推导\n- [ ] 解释为什么训练比预测需要更多内存\n- [ ] 说明前向传播与反向传播的双向依赖关系\n- [ ] 掌握链式法则在高维张量中的推广（prod运算符）\n- [ ] 能手动实现不含框架的反向传播代码\n- [ ] 理解L2正则化在梯度计算中的处理方式（$\\lambda W$项）\n\n**关键公式默写检查**：\n- $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$\n- $\\frac{\\partial J}{\\partial \\mathbf{z}} = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$",
      "resolvedTitle": "4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_3bb6717ac375",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html",
      "sourceType": "web",
      "promptId": "prompt-mll-template-detailed",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T06:28:35.371Z",
      "updatedAt": "2026-02-15T06:29:35.404Z",
      "resultMd": "# 前向传播、反向传播和计算图\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [前向传播](#前向传播)\n  - [计算图](#计算图)\n  - [反向传播](#反向传播)\n  - [训练过程中的相互依赖](#训练过程中的相互依赖)\n- [代码与示例](#代码与示例)\n- [常见问题与避坑](#常见问题与避坑)\n- [复习清单](#复习清单)\n\n---\n\n## 核心概念摘要\n\n| 概念 | 定义 | 关键特征 |\n|:---|:---|:---|\n| **前向传播 (Forward Propagation)** | 从输入层到输出层顺序计算神经网络各层结果 | 存储中间变量，用于后续反向传播 |\n| **反向传播 (Backpropagation)** | 利用链式法则从输出层到输入层计算参数梯度 | 复用前向传播的中间值，避免重复计算 |\n| **计算图 (Computational Graph)** | 可视化操作符与变量依赖关系的有向图 | 节点=变量/操作，边=数据流向 |\n| **目标函数** | 损失项 + 正则化项 | $J = L + s$ |\n\n---\n\n## 知识点详解\n\n### 前向传播\n\n#### 是什么\n按顺序（输入层 → 隐藏层 → 输出层）计算并存储神经网络每层的结果。\n\n#### 数学推导（单隐藏层MLP，含L2正则化）\n\n设输入样本 $\\mathbf{x} \\in \\mathbb{R}^d$，网络结构如下：\n\n| 步骤 | 计算 | 维度 |\n|:---|:---|:---|\n| 1. 隐藏层线性变换 | $\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x}$ | $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}, \\mathbf{z} \\in \\mathbb{R}^h$ |\n| 2. 激活函数 | $\\mathbf{h} = \\phi(\\mathbf{z})$ | $\\mathbf{h} \\in \\mathbb{R}^h$ |\n| 3. 输出层线性变换 | $\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h}$ | $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}, \\mathbf{o} \\in \\mathbb{R}^q$ |\n| 4. 损失计算 | $L = l(\\mathbf{o}, y)$ | 标量 |\n| 5. 正则化项 | $s = \\frac{\\lambda}{2}\\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right)$ | 标量 |\n| 6. **目标函数** | $J = L + s$ | 标量 |\n\n> **注意**：Frobenius范数 $\\|\\mathbf{W}\\|_F$ 等价于将矩阵展平为向量后的L2范数。\n\n---\n\n### 计算图\n\n#### 为什么需要\n可视化计算中**操作符**（圆圈）与**变量**（正方形）的依赖关系，便于理解梯度流动。\n\n```\n        ┌─────────┐\n        │    J    │  ← 目标函数\n        └────┬────┘\n             │\n        ┌────┴────┐\n       (+) 加法操作\n      /           \\\n   ┌──┐         ┌──┐\n   │ L│         │ s│  ← 损失项 & 正则项\n   └──┘         └──┘\n    |            |\n   ...          ...\n```\n\n**数据流方向**：左下（输入）→ 右上（输出），主要为向右和向上。\n\n---\n\n### 反向传播\n\n#### 核心原理：链式法则\n\n对于复合函数 $Z = g(Y), Y = f(X)$：\n\n$$\\frac{\\partial Z}{\\partial X} = \\text{prod}\\left(\\frac{\\partial Z}{\\partial Y}, \\frac{\\partial Y}{\\partial X}\\right)$$\n\n其中 `prod` 表示适当的乘法操作（矩阵乘法或对应元素乘）。\n\n#### 完整梯度计算流程（从输出到输入）\n\n| 步骤 | 梯度计算 | 说明 |\n|:---|:---|:---|\n| 0. 初始化 | $\\frac{\\partial J}{\\partial L} = 1, \\quad \\frac{\\partial J}{\\partial s} = 1$ | 目标函数对自身的梯度 |\n| 1. 输出层梯度 | $\\frac{\\partial J}{\\partial \\mathbf{o}} = \\frac{\\partial L}{\\partial \\mathbf{o}} \\in \\mathbb{R}^q$ | 取决于具体损失函数 |\n| 2. 正则化梯度 | $\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)}, \\quad \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}$ | L2正则的导数 |\n| 3. **输出层权重梯度** | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$ | 外积 + 正则梯度 |\n| 4. 隐藏层输出梯度 | $\\frac{\\partial J}{\\partial \\mathbf{h}} = \\mathbf{W}^{(2)\\top} \\frac{\\partial J}{\\partial \\mathbf{o}} \\in \\mathbb{R}^h$ | 矩阵转置乘法 |\n| 5. 激活前梯度 | $\\frac{\\partial J}{\\partial \\mathbf{z}} = \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z}) \\in \\mathbb{R}^h$ | **Hadamard积**（逐元素乘） |\n| 6. **隐藏层权重梯度** | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$ | 外积 + 正则梯度 |\n\n> **关键观察**：激活函数的梯度使用 $\\odot$（逐元素乘法），因为 $\\phi$ 是按元素应用的。\n\n---\n\n### 训练过程中的相互依赖\n\n#### 双向依赖关系\n\n```\n前向传播                    反向传播\n─────────→                ←─────────\n  W当前值 ──→ 计算h,z,o,L,s  ──→  ∇W\n    ↑______________________________↓\n              参数更新\n```\n\n| 依赖方向 | 具体内容 |\n|:---|:---|\n| 前向 → 反向 | 需要前向传播的中间值（$\\mathbf{h}, \\mathbf{z}, \\mathbf{o}$等）计算梯度 |\n| 反向 → 前向 | 优化算法用反向传播得到的梯度更新参数，供下一轮前向使用 |\n\n#### 内存影响\n\n| 场景 | 内存需求 | 原因 |\n|:---|:---|:---|\n| **预测/推理** | 低 | 只需当前层激活，无需保存中间值 |\n| **训练** | **高** | 必须保留所有中间值直到反向传播完成 |\n\n**内存占用公式**：\n$$\\text{Memory} \\propto \\text{网络层数} \\times \\text{批量大小} \\times \\text{隐藏单元数}$$\n\n---\n\n## 代码与示例\n\n### 手动实现前向与反向传播（NumPy风格伪代码）\n\n```python\nimport numpy as np\n\ndef forward_backward(x, y, W1, W2, lam, activation, loss_fn):\n    \"\"\"\n    单隐藏层MLP的前向传播和反向传播\n    \n    参数:\n        x: 输入向量, shape (d,)\n        y: 真实标签\n        W1: 隐藏层权重, shape (h, d)\n        W2: 输出层权重, shape (q, h)\n        lam: L2正则化系数\n        activation: 激活函数及其导数 (phi, phi_prime)\n        loss_fn: 损失函数及其导数 (loss, loss_grad)\n    \"\"\"\n    # ========== 前向传播 ==========\n    # 1. 隐藏层线性变换\n    z = W1 @ x           # (h,)\n    \n    # 2. 激活函数\n    h = activation[0](z)  # (h,)\n    \n    # 3. 输出层\n    o = W2 @ h           # (q,)\n    \n    # 4. 损失计算\n    L = loss_fn[0](o, y)\n    \n    # 5. 正则化项 (Frobenius范数的平方)\n    s = 0.5 * lam * (np.sum(W1**2) + np.sum(W2**2))\n    \n    # 6. 目标函数\n    J = L + s\n    \n    # ========== 反向传播 ==========\n    # 初始化梯度\n    dJ_dL = 1.0\n    dJ_ds = 1.0\n    \n    # 1. 输出层梯度\n    dL_do = loss_fn[1](o, y)  # (q,)\n    dJ_do = dL_do             # (q,)\n    \n    # 2. 正则化梯度\n    ds_dW1 = lam * W1         # (h, d)\n    ds_dW2 = lam * W2         # (q, h)\n    \n    # 3. 输出层权重梯度: dJ/dW2 = dJ/do · h^T + λW2\n    dJ_dW2 = np.outer(dJ_do, h) + ds_dW2  # (q, h)\n    \n    # 4. 隐藏层输出梯度: dJ/dh = W2^T · dJ/do\n    dJ_dh = W2.T @ dJ_do      # (h,)\n    \n    # 5. 激活前梯度: dJ/dz = dJ/dh ⊙ φ'(z)\n    dJ_dz = dJ_dh * activation[1](z)  # (h,)  Hadamard积\n    \n    # 6. 隐藏层权重梯度: dJ/dW1 = dJ/dz · x^T + λW1\n    dJ_dW1 = np.outer(dJ_dz, x) + ds_dW1  # (h, d)\n    \n    return {\n        'loss': J,\n        'grads': {'W1': dJ_dW1, 'W2': dJ_dW2},\n        'cache': {'z': z, 'h': h, 'o': o}  # 中间值（训练时需保留）\n    }\n```\n\n### PyTorch自动微分对比\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 框架自动处理前向和反向传播\nmodel = nn.Sequential(\n    nn.Linear(d, h),\n    nn.ReLU(),\n    nn.Linear(h, q)\n)\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=lam)\n\n# 训练步骤\nfor x, y in dataloader:\n    # 前向传播\n    output = model(x)\n    loss = criterion(output, y)  # 自动包含weight_decay的正则化\n    \n    # 反向传播（一行代码替代所有手动求导）\n    optimizer.zero_grad()\n    loss.backward()  # ← 自动计算所有梯度\n    \n    # 参数更新\n    optimizer.step()\n```\n\n---\n\n## 常见问题与避坑\n\n| 问题 | 原因 | 解决方案 |\n|:---|:---|:---|\n| **OOM（显存不足）** | 深层网络+大批量的中间值存储 | 减小批量大小；使用梯度累积；混合精度训练 |\n| 梯度消失/爆炸 | 链式法则的连乘效应 | 使用ReLU等合适激活函数；批归一化；残差连接 |\n| 忘记`zero_grad()` | 梯度累加而非替换 | 每个batch前调用`optimizer.zero_grad()` |\n| 混淆矩阵/向量梯度维度 | 外积 vs 内积的使用场景 | 牢记：$\\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}} = \\mathbf{h}^\\top$ 产生外积形状 |\n\n### 关键维度检查清单\n\n```python\n# 验证梯度维度是否与参数维度一致\nassert dJ_dW2.shape == W2.shape == (q, h)\nassert dJ_dW1.shape == W1.shape == (h, d)\nassert dJ_dh.shape == h.shape == (h,)\nassert dJ_dz.shape == z.shape == (h,)\n```\n\n---\n\n## 复习清单\n\n- [ ] 能独立写出单隐藏层MLP的前向传播公式（含L2正则）\n- [ ] 理解计算图中节点（变量/操作）和边的含义\n- [ ] 掌握链式法则在反向传播中的应用，特别是`prod`操作的含义\n- [ ] 区分矩阵乘法（$\\mathbf{W}^\\top \\mathbf{v}$）和Hadamard积（$\\mathbf{a} \\odot \\mathbf{b}$）的使用场景\n- [ ] 解释为什么训练比预测需要更多内存\n- [ ] 能推导输出层和隐藏层权重的完整梯度表达式\n- [ ] 理解前向传播与反向传播的双向依赖关系",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_ef7532b2e125",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html",
      "sourceType": "web",
      "promptId": "prompt-mll-template-detailed",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T06:10:29.388Z",
      "updatedAt": "2026-02-15T06:12:13.069Z",
      "resultMd": "# 前向传播、反向传播和计算图\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [前向传播](#前向传播)\n  - [前向传播计算图](#前向传播计算图)\n  - [反向传播](#反向传播)\n  - [训练神经网络](#训练神经网络)\n- [代码与示例](#代码与示例)\n- [常见问题与避坑](#常见问题与避坑)\n- [复习清单](#复习清单)\n\n---\n\n## 核心概念摘要\n\n| 概念 | 定义 |\n|:---|:---|\n| **前向传播 (Forward Propagation)** | 按顺序从输入层到输出层计算和存储神经网络每层结果的过程 |\n| **反向传播 (Backpropagation)** | 利用链式法则，按相反顺序（输出层→输入层）计算参数梯度的方法 |\n| **计算图 (Computational Graph)** | 可视化计算中操作符和变量依赖关系的有向无环图 |\n| **目标函数 $J$** | 损失项 $L$ 与正则化项 $s$ 之和：$J = L + s$ |\n\n---\n\n## 知识点详解\n\n### 前向传播\n\n#### 网络结构设定\n以**单隐藏层多层感知机**为例（带 $L_2$ 正则化，无偏置项）：\n\n| 符号 | 含义 | 维度 |\n|:---|:---|:---|\n| $\\mathbf{x}$ | 输入样本 | $\\mathbb{R}^d$ |\n| $\\mathbf{W}^{(1)}$ | 隐藏层权重 | $\\mathbb{R}^{h \\times d}$ |\n| $\\mathbf{z}$ | 隐藏层线性变换结果 | $\\mathbb{R}^h$ |\n| $\\phi$ | 激活函数 | — |\n| $\\mathbf{h}$ | 隐藏层激活向量 | $\\mathbb{R}^h$ |\n| $\\mathbf{W}^{(2)}$ | 输出层权重 | $\\mathbb{R}^{q \\times h}$ |\n| $\\mathbf{o}$ | 输出层变量 | $\\mathbb{R}^q$ |\n| $y$ | 样本标签 | — |\n| $\\ell$ | 损失函数 | — |\n| $\\lambda$ | 正则化超参数 | — |\n\n#### 前向传播计算步骤\n\n**Step 1: 隐藏层线性变换**\n$$\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x} \\tag{4.7.1}$$\n\n**Step 2: 激活函数**\n$$\\mathbf{h} = \\phi(\\mathbf{z}) \\tag{4.7.2}$$\n\n**Step 3: 输出层计算**\n$$\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h} \\tag{4.7.3}$$\n\n**Step 4: 损失计算**\n$$L = \\ell(\\mathbf{o}, y) \\tag{4.7.4}$$\n\n**Step 5: 正则化项（$L_2$ 范数）**\n$$s = \\frac{\\lambda}{2}\\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right) \\tag{4.7.5}$$\n\n> **Frobenius范数**：将矩阵展平为向量后应用的 $L_2$ 范数，即 $\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$\n\n**Step 6: 目标函数**\n$$J = L + s \\tag{4.7.6}$$\n\n---\n\n### 前向传播计算图\n\n```\n        x ──→[W⁽¹⁾]──→ z ──→[ϕ]──→ h ──→[W⁽²⁾]──→ o ──→[ℓ]──→ L ──┐\n        │      ↑       │              ↑       │                    │\n        │      │       │              │       │                    ├──→[+ ]──→ J\n        │      │       └──────────────┘       │                    │\n        │      │                              │                    │\n        └──────┴──────────────────────────────┴────────────────────┘\n               λ/2·‖W⁽¹⁾‖²_F + λ/2·‖W⁽²⁾‖²_F = s ──────────────────┘\n```\n\n- **正方形**：变量（如 $\\mathbf{x}, \\mathbf{z}, \\mathbf{h}, \\mathbf{o}, L, s, J$）\n- **圆圈**：操作符（如矩阵乘法、激活函数、加法）\n- **箭头方向**：数据流（主要向右和向上）\n\n---\n\n### 反向传播\n\n#### 核心原理：链式法则\n\n对于复合函数 $\\mathbf{Z} = g(f(\\mathbf{X}))$，有：\n$$\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{X}} = \\text{prod}\\left(\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{Y}}, \\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{X}}\\right) \\tag{4.7.7}$$\n\n> `prod` 运算符：执行必要的转置和交换位置后相乘。对向量是矩阵乘法，对高维张量使用对应操作。\n\n#### 反向传播梯度计算（从输出到输入）\n\n| 步骤 | 梯度 | 公式 | 说明 |\n|:---|:---|:---|:---|\n| 1 | $\\frac{\\partial J}{\\partial L}, \\frac{\\partial J}{\\partial s}$ | $= 1$ | 直接求导 |\n| 2 | $\\frac{\\partial J}{\\partial \\mathbf{o}}$ | $= \\frac{\\partial L}{\\partial \\mathbf{o}} \\in \\mathbb{R}^q$ | 损失对输出的梯度 |\n| 3 | $\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}$ | $= \\lambda \\mathbf{W}^{(1)}, \\lambda \\mathbf{W}^{(2)}$ | 正则化梯度 |\n| 4 | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}$ | $= \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}$ | **输出层权重梯度** |\n| 5 | $\\frac{\\partial J}{\\partial \\mathbf{h}}$ | $= \\mathbf{W}^{(2)\\top} \\frac{\\partial J}{\\partial \\mathbf{o}}$ | 反向传播到隐藏层 |\n| 6 | $\\frac{\\partial J}{\\partial \\mathbf{z}}$ | $= \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'(\\mathbf{z})$ | 逐元素乘法通过激活函数 |\n| 7 | $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}$ | $= \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}$ | **隐藏层权重梯度** |\n\n#### 关键公式详解\n\n**输出层权重梯度**（公式 4.7.11）：\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} = \\underbrace{\\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top}_{\\text{来自损失}} + \\underbrace{\\lambda \\mathbf{W}^{(2)}}_{\\text{来自正则化}}$$\n\n**隐藏层权重梯度**（公式 4.7.14）：\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} = \\underbrace{\\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top}_{\\text{链式传播}} + \\underbrace{\\lambda \\mathbf{W}^{(1)}}_{\\text{来自正则化}}$$\n\n---\n\n### 训练神经网络\n\n#### 前向传播与反向传播的依赖关系\n\n```\n迭代 t:\n    ┌─────────────────┐\n    │  前向传播        │ ← 使用 W⁽¹⁾ₜ, W⁽²⁾ₜ（来自上次迭代的反向传播）\n    │  计算 J, L, s   │\n    │  存储中间值 h    │\n    └────────┬────────┘\n             ▼\n    ┌─────────────────┐\n    │  反向传播        │ ← 使用存储的 h 计算梯度\n    │  得到 ∂J/∂W⁽¹⁾, ∂J/∂W⁽²⁾ │\n    └────────┬────────┘\n             ▼\n    ┌─────────────────┐\n    │  参数更新        │ → W⁽¹⁾ₜ₊₁, W⁽²⁾ₜ₊₁\n    │  (优化算法)      │\n    └─────────────────┘\n```\n\n#### 内存分析\n\n| 场景 | 内存需求 | 原因 |\n|:---|:---|:---|\n| **预测 (Inference)** | 低 | 只需当前层激活，无需保存中间值 |\n| **训练 (Training)** | 高 | 需保留所有中间值直到反向传播完成 |\n\n**内存占用因素**：\n- 网络层数量\n- 批量大小 (batch size)\n- 中间激活值的精度（FP32/FP16）\n\n> ⚠️ **OOM 风险**：更大批量 + 更深网络 → 更容易内存不足\n\n---\n\n## 代码与示例\n\n### 简化的反向传播实现示意\n\n```python\nimport torch\n\ndef forward_backward_demo():\n    \"\"\"\n    演示前向传播和反向传播的基本流程\n    注意：实际框架会自动处理，这里仅展示概念\n    \"\"\"\n    # 假设维度\n    d, h, q = 784, 256, 10  # 输入、隐藏、输出维度\n    batch_size = 64\n    \n    # 初始化参数（需要梯度）\n    W1 = torch.randn(h, d, requires_grad=True)  # W^(1): h × d\n    W2 = torch.randn(q, h, requires_grad=True)  # W^(2): q × h\n    \n    # 输入数据和标签\n    x = torch.randn(d, batch_size)      # 输入: d × batch\n    y_true = torch.randint(0, q, (batch_size,))\n    \n    lambda_reg = 0.01\n    \n    # ========== 前向传播 ==========\n    z = W1 @ x           # (h, d) @ (d, batch) = (h, batch)\n    h_act = torch.relu(z)  # 激活: ϕ(z)\n    o = W2 @ h_act       # (q, h) @ (h, batch) = (q, batch)\n    \n    # 损失计算（交叉熵 + L2正则）\n    loss_fn = torch.nn.CrossEntropyLoss()\n    L = loss_fn(o.T, y_true)  # 转置匹配维度: (batch, q) vs labels\n    \n    # L2 正则化: λ/2 * (||W1||²_F + ||W2||²_F)\n    s = (lambda_reg / 2) * (torch.norm(W1)**2 + torch.norm(W2)**2)\n    \n    J = L + s  # 目标函数\n    \n    print(f\"损失 L = {L.item():.4f}\")\n    print(f\"正则项 s = {s.item():.4f}\")\n    print(f\"目标函数 J = {J.item():.4f}\")\n    \n    # ========== 反向传播 ==========\n    J.backward()  # PyTorch 自动计算所有梯度\n    \n    # 查看梯度（对应公式 4.7.11 和 4.7.14）\n    print(f\"\\nW2 梯度形状: {W2.grad.shape}\")  # 应为 (q, h)\n    print(f\"W1 梯度形状: {W1.grad.shape}\")  # 应为 (h, d)\n    \n    # 手动验证 W2 梯度的结构（损失部分 + 正则部分）\n    # ∂J/∂W^(2) = (∂L/∂o) @ h^T + λ * W^(2)\n    \n    return W1.grad, W2.grad\n\n# 运行演示\ngrad_W1, grad_W2 = forward_backward_demo()\n```\n\n**输出解释**：\n- `W2.grad` 包含两部分：来自损失的梯度（外积形式）+ $\\lambda \\mathbf{W}^{(2)}$\n- `W1.grad` 同理，但需经过激活函数的导数（ReLU的导数为指示函数）\n\n---\n\n## 常见问题与避坑\n\n| 问题 | 原因 | 解决方案 |\n|:---|:---|:---|\n| **CUDA Out of Memory** | 批量太大或网络太深，中间值占用过多显存 | 减小 batch size；使用梯度累积；混合精度训练 |\n| **梯度消失/爆炸** | 深层网络链式法则连乘导致数值不稳定 | 使用批归一化；合适的权重初始化；残差连接 |\n| **忘记设置 `requires_grad=True`** | 参数无法计算梯度 | 检查所有可训练参数的梯度属性 |\n| **在推理时意外计算梯度** | 浪费内存和计算 | 使用 `torch.no_grad()` 或 `model.eval()` |\n| **修改计算图中间节点** | 破坏反向传播 | 避免在 `forward` 中直接修改张量，使用 `clone().detach()` |\n\n### 关键注意事项\n\n1. **中间值保留**：反向传播需要前向传播的中间结果（如 $\\mathbf{h}$），框架会自动管理，但会增加内存占用\n\n2. **计算图动态性**：PyTorch 默认动态图，每次前向传播都会重建计算图\n\n3. **二阶导数**：若需计算二阶导数，计算图会进一步扩展，内存和时间开销显著增加\n\n---\n\n## 复习清单\n\n- [ ] 能写出单隐藏层MLP的前向传播完整公式（含正则化）\n- [ ] 理解 Frobenius 范数与 $L_2$ 范数的关系\n- [ ] 能画出简单网络的计算图，标明数据流向\n- [ ] 掌握链式法则在张量上的应用（`prod` 操作）\n- [ ] 能推导输出层和隐藏层的权重梯度公式\n- [ ] 理解为什么训练比预测需要更多内存\n- [ ] 知道激活函数导数在反向传播中的作用（逐元素乘法）\n- [ ] 能解释前向传播和反向传播的相互依赖关系",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_d1abca741f08",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1MT411x7GH?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=3",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T05:42:31.424Z",
      "updatedAt": "2026-02-15T05:46:14.816Z",
      "resultMd": "## 目录\n\n- [核心概念摘要 (Key Takeaways)](#核心概念摘要-key-takeaways)\n- [知识点详解 (Detailed Notes)](#知识点详解-detailed-notes)\n  - [一、Kubernetes集群搭建方案对比](#一kubernetes集群搭建方案对比)\n  - [二、kubectl与API Server的关系](#二kubectl与api-server的关系)\n  - [三、Pod深度解析：探针与生命周期](#三pod深度解析探针与生命周期)\n  - [四、资源调度核心组件](#四资源调度核心组件)\n  - [五、服务网络：内外访问解决方案](#五服务网络内外访问解决方案)\n  - [六、存储与配置管理](#六存储与配置管理)\n  - [七、高级调度与认证授权](#七高级调度与认证授权)\n  - [八、运维管理篇](#八运维管理篇)\n  - [九、DevOps实战：阿里微服务部署](#九devops实战阿里微服务部署)\n- [避坑指南 & 常见QA (Tips & QA)](#避坑指南--常见qa-tips--qa)\n\n---\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **K8s集群搭建\"先难后易\"原则**：二进制安装虽繁琐，但能深入理解底层架构，排查问题更高效；命令行工具（如kubeadm）适合快速上手，但会屏蔽技术细节。\n2. **kubectl的本质是API封装**：所有K8s操作最终都通过REST API完成，kubectl只是将命令转换为HTTP请求的便捷工具。\n3. **Pod是K8s最小调度单元**：探针（Probe）实现应用健康检查与自动恢复，生命周期钩子支持容器启动前/终止前的自定义操作。\n4. **Label + Selector是资源管理的\"搜索引擎\"**：通过标签选择器实现灵活的资源筛选与关联，是Deployment、Service等组件的基础。\n5. **有状态 vs 无状态应用的差异化部署**：Deployment用于无状态应用，StatefulSet专为有状态应用（如MySQL）设计，保障数据持久性与身份稳定性。\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 一、Kubernetes集群搭建方案对比\n\n| 方案 | 特点 | 适用场景 | 生产环境建议 |\n|:---|:---|:---|:---|\n| **二进制安装** | 手动编译、逐组件部署，完全可控 | 深入学习K8s架构、企业定制需求 | ⭐⭐⭐ 最推荐，出问题可自主排查 |\n| **kubeadm** | 官方提供的命令行工具，简化初始化流程 | 快速搭建测试环境 | ⭐⭐☆ 可用，但需了解其封装逻辑 |\n| **Minikube/Kind** | 单机本地集群，一键启动 | 本地开发、学习入门 | ⭐☆☆ 仅限开发测试，**严禁生产** |\n| **云厂商托管版** | 阿里云ACK、AWS EKS等 | 企业快速上云 | ⭐⭐⭐ 省心，但需掌握底层原理 |\n\n> **讲师观点**：Minikube等工具\"虽然大部分情况下部署没问题，但不能排除它没有问题\"，官方也不建议用于生产环境。\n\n### 二、kubectl与API Server的关系\n\n```\n┌─────────────┐     HTTP/REST      ┌─────────────────┐\n│   kubectl   │ ═══════════════════▶│  kube-apiserver │\n│  (命令行工具)  │    自动转换请求      │   (统一入口网关)   │\n└─────────────┘                    └─────────────────┘\n        ▲                                    │\n        │         返回JSON/YAML结果            │\n        └────────────────────────────────────┘\n```\n\n**核心理解**：K8s的所有操作（创建Pod、查询资源等）最终都是向`kube-apiserver`发送RESTful请求。kubectl的价值在于：\n- 将复杂的HTTP请求封装为简洁的命令（如`kubectl get pods`）\n- 处理认证、序列化、结果格式化等繁琐细节\n\n### 三、Pod深度解析：探针与生命周期\n\n#### 3.1 探针（Probe）—— 容器的\"体检系统\"\n\n| 探针类型 | 作用时机 | 典型用途 |\n|:---|:---|:---|\n| `LivenessProbe` | 容器运行期间持续检测 | 应用卡死时自动重启容器 |\n| `ReadinessProbe` | 容器启动后检测 | 确保流量只打入就绪的应用 |\n| `StartupProbe` | 容器启动初期检测 | 保护慢启动应用不被误杀 |\n\n**核心价值**：实现\"自愈\"——服务异常挂掉后自动重启恢复，无需人工介入。\n\n#### 3.2 生命周期钩子（Lifecycle Hooks）\n\n```yaml\n# Pod生命周期关键阶段示意\nspec:\n  containers:\n    - name: app\n      lifecycle:\n        postStart:      # 容器启动后立即执行\n          exec:\n            command: [\"/bin/sh\", \"-c\", \"echo 'App starting' >> /var/log/lifecycle.log\"]\n        preStop:        # 容器终止前执行（优雅停机）\n          exec:\n            command: [\"/bin/sh\", \"-c\", \"sleep 10 && kill -SIGTERM 1\"]\n```\n\n**应用场景**：\n- `postStart`：预热缓存、注册服务发现\n- `preStop`：优雅关闭连接、保存状态、从负载均衡摘除\n\n### 四、资源调度核心组件\n\n| 组件 | 功能定位 | 关键特性 |\n|:---|:---|:---|\n| **Label & Selector** | 资源的\"标签检索系统\" | 通过键值对标记资源，支持等值/集合选择 |\n| **Deployment** | 无状态应用的\"控制器\" | 滚动更新、回滚、扩缩容、暂停/恢复 |\n| **StatefulSet** | 有状态应用的\"守护者\" | 稳定网络标识、有序部署、持久化存储绑定 |\n| **DaemonSet** | 节点级守护进程 | 每节点运行一个副本（日志收集、监控Agent） |\n| **HPA** | 水平自动扩缩容 | 基于CPU/内存/自定义指标自动调整副本数 |\n\n**有状态 vs 无状态的核心区别**：\n- **无状态**：任意实例可互换，数据不依赖本地存储（如Nginx）\n- **有状态**：实例有唯一身份，数据需持久化（如MySQL主从、ZooKeeper）\n\n### 五、服务网络：内外访问解决方案\n\n| 维度 | 技术组件 | 解决的问题 |\n|:---|:---|:---|\n| **集群内部通信** | Service (ClusterIP) | Pod IP动态变化，通过Service提供稳定虚拟IP |\n| **外部访问入口** | Ingress / NodePort / LoadBalancer | 将集群外流量路由到内部服务 |\n\n> 对比Docker：Docker容器间通信需手动配置网络，K8s通过Service和Ingress实现了开箱即用的服务发现与负载均衡。\n\n### 六、存储与配置管理\n\n#### 6.1 ConfigMap / Secret —— 动态配置注入\n\n**痛点解决**：传统方式修改配置文件需重启容器，K8s支持：\n- 配置与镜像解耦，热更新（需配合应用监听机制）\n- 敏感信息加密存储（Secret）\n\n#### 6.2 持久化存储（Persistent Volume）\n\n```\n┌─────────────┐     ┌─────────────┐     ┌─────────────────┐\n│    Pod      │────▶│  PVC (申请)  │────▶│  PV (实际存储)   │\n│  (MySQL容器)  │     │  指定容量/模式  │     │  NFS/Ceph/云盘等 │\n└─────────────┘     └─────────────┘     └─────────────────┘\n```\n\n**关键原则**：有状态应用的数据\"不能丢\"——MySQL数据丢失则服务无意义。\n\n### 七、高级调度与认证授权\n\n| 模块 | 能力 |\n|:---|:---|\n| **高级调度** | 亲和性/反亲和性、污点与容忍、优先级抢占，实现大规模集群（数百上千节点）的高效管理 |\n| **RBAC认证授权** | 基于角色的访问控制，细粒度管控用户/服务账号对资源的操作权限 |\n\n### 八、运维管理篇\n\n| 工具/技术 | 类比 | 核心功能 |\n|:---|:---|:---|\n| **Helm** | K8s界的\"Maven/NPM/yum\" | 包管理器，通过Chart模板化部署复杂应用 |\n| **Prometheus + Grafana** | 集群的\"体检中心\" | 多维度监控指标采集与可视化 |\n| **EFK/Loki** | 分布式\"日志仓库\" | 统一收集、检索、分析容器日志 |\n| **Dashboard** | Web化管理界面 | 图形化操作K8s资源 |\n\n> **学习建议**：务必先掌握命令行操作，再使用Dashboard。\"先难后易\"才能真正理解K8s在做什么，否则\"很难学得下去\"。\n\n### 九、DevOps实战：阿里微服务部署\n\n完整CI/CD流水线涉及的技术栈：\n\n```\n┌─────────┐    ┌──────────┐    ┌───────────┐    ┌─────────┐    ┌─────────┐\n│  GitLab │───▶│ SonarQube │───▶│  Jenkins  │───▶│ Harbor  │───▶│   K8s   │\n│ (代码仓库)│    │(质量门禁) │    │ (流水线编排)│    │(镜像仓库) │    │(生产部署) │\n└─────────┘    └──────────┘    └───────────┘    └─────────┘    └─────────┘\n        │                              │\n        └──────────────────────────────┘\n              触发构建 → 代码扫描 → 单元测试 → 打包镜像 → 自动化部署\n```\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n| 问题 | 解答/建议 |\n|:---|:---|\n| **Q: Minikube能快速搭建集群，为什么生产不推荐？** | A: Minikube是单节点虚拟机封装，存在隐藏问题；且无法模拟真实多节点网络、存储等复杂场景。 |\n| **Q: 直接学Dashboard跳过命令行不行？** | A: 强烈不建议。Dashboard是\"黑盒\"，不懂底层YAML结构和API原理，遇到故障无法排查。 |\n| **Q: 二进制安装太麻烦，有没有折中方案？** | A: 学习阶段用kubeadm理解初始化流程，生产环境根据团队能力选择二进制或云厂商托管版。 |\n| **⚠️ 易错点：探针配置不当导致频繁重启** | StartupProbe未配置时，LivenessProbe可能误判慢启动应用为\"死亡\"，引发无限重启循环。 |\n| **⚠️ 易错点：StatefulSet与Deployment混用** | 有状态应用误用Deployment会导致数据分散、身份混乱；必须识别业务特征后再选择工作负载类型。 |\n| **⚠️ 易错点：PV/PVC的绑定关系** | PVC是\"申请单\"，PV是\"实际房源\"；删除PVC不会自动释放PV数据，需手动回收策略配置。 |",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_a9e3345b96bf",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1MT411x7GH?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=3",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T04:57:50.783Z",
      "updatedAt": "2026-02-15T05:00:43.773Z",
      "resultMd": "```markdown\n# Kubernetes 实战进阶、运维管理及 DevOps 课程模块解读\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [一、集群搭建方案对比](#一集群搭建方案对比)\n  - [二、kubectl 与 REST API 的关系](#二kubectl-与-rest-api-的关系)\n  - [三、Pod 深入理解：探针与生命周期](#三pod-深入理解探针与生命周期)\n  - [四、资源调度核心组件](#四资源调度核心组件)\n  - [五、服务发布与网络](#五服务发布与网络)\n  - [六、存储与配置管理](#六存储与配置管理)\n  - [七、高级调度与认证授权](#七高级调度与认证授权)\n  - [八、运维管理篇](#八运维管理篇)\n  - [九、DevOps 实战部署](#九devops-实战部署)\n\n---\n\n## 核心概念摘要\n\n1. **K8s 集群搭建的\"学习悖论\"**：二进制安装最繁琐但最利于理解架构，命令行工具（如 kubeadm）最便捷但隐藏细节；生产环境推荐二进制或可靠工具，minikube 仅适合本地测试。\n2. **kubectl 的本质**：并非直接操作集群，而是封装了对 K8s REST API 的 HTTP 请求，降低用户操作门槛。\n3. **Pod 是 K8s 的最小调度单元**：通过**探针（Probe）**实现应用健康检查与自动恢复，通过**生命周期钩子**实现启动前/终止前的自定义操作。\n4. **Label + Selector 是 K8s 的\"导航系统\"**：一切资源查找、调度、服务发现都基于此机制。\n5. **从命令行到可视化**：必须先掌握 kubectl 命令原理，再使用 Dashboard 等界面工具，否则无法理解底层运作机制。\n\n---\n\n## 知识点详解\n\n### 一、集群搭建方案对比\n\n| 方案 | 特点 | 适用场景 | 讲师建议 |\n|:---|:---|:---|:---|\n| **二进制安装** | 手动编译、配置所有组件，过程透明 | 生产环境、深度定制 | ⭐⭐⭐ **最推荐学习**，能彻底理解架构 |\n| **kubeadm** | 官方命令行工具，一键初始化 | 生产环境快速部署 | 可用，但需了解其背后原理 |\n| **minikube** | 单机本地集群，轻量便捷 | 本地开发测试 | ❌ **不推荐用于生产**，存在潜在问题 |\n| **第三方 CLI 工具** | 社区工具众多，质量参差不齐 | 视工具可靠性而定 | 谨慎选择，优先官方方案 |\n\n> **关键洞察**：二进制安装的\"麻烦\"恰恰是其价值——亲手经历 etcd、apiserver、scheduler、controller-manager 等组件的部署，才能在故障排查时快速定位问题层级。\n\n---\n\n### 二、kubectl 与 REST API 的关系\n\n```\n┌─────────────┐     输入命令      ┌─────────────────┐     HTTP 请求      ┌─────────────┐\n│   用户       │ ───────────────→ │    kubectl      │ ────────────────→ │  kube-apiserver │\n│  (开发者)    │                  │  (命令行工具)    │                   │  (REST API)   │\n└─────────────┘                  └─────────────────┘                   └─────────────┘\n                                                                          ↑\n                                                                   所有 K8s 资源操作\n                                                                   均通过 RESTful API 完成\n```\n\n**核心结论**：K8s 的所有操作最终都转化为对 API Server 的 HTTP 请求。kubectl 只是提供了更友好的命令行界面，而非替代 API。\n\n---\n\n### 三、Pod 深入理解：探针与生命周期\n\n#### 3.1 探针（Probe）—— 应用的\"体检系统\"\n\n| 探针类型 | 作用时机 | 典型用途 |\n|:---|:---|:---|\n| `LivenessProbe` | 容器运行期间持续检测 | 应用卡死时自动重启容器 |\n| `ReadinessProbe` | 容器启动后检测 | 确保流量只打入就绪的应用实例 |\n| `StartupProbe` | 容器启动初期检测 | 保护慢启动应用，避免过早判定失败 |\n\n**价值**：实现应用的**自愈能力**——服务异常时无需人工介入，自动恢复。\n\n#### 3.2 生命周期钩子 —— 容器的\"里程碑事件\"\n\n```yaml\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n    - name: app\n      lifecycle:\n        postStart:      # 容器创建后立即执行\n          exec:\n            command: [\"/bin/sh\", \"-c\", \"echo 'Initializing...'\"]\n        preStop:        # 容器终止前执行（优雅停机）\n          exec:\n            command: [\"/bin/sh\", \"-c\", \"echo 'Cleaning up...'; sleep 30\"]\n```\n\n**应用场景**：\n- `postStart`：加载配置、预热缓存\n- `preStop`：保存状态、断开连接、平滑下线（配合 `terminationGracePeriodSeconds`）\n\n---\n\n### 四、资源调度核心组件\n\n| 组件 | 核心功能 | 典型使用场景 |\n|:---|:---|:---|\n| **Label & Selector** | 资源标记与筛选 | 所有资源关联的基础机制 |\n| **Deployment** | 声明式应用管理 | 滚动更新、回滚、扩缩容、暂停/恢复发布 |\n| **StatefulSet** | 有状态应用部署 | MySQL、Kafka、ZooKeeper 等需要稳定网络标识和持久存储的应用 |\n| **DaemonSet** | 节点级守护进程 | 日志收集（Fluentd）、监控代理（Node Exporter）、网络插件 |\n| **HPA（Horizontal Pod Autoscaler）** | 水平自动扩缩容 | 基于 CPU/内存/自定义指标动态调整 Pod 副本数 |\n\n> **有状态 vs 无状态**：Deployment 管理的 Pod 可任意替换（如 Nginx）；StatefulSet 为每个 Pod 分配固定序号和网络标识，保证数据一致性（如主从复制的数据库）。\n\n---\n\n### 五、服务发布与网络\n\nK8s 解决两大网络挑战：\n\n| 场景 | 技术方案 | 说明 |\n|:---|:---|:---|\n| **集群内部通信** | Service（ClusterIP）+ DNS | Pod IP 动态变化，Service 提供稳定的虚拟 IP 和负载均衡 |\n| **外部访问集群** | Ingress / NodePort / LoadBalancer | Ingress 为 L7 路由（HTTP/HTTPS），NodePort 为端口映射，LoadBalancer 对接云厂商 SLB |\n\n**对比 Docker**：Docker 原生网络隔离性强但跨主机通信复杂；K8s 通过 CNI 插件（如 Calico、Flannel）统一了集群网络模型。\n\n---\n\n### 六、存储与配置管理\n\n#### 6.1 ConfigMap / Secret —— 动态配置注入\n\n```yaml\n# 传统方式：修改配置文件 → 重新打包镜像 → 重启容器（繁琐且风险高）\n# K8s 方式：配置与镜像分离，热更新无需重建 Pod\n\napiVersion: v1\nkind: ConfigMap\ndata:\n  database.properties: |\n    db.host=mysql-service\n    db.port=3306\n```\n\n挂载方式：\n- **环境变量**：适合简单键值对，Pod 启动后不可变\n- **Volume 挂载**：配置文件形式，支持动态更新（需应用支持热加载）\n\n#### 6.2 持久化存储（Persistent Volume）\n\n| 概念 | 角色 | 类比 |\n|:---|:---|:---|\n| PV（PersistentVolume） | 集群层面的存储资源 | 硬盘 |\n| PVC（PersistentVolumeClaim） | 用户对存储的申请 | 购物订单 |\n| StorageClass | 动态供应模板 | 自动售货机规格 |\n\n**关键原则**：有状态应用的数据必须持久化，MySQL 数据丢失 = 服务不可用。\n\n---\n\n### 七、高级调度与认证授权\n\n#### 7.1 高级调度特性\n\n利用 **Taints/Tolerations**、**Node Affinity**、**Pod Affinity/Anti-Affinity** 等机制，可实现：\n- 专用节点隔离（如 GPU 节点只跑 AI 任务）\n- 同应用多副本打散部署（高可用）\n- 批量管理数百上千节点的单一命令操作\n\n#### 7.2 RBAC 认证授权\n\n| 组件 | 作用 |\n|:---|:---|\n| Authentication（认证） | 验证\"你是谁\"（证书、Token、OIDC 等） |\n| Authorization（授权） | 验证\"你能做什么\"（RBAC 规则：Role/ClusterRole + RoleBinding） |\n\n> Dashboard 等可视化界面必须配合 RBAC，否则存在严重安全隐患。\n\n---\n\n### 八、运维管理篇\n\n| 模块 | 工具/方案 | 核心价值 |\n|:---|:---|:---|\n| **包管理器 Helm** | K8s 的 \"yum/apt/maven/npm\" | Chart 封装完整应用（Deployment + Service + ConfigMap + ...），一键安装/升级/回滚 |\n| **集群监控** | Prometheus + Grafana | 采集节点/Pod/容器指标，可视化展示资源使用与告警 |\n| **日志管理** | EFK（Elasticsearch + Fluentd + Kibana）或 PLG 栈 | 集中收集分布式日志，统一检索分析 |\n| **可视化界面** | Dashboard / Rancher / KubeSphere | 图形化操作降低使用门槛，但**必须在掌握命令行后学习** |\n\n> **学习路径建议**：先难后易。直接上手 Dashboard 会导致\"知其然不知其所以然\"，遇到异常无法排查。\n\n---\n\n### 九、DevOps 实战部署\n\n基于阿里云 ACK 的完整 CI/CD 流水线：\n\n```\n┌─────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────┐\n│  GitLab │ → │  SonarQube  │ → │   Jenkins   │ → │  Harbor     │ → │   ACK   │\n│ (源码管理)│    │(代码质量扫描) │    │ (流水线编排) │    │ (镜像仓库)   │    │(K8s集群) │\n└─────────┘    └─────────────┘    └─────────────┘    └─────────────┘    └─────────┘\n                                      ↓\n                              自动化测试 → 构建镜像 → 推送仓库 → 触发部署\n```\n\n**涉及技术栈**：\n- **GitLab**：源代码版本控制\n- **SonarQube**：静态代码分析（漏洞、坏味道、测试覆盖率）\n- **Jenkins**：Pipeline 流水线（构建、测试、部署自动化）\n- **Harbor**：企业级 Docker 镜像仓库（含安全扫描）\n- **ACK（阿里云容器服务）**：托管 K8s 集群，承载最终应用\n\n---\n\n## 避坑指南 & 常见 QA\n\n| 坑点 | 说明 | 规避方法 |\n|:---|:---|:---|\n| **minikube 上生产** | 单节点、无高可用，官方明确不建议 | 仅用于本地学习，生产用 kubeadm 或云厂商托管版 |\n| **跳过命令行直接学 Dashboard** | 无法理解资源关系，排障能力为零 | 强制自己用 kubectl 完成所有操作至少 2 周 |\n| **忽略探针配置** | 应用已崩溃但 K8s 认为健康，流量持续打入 | 务必配置合理的 `livenessProbe` 和 `readinessProbe` |\n| **有状态应用用 Deployment** | 数据随 Pod 重建而丢失 | MySQL、Redis 等必须用 StatefulSet + PVC |\n| **ConfigMap 挂载后不生效** | 应用未监听文件变化或 K8s 未触发更新 | 使用支持热加载的框架，或配合 Reloader 等工具 |\n| **Helm 版本混乱** | Helm v2 与 v3 差异巨大，社区 Chart 兼容性不一 | 统一使用 Helm v3，阅读 Chart 的 values.yaml 后再部署 |\n\n**代表性问答**：\n\n> **Q**：为什么二进制安装比 kubeadm 更适合学习？  \n> **A**：kubeadm 黑盒化了证书生成、静态 Pod 部署、kubeconfig 配置等关键步骤。二进制安装强迫你理解每个组件的启动参数、依赖关系和通信机制，这是生产排障的必备知识。\n\n> **Q**：HPA 和 Deployment 的 replicas 冲突怎么办？  \n> **A**：启用 HPA 后应删除 Deployment 中的 `replicas` 字段，由 HPA 控制器全权管理副本数，避免两者竞争。\n\n> **Q**：Ingress 和 Service 的区别？  \n> **A**：Service 工作在 L4（传输层，TCP/UDP），提供集群内负载均衡；Ingress 工作在 L7（应用层，HTTP/HTTPS），提供基于域名/路径的路由、SSL 终止等高级功能。\n```",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_19a17ccb7048",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=32",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T04:26:10.683Z",
      "updatedAt": "2026-02-15T04:30:46.453Z",
      "resultMd": "```markdown\n# 目录\n\n- [核心概念摘要 (Key Takeaways)](#核心概念摘要-key-takeaways)\n- [知识点详解 (Detailed Notes)](#知识点详解-detailed-notes)\n  - [1. 多态的核心价值：解耦与扩展性](#1-多态的核心价值解耦与扩展性)\n  - [2. 面向对象三大特性的演进关系](#2-面向对象三大特性的演进关系)\n  - [3. 抽象类：制定行业标准](#3-抽象类制定行业标准)\n- [代码实战 / 算法推导 (Code & Math)](#代码实战--算法推导-code--math)\n  - [空调案例完整实现](#空调案例完整实现)\n- [避坑指南 & 常见QA (Tips & QA)](#避坑指南--常见qa-tips--qa)\n\n---\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **多态的本质是\"解耦\"**：通过统一的接口调用，实现模块间的松耦合。平台（调用方）无需修改即可兼容未来扩展的新类型。\n2. **Python的\"伪多态\"特性**：Python作为动态弱类型语言，无法像Java/C++那样在编译期精准限定类型，运行时不满足条件也能执行，属于\"鸭子类型\"的灵活多态。\n3. **抽象类 = 行业标准**：包含抽象方法的类称为抽象类，其核心作用是**制定规范**——规定\"必须做什么\"，但不约束\"怎么做\"。\n4. **三大OOP特性的互补关系**：封装解决安全性但增加代码量 → 继承提高复用性但带来强耦合 → 多态实现解耦扩展但无法精准限类型。\n5. **子类必须实现全部抽象方法**：若子类未完全实现父类的抽象方法，则该子类仍为抽象类，无法实例化。\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 1. 多态的核心价值：解耦与扩展性\n\n#### 1.1 游戏对战平台的经典比喻\n讲师以《王者荣耀》为例阐释多态的扩展性：\n- **固定机制**：地图规则、野怪刷新、防御塔机制等由平台统一实现\n- **可变英雄**：100+英雄各自拥有独特技能，但都必须遵循\"移动/攻击/释放技能\"的统一接口\n- **扩展优势**：新增英雄（如\"卷帘大将\"）时，**无需修改任何现有代码**，只需让新英雄继承标准接口即可接入对战\n\n> 关键结论：**机制是一套，至于传过来的是谁，机制不会随内容改变而改变**——这就是可扩展性。\n\n#### 1.2 多态的\"解耦\"含义\n| 角色 | 职责 |\n|:---|:---|\n| **平台（调用者）** | 定义统一接口，编写通用处理逻辑 |\n| **具体类（被调用者）** | 实现接口规定的具体行为 |\n| **结果** | 双方通过抽象层交互，彼此不依赖具体实现 |\n\n---\n\n### 2. 面向对象三大特性的演进关系\n\n```\n┌─────────────────────────────────────────┐\n│  问题：如何保护数据安全？                  │\n│  → 封装（Encapsulation）                 │\n│    ✓ 优点：安全性（私有属性）、可控性        │\n│    ✗ 缺点：代码量增加（需写getter/setter）   │\n└─────────────────────────────────────────┘\n                    ↓\n┌─────────────────────────────────────────┐\n│  问题：如何减少重复代码？                  │\n│  → 继承（Inheritance）                   │\n│    ✓ 优点：提高代码复用性                  │\n│    ✗ 缺点：耦合性增强（父类变更强制影响子类） │\n└─────────────────────────────────────────┘\n                    ↓\n┌─────────────────────────────────────────┐\n│  问题：如何实现灵活扩展？                  │\n│  → 多态（Polymorphism）                  │\n│    ✓ 优点：解耦、可扩展、统一接口          │\n│    ✗ 缺点：无法精准限定类型（Python特有）    │\n└─────────────────────────────────────────┘\n```\n\n**讲师形象总结**：\n- 继承 = \"啃老\"：复用父辈的东西\n- 多态 = \"啃方向\"：在不修改的情况下，扩展使用后来人写的内容\n\n---\n\n### 3. 抽象类：制定行业标准\n\n#### 3.1 核心定义\n| 术语 | 定义 |\n|:---|:---|\n| **抽象方法** | 只有方法声明、没有方法体（`pass`或`raise NotImplementedError`）的方法 |\n| **抽象类** | 包含至少一个抽象方法的类 |\n| **普通类** | 不包含抽象方法的类 |\n\n#### 3.2 现实类比：国家排放标准\n- **抽象类** = 国家发布的\"国六排放标准\"\n- **具体厂商** = 吉利、比亚迪、特斯拉等\n- **实现方式**：各厂商技术路线各异（混动/纯电/氢能源），但最终必须达标\n\n#### 3.3 Python中的实现要点\n- 需借助 `abc` 模块（Abstract Base Classes）\n- 使用 `@abstractmethod` 装饰器标记抽象方法\n- **强制约束**：子类必须实现所有抽象方法，否则无法实例化\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### 空调案例完整实现\n\n```python\nfrom abc import ABC, abstractmethod\n\n\n# ==================== 步骤1：定义抽象类（制定行业标准）====================\nclass AC(ABC):\n    \"\"\"\n    空调抽象基类：国家规定空调必须具备的功能标准\n    \"\"\"\n    \n    @abstractmethod\n    def cool_wind(self) -> None:\n        \"\"\"制冷功能：必须实现，具体技术不限\"\"\"\n        pass\n    \n    @abstractmethod\n    def hot_wind(self) -> None:\n        \"\"\"制热功能：必须实现，具体技术不限\"\"\"\n        pass\n    \n    @abstractmethod\n    def swing_l_r(self) -> None:\n        \"\"\"左右摆风功能：必须实现，具体方式不限\"\"\"\n        pass\n\n\n# ==================== 步骤2：具体厂商实现（各显神通）====================\nclass XiaomiAC(AC):\n    \"\"\"\n    小米空调：采用自研技术实现标准\n    \"\"\"\n    \n    def cool_wind(self) -> None:\n        # 小米核心技术：小冰块制冷技术\n        print(\"小米空调：小冰块核心制冷技术启动\")\n    \n    def hot_wind(self) -> None:\n        # 小米核心技术：小太阳制热技术\n        print(\"小米空调：小太阳核心制热技术启动\")\n    \n    def swing_l_r(self) -> None:\n        # 小米特色：静音左右摆风\n        print(\"小米空调：静音左右摆风\")\n\n\nclass GreeAC(AC):\n    \"\"\"\n    格力空调：采用传统压缩机技术实现标准\n    \"\"\"\n    \n    def cool_wind(self) -> None:\n        print(\"格力空调：变频压缩机核心制冷\")\n    \n    def hot_wind(self) -> None:\n        print(\"格力空调：电辅热核心制热\")\n    \n    def swing_l_r(self) -> None:\n        print(\"格力空调：机械摆风\")\n\n\n# ==================== 错误示例：未完全实现抽象方法 ====================\n# class BadAC(AC):\n#     def cool_wind(self) -> None:\n#         print(\"只实现了一个方法\")\n#     \n#     # 缺少 hot_wind 和 swing_l_r 的实现\n# \n# # 尝试实例化会报错：TypeError: Can't instantiate abstract class BadAC \n# # with abstract methods hot_wind, swing_l_r\n\n\n# ==================== 步骤3：统一测试平台（多态调用）====================\ndef test_ac_platform(ac_unit: AC) -> None:\n    \"\"\"\n    空调测试平台：同一套代码测试不同品牌\n    体现多态性——接收不同类型的AC对象，执行相同接口调用\n    \"\"\"\n    print(f\"\\n{'='*20} 正在测试 {type(ac_unit).__name__} {'='*20}\")\n    ac_unit.cool_wind()\n    ac_unit.hot_wind()\n    ac_unit.swing_l_r()\n\n\n# ==================== 主程序入口 ====================\nif __name__ == \"__main__\":\n    # 创建不同品牌的空调实例\n    xiaomi = XiaomiAC()\n    gree = GreeAC()\n    \n    # 同一平台，不同效果\n    test_ac_platform(xiaomi)   # 输出小米的技术实现\n    test_ac_platform(gree)     # 输出格力的技术实现\n    \n    # 扩展性验证：新增美的空调，无需修改test_ac_platform\n    class MideaAC(AC):\n        def cool_wind(self) -> None:\n            print(\"美的空调：无风感制冷\")\n        def hot_wind(self) -> None:\n            print(\"美的空调：地暖式制热\")\n        def swing_l_r(self) -> None:\n            print(\"美的空调：3D立体摆风\")\n    \n    midea = MideaAC()\n    test_ac_platform(midea)    # 直接可用，平台代码零修改\n```\n\n**输出结果**：\n```\n==================== 正在测试 XiaomiAC ====================\n小米空调：小冰块核心制冷技术启动\n小米空调：小太阳核心制热技术启动\n小米空调：静音左右摆风\n\n==================== 正在测试 GreeAC ====================\n格力空调：变频压缩机核心制冷\n格力空调：电辅热核心制热\n格力空调：机械摆风\n\n==================== 正在测试 MideaAC ====================\n美的空调：无风感制冷\n美的空调：地暖式制热\n美的空调：3D立体摆风\n```\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n| 问题 | 解答 |\n|:---|:---|\n| **Q1：Python没有抽象类也能实现多态，为什么还要用？** | 抽象类提供**强制约束**。无抽象类时，子类可自由选择是否实现方法；有抽象类时，未实现全部抽象方法会导致实例化失败，提前暴露问题。 |\n| **Q2：子类少实现一个抽象方法会怎样？** | 运行时抛出 `TypeError: Can't instantiate abstract class Xxx with abstract methods yyy`。这是Python的保护机制，确保标准被严格执行。 |\n| **Q3：抽象类能否包含普通方法？** | **可以**。抽象类中可同时存在抽象方法（强制子类实现）和普通方法（提供默认实现供子类继承或重写）。 |\n| **Q4：Python vs Java/C++的多态差异？** | Python是**鸭子类型**（duck typing）：不检查类型，只检查行为。`Animal a = new Dog()` 在Python中只是变量重新绑定，无编译期类型检查，因此\"无法精准限定类型\"——传个`Car`对象进去，只要它有同名方法，代码也能跑，但逻辑可能出错。 |\n| **Q5：如何理解\"继承是啃老，多态是啃方向\"？** | 继承关注**复用已有代码**（纵向）；多态关注**扩展未来能力**（横向）。平台代码写好后，新类型横向接入即可，无需纵向修改历史代码。 |\n| **⚠️ 易错点：忘记导入abc模块** | 手写`@abstractmethod`而不`from abc import ABC, abstractmethod`会导致装饰器失效，类变成普通类，失去强制约束能力。 |\n| **⚠️ 易错点：抽象类直接实例化** | 即使抽象类中所有方法都有默认实现，只要含`@abstractmethod`，就不能直接实例化，必须通过子类。 |\n\n---",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_dd69a9613241",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=29",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-15T03:27:43.347Z",
      "updatedAt": "2026-02-15T03:38:12.702Z",
      "resultMd": "## 目录\n\n- [核心概念摘要 (Key Takeaways)](#核心概念摘要-key-takeaways)\n- [知识点详解 (Detailed Notes)](#知识点详解-detailed-notes)\n  - [1. 封装的核心概念](#1-封装的核心概念)\n  - [2. 私有属性与私有方法](#2-私有属性与私有方法)\n  - [3. 访问控制机制：Getter 与 Setter](#3-访问控制机制getter-与-setter)\n  - [4. 面向对象三大特性的设计逻辑](#4-面向对象三大特性的设计逻辑)\n- [代码实战 / 算法推导 (Code & Math)](#代码实战--算法推导-code--math)\n  - [案例：师傅的私房钱](#案例师傅的私房钱)\n  - [关键执行流程图解](#关键执行流程图解)\n- [避坑指南 & 常见QA (Tips & QA)](#避坑指南--常见qa-tips--qa)\n\n---\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **封装的本质**：将对象的属性（数据）和行为（方法）隐藏起来，仅对外暴露公共的访问接口，是面向对象三大特性（封装、继承、多态）的基础。\n2. **Python 私有化语法**：通过双下划线前缀 `__`（如 `__money`）实现属性或方法的私有化，使其在类外部无法直接访问。\n3. **访问控制的实现**：类内部可直接访问私有成员；类外部需通过公共的 Getter/Setter 方法间接访问，实现对数据的安全管控。\n4. **封装的双刃剑效应**：私有化提升安全性，但会增加代码量；**继承**的出现正是为了解决封装带来的代码冗余问题，而**多态**又解决了继承导致的耦合性过强问题。\n5. **命名规范区分**：`__xxx__`（前后双下划线）为 Python 内置属性/方法；`__xxx`（仅前双下划线）为用户自定义私有成员。\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 1. 封装的核心概念\n\n| 维度 | 说明 |\n|:---|:---|\n| **定义** | 隐藏对象的属性和实现细节，仅对外提供公共的访问方式 |\n| **表现形式** | ① 函数（将代码封装到函数中）；② 类（将属性和行为封装到类中） |\n| **核心比喻** | 电脑/手机外壳——用户只能看到最终效果，无法直接接触内部电路、风扇等组件 |\n\n> **讲师金句**：\"你写的电脑和手机就是封装。你能不能透过电脑的壳子，直接看到里面的主板、风扇？看不到，只能看到最终呈现的效果。\"\n\n### 2. 私有属性与私有方法\n\n#### 2.1 私有化语法规则\n\n```python\n# 私有属性\nself.__属性名      # 正确：仅前面加双下划线\n\n# 私有方法\ndef __方法名(self):  # 正确：仅前面加双下划线\n    pass\n```\n\n| 命名形式 | 含义 | 示例 |\n|:---|:---|:---|\n| `__name` | **用户自定义私有成员** | `self.__money` |\n| `__name__` | Python 内置特殊属性/方法 | `__init__`, `__str__` |\n\n> **关键区分**：前后都加双下划线是 Python 内置属性的\"马甲\"，用户自己写的代码只需前面加双下划线即可实现私有化。\n\n#### 2.2 访问权限对比\n\n| 场景 | 公有属性/方法 | 私有属性/方法 |\n|:---|:---|:---|\n| **类内部** | ✅ 直接访问 | ✅ 直接访问 |\n| **类外部** | ✅ 直接访问 | ❌ 无法直接访问（需通过公共方法）|\n\n> **心脏比喻**：私有内容就像你的心脏——在体内（类内）与其他器官自由协作，但无法拿到体外（类外）独立工作。若需外部干预，必须通过专业手段（医生/公共接口）。\n\n### 3. 访问控制机制：Getter 与 Setter\n\n当属性被私有化后，需提供公共方法来控制外部访问：\n\n| 方法类型 | 命名约定 | 功能 | 类比 |\n|:---|:---|:---|:---|\n| **Getter** | `get_属性名()` | 获取私有属性的值 | ATM 查询余额 |\n| **Setter** | `set_属性名(值)` | 修改私有属性的值 | ATM 存款/取款 |\n\n**精细控制策略**：\n- **只读模式**：仅提供 Getter，不提供 Setter → 外界只能看不能改\n- **读写模式**：同时提供 Getter 和 Setter → 外界可查看和修改\n- **完全封闭**：两者都不提供 → 仅在类内部使用\n\n> **ATM 机比喻**：你无法伸手进 ATM 机直接拿钱（私有属性），必须通过插卡、输密码的规范流程（公共接口）来操作。而且 ATM 机可以设计成\"只能取钱\"（只有 Getter）或\"既能取又能存\"（Getter + Setter）。\n\n### 4. 面向对象三大特性的设计逻辑\n\n```\n┌─────────────────────────────────────────────────────────┐\n│  问题演化链：技术演进是为了解决前一个技术的弊端              │\n├─────────────────────────────────────────────────────────┤\n│  【封装】                                                │\n│   ↓ 优势：提高安全性                                     │\n│   ↓ 弊端：代码量增加（每个私有属性都需写 Getter/Setter）   │\n│                                                          │\n│  【继承】← 解决封装代码冗余问题                           │\n│   ↓ 优势：提高代码复用性（子类继承父类的全部功能）          │\n│   ↓ 弊端：耦合性增强（子类被迫继承父类所有成员，想要没有都不行）│\n│                                                          │\n│  【多态】← 解决继承耦合过强问题                           │\n│   ↓ 优势：增强灵活性（同一接口，不同实现）                 │\n└─────────────────────────────────────────────────────────┘\n```\n\n> **讲师核心观点**：\"先理解封装，再理解继承。封装确实提高了安全性，但弊端是代码量增加了。为了解决这个问题，于是有了继承这个技术。继承解决了复用问题，但又带来耦合性强的弊端，于是又有了多态。\"\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### 案例：师傅的私房钱\n\n```python\nclass Master:\n    \"\"\"师傅类：拥有技术和私房钱\"\"\"\n    \n    def __init__(self):\n        self.skill = \"Python开发技术\"      # 公有属性：可传给徒弟\n        self.__money = 20000               # 私有属性：私房钱，不想直接给徒弟\n    \n    # ========== Getter：允许徒弟查看私房钱（只读）==========\n    def get_money(self):\n        \"\"\"获取私房钱金额\"\"\"\n        return self.__money\n    \n    # ========== Setter：允许徒弟\"花\"师傅的钱（受控写入）==========\n    def set_money(self, money):\n        \"\"\"设置私房钱金额（可添加校验逻辑）\"\"\"\n        # 此处可加入业务校验：如花钱不能超过余额等\n        self.__money = money\n\n\nclass Apprentice(Master):\n    \"\"\"徒弟类：继承自师傅\"\"\"\n    pass  # 暂时不添加新功能，测试继承关系\n\n\n# ==================== 测试代码 ====================\nif __name__ == \"__main__\":\n    # 创建徒弟实例\n    ts = Apprentice()\n    \n    # 1. 访问公有属性：成功\n    print(f\"学到技术: {ts.skill}\")        # 输出: 学到技术: Python开发技术\n    \n    # 2. 直接访问私有属性：失败！AttributeError\n    # print(ts.__money)                   # ❌ 报错：'Apprentice' object has no attribute '__money'\n    \n    # 3. 通过 Getter 访问私有属性：成功\n    print(f\"师父私房钱: {ts.get_money()}\")  # 输出: 师父私房钱: 20000\n    \n    # 4. 尝试\"篡改\"：实际上创建了新属性，不影响真正的私有属性\n    ts.__money = 100                      # ⚠️ 危险操作！这创建了一个新属性，而非修改原私有属性\n    print(f\"ts.__money = {ts.__money}\")   # 输出: ts.__money = 100（假属性）\n    print(f\"真私房钱: {ts.get_money()}\")   # 输出: 真私房钱: 20000（原私有属性未被修改）\n    \n    # 5. 通过 Setter 真正修改私有属性\n    ts.set_money(15000)\n    print(f\"花钱后剩余: {ts.get_money()}\")  # 输出: 花钱后剩余: 15000\n```\n\n### 关键执行流程图解\n\n```\n┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n│   外部调用者     │────→│  公共接口方法    │────→│   私有属性      │\n│  (ts.get_money) │     │  (Master类内部)  │     │  (__money)      │\n└─────────────────┘     └─────────────────┘     └─────────────────┘\n                              ↑\n                         类内部可直接访问\n                         \n【名称修饰机制】\nPython 会对私有属性进行名称改写（Name Mangling）：\n    __money  →  _Master__money\n    \n因此 ts.__money = 100 实际上是创建了新的实例属性，\n而非修改原有的私有属性。\n```\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n| 问题 | 解答 |\n|:---|:---|\n| **Q1: 私有属性真的完全无法从外部访问吗？** | 技术上可通过 `_类名__属性名`（如 `_Master__money`）强行访问，但这破坏封装原则，**严禁生产环境使用**。 |\n| **Q2: 为什么 `ts.__money = 100` 没有报错，但也没修改成功？** | Python 的动态特性允许随时给实例添加新属性。这里的 `ts.__money` 是一个**全新的公有属性**，与原有的私有属性 `__money` 完全无关。 |\n| **Q3: Getter/Setter 的方法名可以自定义吗？** | 可以，但**强烈建议遵循 `get_xxx` / `set_xxx` 约定**。后续学习装饰器（`@property`）时，标准命名能无缝衔接。 |\n| **Q4: 什么时候应该用私有属性？** | ① 敏感数据（密码、余额等）；② 内部状态标志；③ 需要严格控制读写权限的属性。简单数据可直接用公有属性。 |\n| **Q5: 继承时子类能访问父类的私有属性吗？** | **不能直接访问**。子类需通过父类提供的公共方法（Getter/Setter）间接访问，这是封装的安全边界。 |\n| **⚠️ 易错点：双下划线位置** | 私有化只需**前面**加双下划线 `__xxx`。前后都加 `__xxx__` 会变成 Python 内置属性风格，虽功能相同但语义混淆。 |\n\n---\n\n```markdown\n<!-- 文件信息 -->\n文件名: python_oop_encapsulation_notes.md\n主题: Python面向对象高级-封装与私有成员\n适用: Python基础/面向对象编程复习\n```",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_68963f0e1745",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1MT411x7GH/?spm_id_from=333.788.player.switch&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=2",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-14T17:16:52.767Z",
      "updatedAt": "2026-02-14T17:33:20.331Z",
      "resultMd": "## 目录\n\n- [核心概念摘要 (Key Takeaways)](#核心概念摘要-key-takeaways)\n- [知识点详解 (Detailed Notes)](#知识点详解-detailed-notes)\n  - [1. 课程定位与前置要求](#1-课程定位与前置要求)\n  - [2. Kubernetes vs Docker：复杂度对比](#2-kubernetes-vs-docker复杂度对比)\n  - [3. 核心概念篇的学习方法论](#3-核心概念篇的学习方法论)\n  - [4. 核心概念详解](#4-核心概念详解)\n    - [4.1 基础介绍与集群架构](#41-基础介绍与集群架构)\n    - [4.2 服务的无状态与有状态](#42-服务的无状态与有状态)\n    - [4.3 资源（Resource）与对象（Object）](#43-资源resource与对象object)\n    - [4.4 资源的三大分类](#44-资源的三大分类)\n    - [4.5 命名空间级别的核心资源](#45-命名空间级别的核心资源)\n- [避坑指南 & 常见QA (Tips & QA)](#避坑指南--常见qa-tips--qa)\n\n---\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **K8s是平台级技术，复杂度远超Docker**：主题数534 vs 240，概念量翻倍，必须先理解理论再实践\n2. **\"一切皆资源\"是K8s的核心理念**：类比Java中\"一切皆对象\"、Linux中\"一切皆文件\"\n3. **Pod是K8s原创且最重要的概念**：调度最小单位，所有上层资源最终都表现为Pod形态\n4. **控制器是对Pod的封装抽象**：Deployment、ReplicaSet、StatefulSet等6种核心控制器对应不同场景（无状态/有状态/守护进程/任务）\n5. **Service与Ingress是网络核心**：Service解决集群内通信，Ingress是网关抽象（Nginx是其一种实现）\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 1. 课程定位与前置要求\n\n| 要求级别 | 技术 | 说明 |\n|---------|------|------|\n| **必须掌握** | Linux基础命令 | `ls`, `cat`, `cd`, 文件操作等，无需复杂脚本能力 |\n| **必须掌握** | Docker | K8s用于管理Docker容器，概念强耦合；不会Docker无法学习 |\n| **可选了解** | Redis | 仅最后两章用到，当作普通软件安装即可 |\n| **建议具备** | Spring Cloud + Java微服务 | 仅实战篇需要，前面三章无要求 |\n\n> **讲师建议**：技术学习要有真实需求驱动，单纯追热点效率低下。\n\n---\n\n### 2. Kubernetes vs Docker：复杂度对比\n\n| 指标 | Docker | Kubernetes |\n|-----|--------|-----------|\n| 主题数 | ~240 | **534**（翻倍还多） |\n| 定位 | 容器引擎 | **容器编排平台** |\n| 核心概念量 | 较少 | 极其庞大 |\n\n**关键洞察**：K8s的复杂度源于其\"平台化\"设计——不仅是跑容器，而是完整的数据中心操作系统。\n\n---\n\n### 3. 核心概念篇的学习方法论\n\n```\n理论学习 → 实操命令 → 结合回顾\n    ↑___________________↓\n```\n\n**讲师强调的三阶段循环**：\n1. **先讲透概念**：不急于命令操作，确保理解\"这是什么、能干什么\"\n2. **再学命令**：在实操篇将命令与理论对应\n3. **双向绑定**：操作时回顾理论，形成知识闭环\n\n> **警告**：若跳过概念直接学命令，后续听到Deployment、Service、StatefulSet会完全懵掉。\n\n---\n\n### 4. 核心概念详解\n\n#### 4.1 基础介绍与集群架构\n\n| 模块 | 内容要点 |\n|-----|---------|\n| K8s基本概念 | 名称由来、本质定义、为什么需要它 |\n| 三足鼎立 | K8s在容器编排领域的地位（与Docker Swarm、Mesos对比） |\n| 集群架构 | 逻辑上永远是集群视角，无论物理上是单节点还是多节点 |\n| 分层架构 | 各组件调用关系、上下游依赖 |\n\n#### 4.2 服务的无状态与有状态\n\n| 类型 | 特征 | K8s处理方式 |\n|-----|------|-----------|\n| **无状态服务** | 请求之间无依赖，可任意水平扩展 | Deployment等控制器 |\n| **有状态服务** | 需要持久化数据、身份标识、有序部署 | StatefulSet等控制器 |\n\n> 接口层面同样区分：无状态接口 vs 有状态接口\n\n#### 4.3 资源（Resource）与对象（Object）\n\n**类比理解**：\n\n| 类比体系 | 模板/类 | 实例/对象 |\n|---------|--------|----------|\n| **Java** | Class | Object |\n| **制造业** | 设计图纸 | 具体产品 |\n| **K8s** | **Resource（资源）** | **Object（对象）** |\n\n- **资源**：不变的定义（如Deployment的配置模板）\n- **对象**：基于资源创建的运行时实例\n- **关系**：一个资源 → 多个对象\n\n#### 4.4 资源的三大分类\n\n| 分类 | 作用范围 | 典型代表 |\n|-----|---------|---------|\n| **元数据型** | 集群级 | 如Namespace本身的定义 |\n| **集群级** | 整个集群 | Node、ClusterRole等 |\n| **命名空间级** | 特定Namespace内 | **46+种核心资源**（重点）|\n\n> 命名空间级资源占日常使用的90%以上，是学习的重中之重。\n\n#### 4.5 命名空间级别的核心资源\n\n##### 4.5.1 工作负载型（Workload）\n\n```\n工作负载型\n├── Pod（原生形态）← K8s原创，调度最小单位\n│   └── 所有上层资源最终都表现为Pod\n└── 控制器（对Pod的封装抽象）\n    ├── 无状态服务：Deployment、ReplicaSet\n    ├── 有状态服务：StatefulSet\n    ├── 守护进程：DaemonSet\n    └── 任务型：Job、CronJob\n```\n\n**核心控制器优先级**：\n| 优先级 | 控制器 | 适用场景 |\n|-------|--------|---------|\n| ⭐⭐⭐ | **Deployment** | 无状态应用（最常用） |\n| ⭐⭐⭐ | **ReplicaSet** | 副本控制（通常被Deployment托管） |\n| ⭐⭐⭐ | **StatefulSet** | 有状态应用（数据库等） |\n| ⭐⭐☆ | DaemonSet | 每个节点跑一个Pod（监控、日志） |\n| ⭐⭐☆ | Job/CronJob | 一次性/定时任务 |\n\n**Pod的关键地位**：\n- K8s成功的核心原因之一\n- 让容器管理更方便、更解耦\n- **所有资源的最终形态都是Pod**\n\n##### 4.5.2 服务发现与负载均衡\n\n| 资源 | 角色定位 | 类比 |\n|-----|---------|------|\n| **Service** | 集群内部Pod间通信的基础设施 | 内部DNS + 负载均衡器 |\n| **Ingress** | 集群流量入口的**抽象规范** | Nginx的上层抽象 |\n\n**Ingress的本质**：\n- 不是具体的网关，而是**网关标准**\n- Nginx是Ingress的一种实现方案\n- 类比：JDBC是数据库访问抽象，MySQL Driver是具体实现\n\n##### 4.5.3 配置与存储\n\n| 类别 | 资源 | 用途 |\n|-----|------|------|\n| **存储** | PersistentVolume、PersistentVolumeClaim | 数据安全与持久化 |\n| **配置** | ConfigMap、Secret | 配置文件、敏感参数管理 |\n\n##### 4.5.4 访问控制\n\n- **RBAC**（Role-Based Access Control）：基于角色的权限控制\n- ServiceAccount、Role、RoleBinding等资源\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n| 坑点 | 解决方案 |\n|-----|---------|\n| ❌ 跳过概念直接敲命令 | ✅ 强制自己先理解Pod/Controller/Service/Ingress四大核心 |\n| ❌ 把Ingress当成Nginx替代品 | ✅ 理解Ingress是**抽象接口**，Nginx只是实现之一 |\n| ❌ 混淆资源和对象 | ✅ 记住\"资源=类，对象=实例\"的Java类比 |\n| ❌ 忽视无状态/有状态的区别 | ✅ 这是选择Deployment vs StatefulSet的决定性因素 |\n| ❌ 试图死记所有46种资源 | ✅ 聚焦7种核心控制器 + Service/Ingress，其余按需查文档 |\n\n**资源清单（YAML）的核心地位**：\n> \"真正学会K8s的标志——能用YAML配置文件描述所有要构建的东西。\"",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_d5412f8ea4af",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html",
      "sourceType": "web",
      "promptId": "prompt-mll-template-detailed",
      "modelId": "openai_compatible",
      "formats": [
        "toc"
      ],
      "retryable": false,
      "createdAt": "2026-02-14T16:09:25.789Z",
      "updatedAt": "2026-02-14T16:10:05.784Z",
      "resultMd": "# 暂退法（Dropout）\n\n## 目录\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [重新审视过拟合](#重新审视过拟合)\n  - [扰动的稳健性](#扰动的稳健性)\n  - [实践中的暂退法](#实践中的暂退法)\n- [代码与示例](#代码与示例)\n  - [从零开始实现](#从零开始实现)\n  - [简洁实现](#简洁实现)\n- [常见问题与避坑](#常见问题与避坑)\n- [复习清单](#复习清单)\n\n---\n\n## 核心概念摘要\n\n| 要点 | 说明 |\n|:---|:---|\n| **定义** | Dropout是一种正则化技术，在训练过程中以概率 $p$ 随机将神经元输出置零 |\n| **目的** | 防止神经网络过拟合，增强模型泛化能力 |\n| **核心机制** | 破坏神经元之间的\"共适应性\"，避免网络过度依赖特定特征组合 |\n| **关键特性** | 训练时随机丢弃 + 缩放；测试时关闭，使用完整网络 |\n| **数学本质** | 无偏估计：$E[h'] = h$，通过除以 $(1-p)$ 保持期望值不变 |\n\n---\n\n## 知识点详解\n\n### 重新审视过拟合\n\n**偏差-方差权衡（Bias-Variance Tradeoff）**\n\n| 模型类型 | 偏差 | 方差 | 特点 |\n|:---|:---|:---|:---|\n| 线性模型 | 高 | 低 | 只能表示简单函数，但结果稳定 |\n| 深度神经网络 | 低 | 高 | 能学习复杂特征交互，但容易过拟合 |\n\n**神经网络的过拟合风险**\n- 即使样本数远多于特征数，深度网络仍可能过拟合\n- 2017年实验：在**随机标记**的图像上训练，网络仍能完美拟合训练集（泛化差距高达90%）\n- 这说明神经网络的表达能力极强，需要显式正则化约束\n\n---\n\n### 扰动的稳健性\n\n**\"好\"模型的标准**：对输入的微小扰动不敏感（平滑性）\n\n**理论联系**\n- **Bishop (1995)**：输入噪声训练 $\\Leftrightarrow$ Tikhonov正则化\n- **Srivastava et al. (2014)**：将噪声注入扩展到**内部层** → Dropout\n\n**无偏噪声注入**\n\n对于中间激活值 $h$，以暂退概率 $p$ 替换为随机变量 $h'$：\n\n$$\nh' = \\begin{cases} \n0 & \\text{概率为 } p \\\\\n\\frac{h}{1-p} & \\text{其他情况}\n\\end{cases}\n$$\n\n**期望值保持不变**：\n$$E[h'] = p \\cdot 0 + (1-p) \\cdot \\frac{h}{1-p} = h$$\n\n> 除以 $(1-p)$ 的缩放操作称为**反向暂退法（inverted dropout）**，确保训练时激活值的期望与测试时一致。\n\n---\n\n### 实践中的暂退法\n\n**网络结构变化示意**\n\n```\n原始网络:        Dropout后:\n    h₁              h₁ (保留)\n    ↓               ↓\n    h₂      →       0  (丢弃)\n    ↓               \n    h₃              h₃ (保留)\n    ↓               ↓\n    h₄              h₄ (保留)\n    ↓               ↓\n    h₅      →       0  (丢弃)\n```\n\n**关键设计原则**\n| 层面 | 策略 |\n|:---|:---|\n| 应用位置 | 通常在隐藏层激活函数之后 |\n| 概率设置 | 靠近输入层用较低概率（如0.2），深层可用较高概率（如0.5）|\n| 训练 vs 测试 | **仅训练时使用**，测试时关闭 |\n| 测试时例外 | 可用于估计预测不确定性（多次采样看一致性）|\n\n---\n\n## 代码与示例\n\n### 从零开始实现\n\n#### 1. 基础Dropout层实现（PyTorch版）\n\n```python\nimport torch\nfrom torch import nn\n\ndef dropout_layer(X, dropout):\n    \"\"\"\n    实现Dropout层\n    \n    参数:\n        X: 输入张量\n        dropout: 暂退概率 (0 <= dropout <= 1)\n    \"\"\"\n    assert 0 <= dropout <= 1, \"dropout概率必须在[0,1]范围内\"\n    \n    # 边界情况处理\n    if dropout == 1:\n        return torch.zeros_like(X)      # 全部丢弃\n    if dropout == 0:\n        return X                         # 全部保留\n    \n    # 生成掩码: 从均匀分布采样，大于dropout的位置保留\n    mask = (torch.rand(X.shape) > dropout).float()\n    \n    # 关键：除以(1-dropout)进行缩放，保持期望值不变\n    return mask * X / (1.0 - dropout)\n```\n\n**验证期望值不变性**：\n```python\nX = torch.arange(16, dtype=torch.float32).reshape((2, 8))\n\nprint(\"原始输入:\\n\", X)\nprint(\"dropout=0 (全保留):\\n\", dropout_layer(X, 0.))\nprint(\"dropout=0.5 (随机丢弃50%并缩放):\\n\", dropout_layer(X, 0.5))\nprint(\"dropout=1 (全丢弃):\\n\", dropout_layer(X, 1.))\n```\n\n#### 2. 完整MLP模型（带Dropout）\n\n```python\nclass Net(nn.Module):\n    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n                 is_training=True):\n        super(Net, self).__init__()\n        self.num_inputs = num_inputs\n        self.training = is_training\n        \n        # 三层网络结构\n        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        # 第一层\n        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n        if self.training:                    # ← 仅在训练时应用dropout\n            H1 = dropout_layer(H1, dropout1) # dropout1 = 0.2\n        \n        # 第二层  \n        H2 = self.relu(self.lin2(H1))\n        if self.training:                    # ← 仅在训练时应用dropout\n            H2 = dropout_layer(H2, dropout2) # dropout2 = 0.5\n        \n        # 输出层（通常不加dropout）\n        out = self.lin3(H2)\n        return out\n\n# 超参数设置\nnum_inputs, num_outputs = 784, 10      # Fashion-MNIST: 28×28=784, 10类\nnum_hiddens1, num_hiddens2 = 256, 256\ndropout1, dropout2 = 0.2, 0.5          # 输入侧概率低，深层概率高\n\nnet = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\n```\n\n#### 3. 训练流程\n\n```python\nnum_epochs, lr, batch_size = 10, 0.5, 256\nloss = nn.CrossEntropyLoss(reduction='none')\n\n# 加载数据\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n\n# 优化器\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\n\n# 训练（框架会自动切换net.training标志）\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n---\n\n### 简洁实现\n\n使用框架高级API，无需手动实现`dropout_layer`：\n\n```python\nnet = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Dropout(dropout1),      # ← 直接添加Dropout层\n    nn.Linear(256, 256),\n    nn.ReLU(),\n    nn.Dropout(dropout2),      # ← 直接添加Dropout层\n    nn.Linear(256, 10)\n)\n\n# 权重初始化\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\nnet.apply(init_weights)\n\n# 训练（框架自动处理train/eval模式切换）\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n**框架行为**：\n- `model.train()`：启用Dropout（训练模式）\n- `model.eval()`：禁用Dropout（评估/测试模式）\n\n---\n\n## 常见问题与避坑\n\n| 问题 | 原因 | 解决方案 |\n|:---|:---|:---|\n| 训练效果好但测试效果差 | Dropout概率设置过高 | 降低dropout概率，或逐层调整 |\n| 测试时忘记切换eval模式 | 测试时仍应用Dropout | 确保调用`model.eval()` |\n| 输出层加了Dropout | 破坏最终预测稳定性 | 通常只在隐藏层使用Dropout |\n| 所有层用相同dropout概率 | 浅层信息损失过多 | 输入层附近用低概率（0.1-0.3），深层可用高概率（0.5）|\n| 与BatchNorm一起用时效果差 | 两者都改变统计特性 | 注意顺序：通常Conv→BN→ReLU→Dropout |\n\n**重要区分**\n\n| 技术 | 作用对象 | 作用时机 |\n|:---|:---|:---|\n| Dropout | 激活值（神经元输出）| 前向传播时随机置零 |\n| 权重衰减(L2) | 权重参数 | 反向传播时梯度惩罚 |\n\n---\n\n## 复习清单\n\n- [ ] 理解Dropout的核心动机：破坏共适应性，防止特征过度耦合\n- [ ] 掌握数学公式：$h' = \\frac{h}{1-p}$（保留时）或 $0$（丢弃时），且 $E[h']=h$\n- [ ] 明确训练与测试的行为差异：训练时随机丢弃+缩放，测试时完整网络\n- [ ] 能独立实现`dropout_layer`函数，理解掩码生成和缩放操作\n- [ ] 知道概率设置的一般原则：浅层低、深层高\n- [ ] 理解Dropout与权重衰减的区别与互补性\n- [ ] 熟悉框架中`nn.Dropout`的使用及`train()`/`eval()`模式切换",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_fa510845480d",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc",
        "screenshot"
      ],
      "retryable": false,
      "createdAt": "2026-02-14T14:49:46.483Z",
      "updatedAt": "2026-02-14T15:00:06.997Z",
      "resultMd": "## 目录\n\n- [核心概念摘要](#核心概念摘要)\n- [知识点详解](#知识点详解)\n  - [什么是多层继承](#什么是多层继承)\n  - [多层继承 vs 循环继承](#多层继承-vs-循环继承)\n  - [继承链与功能传递](#继承链与功能传递)\n  - [super() 的本质](#super-的本质)\n- [代码实战](#代码实战)\n- [避坑指南 & 常见QA](#避坑指南--常见qa)\n\n---\n\n## 核心概念摘要\n\n1. **多层继承**（Multi-level Inheritance）：类A继承类B，类B继承类C，形成 `C → B → A` 的纵向继承链，区别于多继承的横向扩展\n2. **功能累积效应**：子类自动拥有所有祖先类的属性和方法，\"子孙类\"最为\"幸福\"\n3. **MRO（方法解析顺序）**：Python通过C3线性化算法确定属性查找路径，可通过 `__mro__` 或 `mro()` 查看\n4. **`super()` 的本质**：不是简单的\"父类\"，而是**下一个类**的引用，遵循MRO链条逐层向上查找\n5. **两种调用父类方法的方式**：`ParentClass.method(self)` 显式指定 vs `super().method()` 动态解析\n\n---\n\n## 知识点详解\n\n### 什么是多层继承\n\n多层继承指继承关系呈**纵向链条状**延伸，而非横向扩展：\n\n```\nMaster (师傅) ──┐\n              ├──→ Tutu (徒弟) ──→ Tusun (徒孙)\nSchool (学校) ──┘\n```\n\n![](/static/screenshots/task_fa510845480d/screenshot_001_63.jpg)*\n\n讲师用\"三代同堂\"作比喻：大多数家庭是两代或三代同堂，四代、五代甚至六代同堂虽少见但存在。类比到代码中，就是继承链可以持续延伸。\n\n> **关键区分**：\n> - **多层继承**（本课）：`A → B → C` 单线纵深\n> - **多继承**（上节课）：`A ← B, A ← C` 多线汇聚\n\n### 多层继承 vs 循环继承\n\n| 类型 | 结构 | Python是否允许 |\n|:---|:---|:---|\n| 多层继承 | `A → B → C`（单向无环） | ✅ 允许 |\n| 循环继承 | `A → B → C → A`（形成环路） | ❌ 禁止 |\n\n![](/static/screenshots/task_fa510845480d/screenshot_002_55.jpg)*\n\n循环继承会导致逻辑悖论（\"我继承我自己\"），Python会在类定义阶段抛出异常。\n\n### 继承链与功能传递\n\n以讲师构建的体系为例：\n\n```python\n# 继承链：object → Master/School → Tutu → Tusun\nclass Master:\n    def make_master(self):\n        print(\"精通师傅技能\")\n\nclass School:\n    def make_school(self):\n        print(\"掌握学校技能\")\n\nclass Tutu(Master, School):  # 徒弟继承两个父类（多继承）\n    pass\n\nclass Tusun(Tutu):  # 徒孙继承徒弟（多层继承）\n    pass\n```\n\n![](/static/screenshots/task_fa510845480d/screenshot_003_140.jpg)*\n\n**Tusun（徒孙）的最终能力**：\n- 直接获得 `Tutu` 的所有方法\n- 间接获得 `Master.make_master()` 和 `School.make_school()`\n- 通过IDE的\"Ctrl+点击\"可追溯至最原始的父类定义\n\n### super() 的本质\n\n讲师澄清了一个常见误解：\n\n| 符号 | 含义 | 指向 |\n|:---|:---|:---|\n| `self` | 当前**实例**的引用 | 本类创建的对象 |\n| `cls` | 当前**类**的引用 | 本类本身 |\n| `super()` | **下一个类**的引用 | MRO中的下一个类，不一定是直接父类 |\n\n![](/static/screenshots/task_fa510845480d/screenshot_004_411.jpg)*\n\n> `super()` 代表**父类的引用**是一种简化理解，更准确的说法是：**代表MRO链条中当前类的下一个类**。\n\n在多层继承中，`super()` 会逐层向上传递，实现\"层层递进\"的效果。\n\n---\n\n## 代码实战\n\n### 完整代码示例\n\n```python\n# ========== 基类层 ==========\nclass Master:\n    \"\"\"师傅类：传授传统技艺\"\"\"\n    \n    def __init__(self):\n        self.kungfu = \"古法煎饼果子\"\n    \n    def make_master(self):\n        print(f\"运用【{self.kungfu}】制作\")\n        return \"master_cake\"\n\n\nclass School:\n    \"\"\"学校类：传授现代技术\"\"\"\n    \n    def __init__(self):\n        self.tech = \"智能温控系统\"\n    \n    def make_school(self):\n        print(f\"使用【{self.tech}】辅助\")\n        return \"school_cake\"\n\n\n# ========== 中间层（多继承 + 多层继承的结合点）==========\nclass Tutu(Master, School):\n    \"\"\"\n    徒弟类：继承师傅和学校\n    是多层继承的中间节点，也是多继承的体现\n    \"\"\"\n    \n    def __init__(self):\n        # 方式1：显式调用指定父类（需要手动处理多个父类）\n        Master.__init__(self)\n        School.__init__(self)\n        self.skill = \"独门秘制酱料\"\n    \n    def make_tutu(self):\n        # 整合所有继承的能力\n        master_result = self.make_master()\n        school_result = self.make_school()\n        print(f\"加入【{self.skill}】完成制作\")\n        return f\"tutu_special_{master_result}_{school_result}\"\n\n\n# ========== 最底层（纯多层继承）==========\nclass Tusun(Tutu):\n    \"\"\"\n    徒孙类：仅继承Tutu\n    体现多层继承——自动获得所有祖先能力\n    \"\"\"\n    \n    def __init__(self):\n        # 方式2：使用super()，自动遵循MRO\n        super().__init__()  # 调用Tutu.__init__\n        self.innovation = \"网红包装营销\"\n    \n    def make_tusun(self):\n        # 调用从Tutu继承的方法\n        base = super().make_tutu()  # 等价于 Tutu.make_tutu(self)\n        print(f\"加上【{self.innovation}】推向市场\")\n        return f\"tusun_brand_{base}\"\n    \n    def show_lineage(self):\n        \"\"\"展示继承链\"\"\"\n        print(f\"\\n{'='*40}\")\n        print(f\"我的身份：{self.__class__.__name__}\")\n        print(f\"我的技能：{self.innovation}\")\n        print(f\"继承的技能：{self.skill}\")  # 来自Tutu\n        print(f\"继承的技术：{self.tech}\")   # 来自School\n        print(f\"继承的功夫：{self.kungfu}\") # 来自Master\n        print(f\"MRO链条：{[c.__name__ for c in self.__class__.__mro__]}\")\n        print(f\"{'='*40}\\n\")\n\n\n# ========== 测试验证 ==========\nif __name__ == \"__main__\":\n    # 创建徒孙实例\n    ts = Tusun()\n    \n    # 验证多层继承的功能累积\n    ts.show_lineage()\n    \n    # 调用各层级方法\n    print(\">>> 调用徒孙专属方法：\")\n    result = ts.make_tusun()\n    print(f\"最终产出：{result}\\n\")\n    \n    # 直接调用祖先方法\n    print(\">>> 直接调用继承自不同祖先的方法：\")\n    ts.make_master()   # 来自Master（爷爷辈）\n    ts.make_school()   # 来自School（爷爷辈）\n    ts.make_tutu()     # 来自Tutu（爸爸辈）\n```\n\n### 运行结果\n\n```\n========================================\n我的身份：Tusun\n我的技能：网红包装营销\n继承的技能：独门秘制酱料\n继承的技术：智能温控系统\n继承的功夫：古法煎饼果子\nMRO链条：['Tusun', 'Tutu', 'Master', 'School', 'object']\n========================================\n\n>>> 调用徒孙专属方法：\n运用【古法煎饼果子】制作\n使用【智能温控系统】辅助\n加入【独门秘制酱料】完成制作\n加上【网红包装营销】推向市场\n最终产出：tusun_brand_tutu_special_master_cake_school_cake\n\n>>> 直接调用继承自不同祖先的方法：\n运用【古法煎饼果子】制作\n使用【智能温控系统】辅助\n运用【古法煎饼果子】制作\n使用【智能温控系统】辅助\n加入【独门秘制酱料】完成制作\n```\n\n### MRO解析图示\n\n```\nTusun.__mro__ = (\n    Tusun,      # 自己\n    Tutu,       # 直接父类\n    Master,     # 爷爷辈（Tutu的第一个父类）\n    School,     # 爷爷辈（Tutu的第二个父类）\n    object      # 终极基类\n)\n```\n\n![](/static/screenshots/task_fa510845480d/screenshot_005_255.jpg)*\n\n当调用 `super()` 时，Python按此顺序查找下一个类。\n\n---\n\n## 避坑指南 & 常见QA\n\n### Q1：子类重写父类方法后，会优先调用哪个？\n\n> **A**：**子类自己的方法**。这是\"方法重写（Override）\"的基本规则。\n> \n> 若想在重写后仍使用父类版本，需显式调用：`super().method()` 或 `ParentClass.method(self)`\n\n### Q2：子类调用父类方法有几种方式？优劣对比\n\n| 方式 | 语法 | 优点 | 缺点 |\n|:---|:---|:---|:---|\n| **显式类名** | `Parent.method(self)` | 可精确指定任意父类 | 硬编码，维护性差；多继承时需多次调用 |\n| **super()** | `super().method()` | 自动遵循MRO，代码健壮；支持协作多重继承 | 只能按MRO顺序调用，无法跳过 |\n\n![](/static/screenshots/task_fa510845480d/screenshot_006_342.jpg)*\n\n```python\n# 显式类名：适合需要精确控制时\nclass Tutu(Master, School):\n    def __init__(self):\n        Master.__init__(self)   # 必须手动逐个调用\n        School.__init__(self)   # 若父类多，代码冗余\n\n# super()：适合标准继承场景\nclass Tusun(Tutu):\n    def __init__(self):\n        super().__init__()      # 自动找到MRO中的下一个类（Tutu）\n```\n\n### Q3：多层继承的核心特征是什么？\n\n> **A**：**类与类之间层层传递**，形成纵向链条。关键口诀：\n> - 不是\"A继承B且A继承C\"（多继承）\n> - 而是\"C继承B，B继承A\"（多层继承）\n\n### ⚠️ 易错点提醒\n\n| 陷阱 | 说明 | 解决方案 |\n|:---|:---|:---|\n| **混淆继承方向** | `class B(A)` 表示B继承A，不是A继承B | 记住：括号内的是**父类** |\n| **super()≠直接父类** | 在多继承中，`super()`可能跳过直接父类 | 打印 `__mro__` 确认实际顺序 |\n| **钻石问题** | 多继承+多层继承混合时，共同祖先被多次初始化 | 统一使用 `super()` 配合 `**kwargs` |\n| **无限递归** | `__init__` 中错误使用 `self.__init__()` | 改用 `super().__init__()` |\n\n### 课堂问答精华\n\n> **学生问**：`super()` 到底代表什么？\n> \n> **讲师答**：简单理解为\"父类的引用\"，准确说是**MRO中下一个类的引用**。就像 `self` 是本类的引用，`super()` 是父类（更准确地说是\"上一个类\"）的引用。\n\n![](/static/screenshots/task_fa510845480d/screenshot_007_428.jpg)*",
      "debug": {
        "keyframeStats": [
          {
            "url": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
            "sceneCount": 10,
            "candidateCount": 20,
            "afterBlackFilter": 20,
            "afterBlurFilter": 20,
            "afterDedupe": 12,
            "finalCount": 12,
            "elapsedMs": 27720
          }
        ],
        "keyframeWarnings": [
          "[1/1] KEYFRAME_WARN_ADAPTIVE_PROFILE:short"
        ]
      }
    },
    {
      "id": "task_4dbd98a535e9",
      "status": "failed",
      "stage": "generate",
      "progress": 92,
      "message": "模型生成失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc",
        "screenshot"
      ],
      "retryable": true,
      "createdAt": "2026-02-14T14:28:13.562Z",
      "updatedAt": "2026-02-14T14:35:45.271Z",
      "preparedMd": "# 多链接转录内容汇总\n\n## 来源 1\n\n原始链接：https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28\n\n# 第二章-面向对象高级-04多层继承\n\n## 转录全文\n\n接下来,我再给大家演续个效果叫多层计创 那么多层计创之杂,就是N以后小明老了 想把自己会的内容全部穿给他的同意 那么这种情况,你看,应该怎么做啊,肯定是C 我们把刚才者和这个内容给他继续这个二把肯定是C 肯定是为来个,这个操作就是04 04在这里边,我们把它称为叫多层计创 好,什么叫多层计创呢?我们先写过来 上班这个地方,我们要用戒RayFactor改革名称 这个不是离达,要把它改成离达 三改成离达 来了,多层计创,我来解释一下 阿利,叫也是多层计创 什么叫多层计创呢?在这里边来跟解释 其实非常简单 非常简单 那不允许自己造成那个造成那个例子 我还给你们讲过,是吧? 那A计创B,B计创C,C计创A 循环计创,不能那么玩啊 所以说多层计创的解释是这样的 好,那么干嘛呢? 就好了,类A计创类B 类B计创类C 这就是什么呢?多层计创 比如说咱现实生活中的 就是我们接着说了一句话叫三代图堂 你看,像有的家庭可能是正常大多数的家庭 应该是两代或者三代图堂 也有一些是,比如说就一代 也有一些就是网上的就比较多的 可能就四代,应该叫四试图堂 然后三网上一点的 还要就是比较少见了 就无事,然后还有六试 那这种可能就比较少见一点 当然有啊,国内之有的 直播会上一些而已 来说回来,所以类A计创类B 类B计创类C 这就是多层计创的 好,那么接下来在这里边 我们也是可以的 目前提设 提设,然后中的一个计创体系 是这样的 来,什么体系呢? 你看着我这样讲,能不能听懂啊 偶尔贵克特 往下野生出来一个什么? 两个此类 哪个此类呢? 第1个是Master这个此类 第2个是什么呢? Square SH-O-L Square这个此类 然后往下野生出谁呢? 野生出一个主人体试 是不是咱这个图地类啊? 那为你个问题啊? 图地类能不能往下 现在也是一个叫图孙 啊,图孙类,可不可以啊? 可以的 那图孙类是由前边所有的这些副类都成员啊 每次一样的 所以接下来来了 Master 这底了 然后Square在这儿 这底了 那图地类在这里边 你看图地类是不是有个maker O的 是不是有这么一个功能啊? 那你其实也可以把它这底了 我不要这个 我要哪个呢? 我要上班的操作 那这样的话 每个Master每个时候 那这样的话 你的图地是不是能把自己的功能 开了两个副类的功能 是不是全创给图孙啊? 好,所以来了 接下来 把这弹板出去了 然后接下来测试 测试不着急 不是测试了 而是第五个 那第四个就是 干嘛呀? 创建这个类 图孙 图孙类 那图孙 class 就叫图孙啦 便宜啦 然后小空号里边寫他一个 图地类 是不是说明图孙寄成似 图地类啊? 里边寄个pass 就好了 那么接下来我们的测试 接对就不要了 全删掉 那么5.1 创建什么? 好说 创建 图孙类 那就是 TS TS 等约 图孙就可以了 这个顶行啊 这个顶行 来,接下来 第二个就是 5.2 调用 调用功能 你告诉我 这个地方如果 TS就是 5.2 这个顶行 就是哪个类的 便宜 TSC 图地类的 哎,能跟上来吗? 所以又见知己了 又被发现 我掉的是独装 那其实来 如果我来个 TSC Maker Maker Master TSC Maker School 那就会发现 这两个都是哪个类的呢? 理论上来讲 都是 图地类的 但是你的图地类 底层 在这里面 你看 这儿不松手 鼠标 鼠标 左键点击 又会发现 是否会调整到 喊出的类伙啊? 它是Master 所以本身上来讲 又会发现 哦,原来 这个是 普通的PS类 但普通的PS类 它又去掉 Master 就是Master类 那么这个类容是谁啊? 普通的 是普通的 好了 所以你会发现 整个这个图孙 是不是最幸福了 然后呢 看到复习 还有看到 再上一集 说说所有功能 都可以即使过来啊 好,OK 那么这就是最遥的 业下这个蛋码 叫什么技术啊? 比较多 怎么办啊? 比较多 好了 那说过来 我们落地啊 接下来在这里面 我就看来落地了 这里边有主机 首先 赵从来提问 哪一个呢? 哪一个呢? 顺序吧 来 第1个是谁? 好,柴冬 好,来 好,此类 重写复类的 读名方法 或复习后 会优先调用 此类方法吗? 对 就是此类 把复类的复习了的方法复习后 会不会优先调? 此类的 对 会 好,OK啊 会的 因为咱刚讲的方法复习 是不是那大? 好,那接下来第2个 此类调用复类方法 有几种方式 优削点是什么? 那两种 复类名字 第二,复类方法名字 小复法 是一个 下复 好,那这地方有个问题 行,没事,先调 那这种方式 它能怎么办? 就是哪个复类都可以,是吧? 好,那第二种方式呢? 随便行,复法 然后接着说 后边怎么卸? 嗯 里面要不要传这个下复? 好,OK 那这种方式呢? 诸层查证 诸作用 对吧? 好,OK啊 那接下来 这个地方最后一个 多层技术是什么? 多层技术是什么? 类技术,类弊 类弊技术 类弊是什么? 诸层传递啊 好,OK 好,成功啊,成功 好,那么 这时我们去讲的最终 有一个问题 我们又给大家解释 有人会问 老师,修法是啥? 这个大伺服是不是没解释过啊? 那问你个问题啊 那个Zace说 那个Self是什么? 大家还有印象吗? 嗯 在这个位置 Self是什么? Self是本类的营用 跟上来了吗? 那修法是什么的? 修法就是 副类的营用 就是副类的营用 比如说当前的这个子类 他的副类的营用 就叫Self 注意一下就可以了 不要简单 你可以立即为 修法代表副类 好,那关于我们的这个技术了 我们就先不要到这个地方 来,我们保持一下\n\n## 分段时间轴\n\n- [00:00-00:04] 接下来,我再给大家演续个效果叫多层计创\n- [00:04-00:08] 那么多层计创之杂,就是N以后小明老了\n- [00:08-00:11] 想把自己会的内容全部穿给他的同意\n- [00:11-00:15] 那么这种情况,你看,应该怎么做啊,肯定是C\n- [00:15-00:19] 我们把刚才者和这个内容给他继续这个二把肯定是C\n- [00:19-00:23] 肯定是为来个,这个操作就是04\n- [00:23-00:27] 04在这里边,我们把它称为叫多层计创\n- [00:28-00:30] 好,什么叫多层计创呢?我们先写过来\n- [00:30-00:34] 上班这个地方,我们要用戒RayFactor改革名称\n- [00:34-00:36] 这个不是离达,要把它改成离达\n- [00:36-00:38] 三改成离达\n- [00:38-00:40] 来了,多层计创,我来解释一下\n- [00:40-00:43] 阿利,叫也是多层计创\n- [00:45-00:48] 什么叫多层计创呢?在这里边来跟解释\n- [00:48-00:50] 其实非常简单\n- [00:50-00:51] 非常简单\n- [00:51-00:53] 那不允许自己造成那个造成那个例子\n- [00:53-00:55] 我还给你们讲过,是吧?\n- [00:55-00:57] 那A计创B,B计创C,C计创A\n- [00:57-00:59] 循环计创,不能那么玩啊\n- [00:59-01:03] 所以说多层计创的解释是这样的\n- [01:03-01:04] 好,那么干嘛呢?\n- [01:04-01:07] 就好了,类A计创类B\n- [01:07-01:10] 类B计创类C\n- [01:10-01:13] 这就是什么呢?多层计创\n- [01:13-01:15] 比如说咱现实生活中的\n- [01:15-01:19] 就是我们接着说了一句话叫三代图堂\n- [01:19-01:23] 你看,像有的家庭可能是正常大多数的家庭\n- [01:23-01:25] 应该是两代或者三代图堂\n- [01:25-01:27] 也有一些是,比如说就一代\n- [01:27-01:29] 也有一些就是网上的就比较多的\n- [01:29-01:32] 可能就四代,应该叫四试图堂\n- [01:32-01:33] 然后三网上一点的\n- [01:33-01:35] 还要就是比较少见了\n- [01:35-01:37] 就无事,然后还有六试\n- [01:37-01:39] 那这种可能就比较少见一点\n- [01:39-01:41] 当然有啊,国内之有的\n- [01:41-01:43] 直播会上一些而已\n- [01:43-01:45] 来说回来,所以类A计创类B\n- [01:45-01:47] 类B计创类C\n- [01:47-01:49] 这就是多层计创的\n- [01:49-01:51] 好,那么接下来在这里边\n- [01:51-01:52] 我们也是可以的\n- [01:52-01:54] 目前提设\n- [01:56-01:59] 提设,然后中的一个计创体系\n- [01:59-02:00] 是这样的\n- [02:00-02:01] 来,什么体系呢?\n- [02:01-02:03] 你看着我这样讲,能不能听懂啊\n- [02:03-02:05] 偶尔贵克特\n- [02:05-02:07] 往下野生出来一个什么?\n- [02:07-02:09] 两个此类\n- [02:09-02:10] 哪个此类呢?\n- [02:10-02:11] 第1个是Master这个此类\n- [02:11-02:12] 第2个是什么呢?\n- [02:12-02:13] Square\n- [02:13-02:14] SH-O-L\n- [02:14-02:15] Square这个此类\n- [02:15-02:17] 然后往下野生出谁呢?\n- [02:17-02:19] 野生出一个主人体试\n- [02:19-02:21] 是不是咱这个图地类啊?\n- [02:21-02:22] 那为你个问题啊?\n- [02:22-02:23] 图地类能不能往下\n- [02:23-02:26] 现在也是一个叫图孙\n- [02:26-02:28] 啊,图孙类,可不可以啊?\n- [02:28-02:29] 可以的\n- [02:29-02:33] 那图孙类是由前边所有的这些副类都成员啊\n- [02:33-02:34] 每次一样的\n- [02:34-02:35] 所以接下来来了\n- [02:35-02:37] Master\n- [02:37-02:38] 这底了\n- [02:38-02:40] 然后Square在这儿\n- [02:40-02:41] 这底了\n- [02:41-02:42] 那图地类在这里边\n- [02:42-02:44] 你看图地类是不是有个maker\n- [02:44-02:45] O的\n- [02:45-02:46] 是不是有这么一个功能啊?\n- [02:46-02:48] 那你其实也可以把它这底了\n- [02:48-02:49] 我不要这个\n- [02:49-02:50] 我要哪个呢?\n- [02:50-02:52] 我要上班的操作\n- [02:52-02:53] 那这样的话\n- [02:53-02:55] 每个Master每个时候\n- [02:55-02:56] 那这样的话\n- [02:56-02:58] 你的图地是不是能把自己的功能\n- [02:58-02:59] 开了两个副类的功能\n- [02:59-03:01] 是不是全创给图孙啊?\n- [03:01-03:02] 好,所以来了\n- [03:02-03:03] 接下来\n- [03:03-03:04] 把这弹板出去了\n- [03:04-03:05] 然后接下来测试\n- [03:05-03:06] 测试不着急\n- [03:06-03:07] 不是测试了\n- [03:07-03:08] 而是第五个\n- [03:08-03:10] 那第四个就是\n- [03:10-03:11] 干嘛呀?\n- [03:11-03:12] 创建这个类\n- [03:12-03:13] 图孙\n- [03:13-03:14] 图孙类\n- [03:14-03:15] 那图孙\n- [03:15-03:16] class\n- [03:16-03:17] 就叫图孙啦\n- [03:17-03:18] 便宜啦\n- [03:18-03:19] 然后小空号里边寫他一个\n- [03:19-03:20] 图地类\n- [03:20-03:22] 是不是说明图孙寄成似\n- [03:22-03:23] 图地类啊?\n- [03:23-03:24] 里边寄个pass\n- [03:24-03:25] 就好了\n- [03:25-03:26] 那么接下来我们的测试\n- [03:26-03:27] 接对就不要了\n- [03:27-03:28] 全删掉\n- [03:28-03:31] 那么5.1\n- [03:31-03:32] 创建什么?\n- [03:32-03:33] 好说\n- [03:33-03:34] 创建\n- [03:34-03:35] 图孙类\n- [03:35-03:36] 那就是\n- [03:36-03:37] TS\n- [03:37-03:38] TS\n- [03:38-03:39] 等约\n- [03:39-03:40] 图孙就可以了\n- [03:40-03:41] 这个顶行啊\n- [03:41-03:42] 这个顶行\n- [03:42-03:43] 来,接下来\n- [03:43-03:44] 第二个就是\n- [03:44-03:45] 5.2\n- [03:45-03:46] 调用\n- [03:46-03:47] 调用功能\n- [03:47-03:48] 你告诉我\n- [03:48-03:49] 这个地方如果\n- [03:49-03:50] TS就是\n- [03:50-03:51] 5.2\n- [03:51-03:52] 这个顶行\n- [03:52-03:53] 就是哪个类的\n- [03:53-03:54] 便宜\n- [03:54-03:55] TSC\n- [03:55-03:56] 图地类的\n- [03:56-03:58] 哎,能跟上来吗?\n- [03:58-03:59] 所以又见知己了\n- [03:59-04:00] 又被发现\n- [04:00-04:01] 我掉的是独装\n- [04:01-04:02] 那其实来\n- [04:02-04:03] 如果我来个\n- [04:03-04:04] TSC\n- [04:04-04:05] Maker\n- [04:05-04:06] Maker Master\n- [04:06-04:07] TSC\n- [04:07-04:08] Maker\n- [04:08-04:09] School\n- [04:09-04:10] 那就会发现\n- [04:10-04:11] 这两个都是哪个类的呢?\n- [04:11-04:12] 理论上来讲\n- [04:12-04:13] 都是\n- [04:13-04:14] 图地类的\n- [04:14-04:15] 但是你的图地类\n- [04:15-04:16] 底层\n- [04:16-04:17] 在这里面\n- [04:17-04:18] 你看\n- [04:18-04:19] 这儿不松手\n- [04:19-04:20] 鼠标\n- [04:20-04:21] 鼠标\n- [04:21-04:22] 左键点击\n- [04:22-04:23] 又会发现\n- [04:23-04:24] 是否会调整到\n- [04:24-04:25] 喊出的类伙啊?\n- [04:25-04:26] 它是Master\n- [04:26-04:27] 所以本身上来讲\n- [04:27-04:28] 又会发现\n- [04:28-04:29] 哦,原来\n- [04:29-04:30] 这个是\n- [04:30-04:31] 普通的PS类\n- [04:31-04:32] 但普通的PS类\n- [04:32-04:33] 它又去掉\n- [04:33-04:34] Master\n- [04:34-04:35] 就是Master类\n- [04:35-04:36] 那么这个类容是谁啊?\n- [04:36-04:37] 普通的\n- [04:37-04:38] 是普通的\n- [04:38-04:41] 好了\n- [04:41-04:42] 所以你会发现\n- [04:42-04:44] 整个这个图孙\n- [04:44-04:45] 是不是最幸福了\n- [04:45-04:46] 然后呢\n- [04:46-04:47] 看到复习\n- [04:47-04:48] 还有看到\n- [04:48-04:49] 再上一集\n- [04:49-04:50] 说说所有功能\n- [04:50-04:51] 都可以即使过来啊\n- [04:51-04:52] 好,OK\n- [04:52-04:53] 那么这就是最遥的\n- [04:53-04:54] 业下这个蛋码\n- [04:54-04:55] 叫什么技术啊?\n- [04:55-04:56] 比较多\n- [04:56-04:57] 怎么办啊?\n- [04:57-04:58] 比较多\n- [04:58-04:59] 好了\n- [04:59-05:00] 那说过来\n- [05:00-05:01] 我们落地啊\n- [05:01-05:02] 接下来在这里面\n- [05:02-05:03] 我就看来落地了\n- [05:03-05:04] 这里边有主机\n- [05:04-05:05] 首先\n- [05:05-05:07] 赵从来提问\n- [05:07-05:09] 哪一个呢?\n- [05:09-05:10] 哪一个呢?\n- [05:10-05:11] 顺序吧\n- [05:11-05:12] 来\n- [05:12-05:13] 第1个是谁?\n- [05:13-05:16] 好,柴冬\n- [05:16-05:17] 好,来\n- [05:17-05:18] 好,此类\n- [05:18-05:20] 重写复类的\n- [05:20-05:21] 读名方法\n- [05:21-05:22] 或复习后\n- [05:22-05:24] 会优先调用\n- [05:24-05:25] 此类方法吗?\n- [05:25-05:31] 对\n- [05:31-05:32] 就是此类\n- [05:32-05:33] 把复类的复习了的方法复习后\n- [05:33-05:34] 会不会优先调?\n- [05:34-05:35] 此类的\n- [05:35-05:36] 对\n- [05:36-05:37] 会\n- [05:37-05:38] 好,OK啊\n- [05:38-05:39] 会的\n- [05:39-05:40] 因为咱刚讲的方法复习\n- [05:40-05:41] 是不是那大?\n- [05:41-05:42] 好,那接下来第2个\n- [05:42-05:44] 此类调用复类方法\n- [05:44-05:45] 有几种方式\n- [05:45-05:47] 优削点是什么?\n- [05:47-05:48] 那两种\n- [05:48-05:54] 复类名字\n- [05:54-05:55] 第二,复类方法名字\n- [05:55-05:56] 小复法\n- [05:56-05:57] 是一个\n- [05:57-05:58] 下复\n- [05:58-05:59] 好,那这地方有个问题\n- [05:59-06:01] 行,没事,先调\n- [06:01-06:02] 那这种方式\n- [06:02-06:03] 它能怎么办?\n- [06:03-06:06] 就是哪个复类都可以,是吧?\n- [06:06-06:10] 好,那第二种方式呢?\n- [06:10-06:11] 随便行,复法\n- [06:11-06:12] 然后接着说\n- [06:12-06:13] 后边怎么卸?\n- [06:13-06:17] 嗯\n- [06:17-06:19] 里面要不要传这个下复?\n- [06:19-06:21] 好,OK\n- [06:21-06:22] 那这种方式呢?\n- [06:22-06:26] 诸层查证\n- [06:26-06:27] 诸作用\n- [06:27-06:28] 对吧?\n- [06:28-06:29] 好,OK啊\n- [06:29-06:30] 那接下来\n- [06:30-06:31] 这个地方最后一个\n- [06:31-06:32] 多层技术是什么?\n- [06:32-06:39] 多层技术是什么?\n- [06:39-06:41] 类技术,类弊\n- [06:41-06:42] 类弊技术\n- [06:42-06:43] 类弊是什么?\n- [06:43-06:44] 诸层传递啊\n- [06:44-06:45] 好,OK\n- [06:45-06:46] 好,成功啊,成功\n- [06:46-06:47] 好,那么\n- [06:47-06:48] 这时我们去讲的最终\n- [06:48-06:49] 有一个问题\n- [06:49-06:50] 我们又给大家解释\n- [06:50-06:51] 有人会问\n- [06:51-06:52] 老师,修法是啥?\n- [06:52-06:54] 这个大伺服是不是没解释过啊?\n- [06:54-06:55] 那问你个问题啊\n- [06:55-06:57] 那个Zace说\n- [06:57-06:58] 那个Self是什么?\n- [06:58-06:59] 大家还有印象吗?\n- [06:59-07:00] 嗯\n- [07:00-07:01] 在这个位置\n- [07:01-07:02] Self是什么?\n- [07:02-07:05] Self是本类的营用\n- [07:05-07:06] 跟上来了吗?\n- [07:06-07:07] 那修法是什么的?\n- [07:07-07:08] 修法就是\n- [07:08-07:11] 副类的营用\n- [07:11-07:12] 就是副类的营用\n- [07:12-07:14] 比如说当前的这个子类\n- [07:14-07:16] 他的副类的营用\n- [07:16-07:17] 就叫Self\n- [07:17-07:18] 注意一下就可以了\n- [07:18-07:19] 不要简单\n- [07:19-07:20] 你可以立即为\n- [07:20-07:21] 修法代表副类\n- [07:21-07:23] 好,那关于我们的这个技术了\n- [07:23-07:25] 我们就先不要到这个地方\n- [07:25-07:26] 来,我们保持一下",
      "error": "连接超时（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）",
      "debug": {
        "keyframeStats": [
          {
            "url": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
            "sceneCount": 10,
            "candidateCount": 20,
            "afterBlackFilter": 20,
            "afterBlurFilter": 20,
            "afterDedupe": 12,
            "finalCount": 12,
            "elapsedMs": 36481
          }
        ],
        "keyframeWarnings": [
          "[1/1] KEYFRAME_WARN_ADAPTIVE_PROFILE:short"
        ]
      }
    },
    {
      "id": "task_4973c91eff08",
      "status": "failed",
      "stage": "local_transcribe",
      "progress": 15,
      "message": "第 1 条链接处理失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc",
        "screenshot"
      ],
      "retryable": false,
      "createdAt": "2026-02-14T14:00:31.637Z",
      "updatedAt": "2026-02-14T14:01:54.102Z",
      "error": "第 1 条链接处理失败：Command failed: D:\\ProgramData\\VScode\\SuperNote\\env\\python.exe D:\\ProgramData\\VScode\\SuperNote\\apps\\server\\scripts\\transcribe_faster_whisper.py D:\\ProgramData\\VScode\\SuperNote\\storage\\temp\\asr_tmp_45abf4dcb227\\audio.wav --model base --output_dir D:\\ProgramData\\VScode\\SuperNote\\storage\\temp\\asr_tmp_45abf4dcb227 --temperature 0 --beam_size 5 --language zh\nWarning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\r\nError during transcription: Library cublas64_12.dll is not found or cannot be loaded\r\n（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）"
    },
    {
      "id": "task_f9392db1380a",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html",
      "sourceType": "web",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [],
      "retryable": false,
      "createdAt": "2026-02-14T13:57:41.298Z",
      "updatedAt": "2026-02-14T13:58:33.443Z",
      "resultMd": "# 暂退法（Dropout）学习笔记\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **Dropout的本质**：一种通过在训练时随机\"关闭\"神经元来防止过拟合的正则化技术，可视为对网络内部层注入结构化噪声\n2. **无偏性保证**：通过将保留的神经元输出除以 $(1-p)$，确保训练时神经元的期望输出与测试时一致\n3. **破坏共适应性**：类比有性繁殖打破基因共适应，Dropout阻止神经元之间形成过度依赖的虚假关联\n4. **训练-测试差异**：仅在训练阶段启用，测试时使用完整网络以获得确定性预测\n5. **层级差异化配置**：通常输入侧Dropout概率较低（如0.2），深层可适当提高（如0.5）\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 一、过拟合问题的重新审视\n\n| 模型类型 | 偏差特性 | 方差特性 | 过拟合风险 |\n|---------|---------|---------|-----------|\n| 线性模型 | 高（假设空间小） | 低（结果稳定） | 样本不足时易过拟合特征 |\n| 深度神经网络 | 低（表达能力极强） | 高（结果波动大） | **即使样本充足也可能严重过拟合** |\n\n> **关键洞察**：2017年的经典实验表明，深度网络甚至能完美拟合**随机标签**的数据，泛化差距可达90%，证明其灵活性既是优势也是隐患。\n\n### 二、扰动稳健性的理论根基\n\n**Bishop (1995) 的核心结论**：对输入添加高斯噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 的训练，数学上等价于Tikhonov正则化。\n\n$$\nx' = x + \\epsilon, \\quad \\mathbb{E}[x'] = x\n$$\n\n**Srivastava et al. (2014) 的创新**：将噪声注入从**输入层**扩展到**所有隐藏层**，形成标准Dropout。\n\n### 三、Dropout的数学定义\n\n对于中间激活值 $h$，以暂退概率 $p$ 进行随机替换：\n\n$$\nh' = \\begin{cases} \n0 & \\text{概率为 } p \\\\[6pt]\n\\dfrac{h}{1-p} & \\text{其他情况}\n\\end{cases}\n$$\n\n**期望值不变性验证**：\n$$\n\\mathbb{E}[h'] = p \\cdot 0 + (1-p) \\cdot \\frac{h}{1-p} = h\n$$\n\n> **缩放因子的必要性**：若不除以 $(1-p)$，训练时神经元期望输出为 $(1-p)h$，而测试时为 $h$，造成分布偏移。\n\n### 四、网络结构的动态变化\n\n```\n原始网络:  输入 → [h₁, h₂, h₃, h₄, h₅] → 输出\n                    ↓    ↓\nDropout后: 输入 → [h₁,  0 , h₃, h₄,  0 ] → 输出\n                  (h₂,h₅被置零，其余放大至 1/(1-p) 倍)\n```\n\n**效果解读**：每次前向传播都采样一个\"子网络\"，最终模型相当于指数级数量子网络的集成（ensemble）。\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### 从零实现Dropout层\n\n```python\nimport torch\nfrom torch import nn\n\ndef dropout_layer(X: torch.Tensor, dropout: float) -> torch.Tensor:\n    \"\"\"\n    自定义Dropout层实现\n    \n    Args:\n        X: 输入张量\n        dropout: 暂退概率，范围[0, 1]\n    \n    Returns:\n        经过dropout处理的张量\n    \"\"\"\n    assert 0 <= dropout <= 1, \"Dropout概率必须在[0,1]范围内\"\n    \n    # 边界情况处理\n    if dropout == 1:\n        return torch.zeros_like(X)      # 全部丢弃\n    if dropout == 0:\n        return X                         # 全部保留\n    \n    # 生成掩码：均匀采样 > dropout 的位置保留\n    mask = (torch.rand(X.shape) > dropout).float()\n    \n    # 关键：缩放保留元素，保证期望不变\n    return mask * X / (1.0 - dropout)\n```\n\n**验证输出示例**：\n```python\nX = torch.arange(16, dtype=torch.float32).reshape((2, 8))\nprint(f\"原始输入:\\n{X}\")\n\nprint(f\"\\ndropout=0.0:\\n{dropout_layer(X, 0.0)}\")   # 完全保留\nprint(f\"\\ndropout=0.5:\\n{dropout_layer(X, 0.5)}\")   # 随机丢弃约50%，保留值×2\nprint(f\"\\ndropout=1.0:\\n{dropout_layer(X, 1.0)}\")   # 全部置零\n```\n\n### 完整MLP模型（带Dropout）\n\n```python\nclass DropoutMLP(nn.Module):\n    def __init__(\n        self,\n        num_inputs: int = 784,\n        num_outputs: int = 10,\n        num_hiddens1: int = 256,\n        num_hiddens2: int = 256,\n        dropout1: float = 0.2,    # 第一层dropout率（较低）\n        dropout2: float = 0.5,    # 第二层dropout率（较高）\n    ):\n        super().__init__()\n        \n        self.num_inputs = num_inputs\n        \n        # 网络结构\n        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n        \n        self.relu = nn.ReLU()\n        self.dropout1 = dropout1\n        self.dropout2 = dropout2\n        \n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        # 展平输入\n        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n        \n        # 训练时才应用dropout（通过self.training判断）\n        if self.training:\n            H1 = dropout_layer(H1, self.dropout1)\n        \n        H2 = self.relu(self.lin2(H1))\n        if self.training:\n            H2 = dropout_layer(H2, self.dropout2)\n        \n        return self.lin3(H2)  # 输出层不加dropout\n```\n\n### 简洁实现（PyTorch高级API）\n\n```python\nnet = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Dropout(0.2),           # 内置Dropout层，自动处理train/eval模式\n    nn.Linear(256, 256),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 10)\n)\n\n# 权重初始化\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n\n# 训练流程（注意模式切换）\nnet.train()   # 启用dropout\n# ... 训练代码 ...\n\nnet.eval()    # 禁用dropout，用于评估/推理\n# ... 测试代码 ...\n```\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n### ⚠️ 关键注意事项\n\n| 问题 | 解决方案 |\n|-----|---------|\n| **忘记切换eval模式** | 测试前务必调用 `net.eval()`，否则预测结果会随机波动 |\n| **dropout位置错误** | 应放在**激活函数之后**，而非线性变换之前 |\n| **概率设置不当** | 输入层附近宜低（0.1-0.3），深层可高（0.3-0.5）；超过0.5可能欠拟合 |\n| **与BN的冲突** | Dropout和BatchNorm同时使用时需谨慎，现代架构往往只选其一 |\n\n### 🔍 代表性问答\n\n**Q1: 为什么测试时不使用Dropout？**\n\n> 测试时需要**确定性的预测**。若启用Dropout，同一输入多次前向会得到不同结果，无法提供稳定的模型评估。此外，测试时使用完整网络是对所有可能子网络的近似集成。\n\n**Q2: Dropout与L2正则化的关系？**\n\n> 两者正交互补：\n> - **L2正则化**：限制权重大小，使模型\"简单\"\n> - **Dropout**：增加鲁棒性，使模型\"平滑\"\n> \n> 实践中常同时使用，但需注意叠加效应可能导致欠拟合。\n\n**Q3: 为什么靠近输入层的dropout概率通常更低？**\n\n> 浅层特征更为基础通用，过度丢弃会破坏有效信息提取；深层特征更抽象，适度丢弃可促进特征组合的多样性。\n\n**Q4: Dropout能否应用于卷积层？**\n\n> 可以但较少见。卷积层本身参数共享已具一定正则效果，且空间相关性使得随机丢弃像素不如在全连接层有效。现代CNN更多使用**Spatial Dropout**（整通道丢弃）或**DropBlock**（连续区域丢弃）。\n\n### 📊 调试技巧\n\n```python\n# 监控各层激活值的稀疏度\ndef monitor_dropout_stats(model, sample_input):\n    model.train()  # 确保dropout开启\n    \n    activations = {}\n    hooks = []\n    \n    def hook_fn(name):\n        def fn(module, input, output):\n            activations[name] = {\n                'mean': output.mean().item(),\n                'std': output.std().item(),\n                'sparsity': (output == 0).float().mean().item()\n            }\n        return fn\n    \n    # 注册钩子\n    for name, module in model.named_modules():\n        if isinstance(module, (nn.ReLU, nn.Dropout)):\n            hooks.append(module.register_forward_hook(hook_fn(name)))\n    \n    with torch.no_grad():\n        _ = model(sample_input)\n    \n    # 清理钩子\n    for h in hooks:\n        h.remove()\n    \n    return activations\n```\n\n---\n\n*本笔记基于《动手学深度学习》第4.6节整理，建议配合原书代码实践以加深理解。*",
      "debug": {
        "keyframeStats": [],
        "keyframeWarnings": []
      }
    },
    {
      "id": "task_f29d94747c21",
      "status": "failed",
      "stage": "local_transcribe",
      "progress": 15,
      "message": "第 1 条链接处理失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc",
        "screenshot"
      ],
      "retryable": false,
      "createdAt": "2026-02-14T13:55:40.911Z",
      "updatedAt": "2026-02-14T13:56:33.161Z",
      "error": "第 1 条链接处理失败：Command failed: D:\\ProgramData\\VScode\\SuperNote\\env\\python.exe D:\\ProgramData\\VScode\\SuperNote\\apps\\server\\scripts\\transcribe_faster_whisper.py D:\\ProgramData\\VScode\\SuperNote\\storage\\temp\\asr_tmp_11dbaf4e37d7\\audio.wav --model base --output_dir D:\\ProgramData\\VScode\\SuperNote\\storage\\temp\\asr_tmp_11dbaf4e37d7 --temperature 0 --beam_size 5 --language zh\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\VScode\\SuperNote\\apps\\server\\scripts\\transcribe_faster_whisper.py\", line 103, in <module>\r\n    main()\r\n  File \"D:\\ProgramData\\VScode\\SuperNote\\apps\\server\\scripts\\transcribe_faster_whisper.py\", line 28, in main\r\n    import torch\r\nModuleNotFoundError: No module named 'torch'\r\n（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）"
    },
    {
      "id": "task_293dd4426371",
      "status": "failed",
      "stage": "local_transcribe",
      "progress": 15,
      "message": "第 1 条链接处理失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc",
        "screenshot"
      ],
      "retryable": false,
      "createdAt": "2026-02-14T07:49:42.732Z",
      "updatedAt": "2026-02-14T08:20:11.888Z",
      "error": "第 1 条链接处理失败：local transcribe timeout（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）"
    },
    {
      "id": "task_4ebf0ec224bd",
      "status": "failed",
      "stage": "local_transcribe",
      "progress": 15,
      "message": "第 1 条链接处理失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc",
        "screenshot"
      ],
      "retryable": false,
      "createdAt": "2026-02-14T07:46:31.408Z",
      "updatedAt": "2026-02-14T07:47:02.251Z",
      "error": "第 1 条链接处理失败：resolveCommand is not defined（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）"
    },
    {
      "id": "task_0838e4ba82fb",
      "status": "failed",
      "stage": "local_transcribe",
      "progress": 15,
      "message": "第 1 条链接处理失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=29",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc",
        "screenshot"
      ],
      "retryable": false,
      "createdAt": "2026-02-14T03:18:53.649Z",
      "updatedAt": "2026-02-14T03:50:26.977Z",
      "error": "第 1 条链接处理失败：local transcribe timeout（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）"
    },
    {
      "id": "task_05d0b1b8ed0a",
      "status": "cancelled",
      "stage": "transcribe",
      "progress": 15,
      "message": "任务已取消",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=29",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "formats": [
        "toc",
        "screenshot"
      ],
      "retryable": false,
      "createdAt": "2026-02-13T17:03:20.663Z",
      "updatedAt": "2026-02-13T17:03:25.062Z",
      "cancelReason": "user_cancelled"
    },
    {
      "id": "task_8dcaf1f9191f",
      "status": "failed",
      "stage": "local_transcribe",
      "progress": 15,
      "message": "第 1 条链接处理失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1ABziBrE9a/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=2a985ca1c3a6930df4b372bbf6082ce6",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "retryable": false,
      "createdAt": "2026-02-12T16:37:43.595Z",
      "updatedAt": "2026-02-12T17:10:30.552Z",
      "error": "第 1 条链接处理失败：local transcribe timeout（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）"
    },
    {
      "id": "task_5dd983886546",
      "status": "failed",
      "stage": "local_transcribe",
      "progress": 15,
      "message": "第 1 条链接处理失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1ABziBrE9a/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=2a985ca1c3a6930df4b372bbf6082ce6",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "retryable": false,
      "createdAt": "2026-02-12T15:52:11.958Z",
      "updatedAt": "2026-02-12T15:55:06.007Z",
      "error": "第 1 条链接处理失败：Command failed: whisper D:\\ProgramData\\VScode\\SuperNote\\storage\\temp\\asr_tmp_54ad69473186\\audio.wav --model small --output_format json --output_dir D:\\ProgramData\\VScode\\SuperNote\\storage\\temp\\asr_tmp_54ad69473186 --task transcribe --fp16 False --temperature 0 --beam_size 5 --verbose False --language zh --device cuda\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"D:\\ProgramSoftware\\Conda\\Scripts\\whisper.exe\\__main__.py\", line 6, in <module>\r\n    sys.exit(cli())\r\n             ~~~^^\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\whisper\\transcribe.py\", line 595, in cli\r\n    model = load_model(model_name, device=device, download_root=model_dir)\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\whisper\\__init__.py\", line 151, in load_model\r\n    checkpoint = torch.load(fp, map_location=device, **kwargs)\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\serialization.py\", line 1540, in load\r\n    return _load(\r\n        opened_zipfile,\r\n    ...<3 lines>...\r\n        **pickle_load_args,\r\n    )\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\serialization.py\", line 2143, in _load\r\n    result = unpickler.load()\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\_weights_only_unpickler.py\", line 539, in load\r\n    self.append(self.persistent_load(pid))\r\n                ~~~~~~~~~~~~~~~~~~~~^^^^^\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\serialization.py\", line 2107, in persistent_load\r\n    typed_storage = load_tensor(\r\n        dtype, nbytes, key, _maybe_decode_ascii(location)\r\n    )\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\serialization.py\", line 2073, in load_tensor\r\n    wrap_storage = restore_location(storage, location)\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\serialization.py\", line 1879, in restore_location\r\n    return default_restore_location(storage, map_location)\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\serialization.py\", line 707, in default_restore_location\r\n    result = fn(storage, location)\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\serialization.py\", line 640, in _deserialize\r\n    device = _validate_device(location, backend_name)\r\n  File \"D:\\ProgramSoftware\\Conda\\Lib\\site-packages\\torch\\serialization.py\", line 609, in _validate_device\r\n    raise RuntimeError(\r\n    ...<5 lines>...\r\n    )\r\nRuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\r\n（诊断日志：D:\\ProgramData\\VScode\\SuperNote\\storage\\data\\logs\\note-generation-debug.log）"
    },
    {
      "id": "task_5b2aed6ad7bd",
      "status": "cancelled",
      "stage": "local_transcribe",
      "progress": 52,
      "message": "任务已取消",
      "sourceUrl": "https://www.bilibili.com/video/BV1ABziBrE9a/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=2a985ca1c3a6930df4b372bbf6082ce6",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "retryable": false,
      "createdAt": "2026-02-12T15:39:15.208Z",
      "updatedAt": "2026-02-12T15:48:18.465Z",
      "cancelReason": "user_cancelled"
    },
    {
      "id": "task_3d1be652dcaf",
      "status": "cancelled",
      "stage": "local_transcribe",
      "progress": 52,
      "message": "任务已取消",
      "sourceUrl": "https://www.bilibili.com/video/BV1ABziBrE9a/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=2a985ca1c3a6930df4b372bbf6082ce6",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "retryable": false,
      "createdAt": "2026-02-12T14:28:35.236Z",
      "updatedAt": "2026-02-12T14:48:15.135Z",
      "cancelReason": "user_cancelled"
    },
    {
      "id": "task_7b6f87e29d3b",
      "status": "cancelled",
      "stage": "transcribe",
      "progress": 24,
      "message": "任务已取消",
      "sourceUrl": "https://www.bilibili.com/video/BV1ABziBrE9a/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=2a985ca1c3a6930df4b372bbf6082ce6",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "retryable": false,
      "createdAt": "2026-02-12T14:28:16.880Z",
      "updatedAt": "2026-02-12T14:28:22.726Z",
      "cancelReason": "user_cancelled"
    },
    {
      "id": "task_a722be5304b3",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "retryable": false,
      "createdAt": "2026-02-12T11:08:30.666Z",
      "updatedAt": "2026-02-12T11:37:17.084Z",
      "resultMd": "# Python 面向对象高级：多层继承\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **多层继承定义**：类A继承类B，类B继承类C，形成链式继承结构（A → B → C），子类自动拥有所有父类的属性和方法\n2. **\"三代同堂\"模型**：最底层子类可以调用整个继承链上任意层级父类的方法，实现功能的逐层传递与累积\n3. **方法解析顺序（MRO）**：Python使用C3线性化算法确定方法调用优先级，可通过 `__mro__` 或 `mro()` 查看\n4. **`super()`的本质**：代表**直接父类**的引用，用于在子类中调用父类方法，遵循从左到右的继承查找顺序\n5. **两种调用父类方法的方式**：`ParentClass.method(self, ...)` 显式指定 vs `super().method(...)` 动态解析\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 一、多层继承的概念与类比\n\n| 术语 | 解释 |\n|:---|:---|\n| **多层继承** | 继承关系形成链条：A ← B ← C（C继承B，B继承A） |\n| **生活类比** | \"三代同堂\"/\"四世同堂\"——祖辈→父辈→子辈→孙辈，后代继承前代全部\"遗产\" |\n\n> **讲师比喻**：就像武术传承体系——祖师爷(Grandmaster) → 门派(Master/School) → 徒弟(Disciple) → 徒孙(GrandDisciple)。徒孙不仅学会师父的功夫，还能追溯调用师祖、祖师爷的绝技。\n\n### 二、继承体系的构建逻辑\n\n```\nobject (所有类的基类)\n    │\n    ├── Master (大师类)\n    │      └── make_master()\n    │\n    └── School (门派类)\n           └── make_school()\n                  │\n                  └── Disciple (徒弟类) ──继承──→ Master + School\n                         ├── make_disciple() [自身方法]\n                         ├── make_master()   [来自Master]\n                         └── make_school()   [来自School]\n                                │\n                                └── GrandDisciple (徒孙类) ──继承──→ Disciple的全部\n                                       └── 可调用: make_disciple / make_master / make_school\n```\n\n### 三、关键机制深度解析\n\n#### 3.1 `self` vs `super()` 的本质区别\n\n| 关键字 | 含义 | 使用场景 |\n|:---|:---|:---|\n| `self` | **当前实例**的引用 | 访问本类属性/方法；作为参数传递给其他类的方法 |\n| `super()` | **直接父类**的引用 | 在子类中调用父类被重写的方法；实现 cooperative multiple inheritance |\n\n```python\n# super() 的等价理解（简化版）\nsuper()  ≈  当前类的直接父类\n```\n\n#### 3.2 两种调用父类方法的对比\n\n| 方式 | 语法 | 特点 | 适用场景 |\n|:---|:---|:---|:---|\n| **显式类名调用** | `ParentClass.method(self, args)` | 可指定任意父类；需手动传`self` | 需要明确调用特定父类时 |\n| **super()调用** | `super().method(args)` | 自动按MRO查找下一个类；无需传`self` | 标准单继承链；多继承协作 |\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### 完整代码示例：三层继承体系\n\n```python\n# ========== 第一层：基类 ==========\n\nclass Master:\n    \"\"\"武术大师 - 继承链顶端\"\"\"\n    \n    def __init__(self):\n        self.kungfu = \"古法太极拳\"\n    \n    def make_master(self):\n        \"\"\"大师绝技\"\"\"\n        print(f\"运用【{self.kungfu}】\")\n        return self  # 支持链式调用\n\n\nclass School:\n    \"\"\"武术门派 - 与Master同级，不同分支\"\"\"\n    \n    def __init__(self):\n        self.kungfu = \"新派咏春拳\"\n    \n    def make_school(self):\n        \"\"\"门派绝技\"\"\"\n        print(f\"运用【{self.kungfu}】\")\n        return self\n\n\n# ========== 第二层：中间层 ==========\n\nclass Disciple(Master, School):  # 多继承：同时继承Master和School\n    \"\"\"\n    徒弟类 - 继承Master和School\n    MRO: Disciple → Master → School → object\n    \"\"\"\n    \n    def __init__(self):\n        # 调用第一个父类(Master)的构造\n        super().__init__()\n        self.skill = \"融会贯通\"\n    \n    def make_disciple(self):\n        \"\"\"徒弟自身技能\"\"\"\n        print(f\"以【{self.skill}】驾驭武学\")\n        # 调用父类方法（方式一：super()）\n        super().make_master()\n        return self\n    \n    def make_school(self):\n        \"\"\"重写School的方法 - 优先调用子类版本\"\"\"\n        print(\"徒弟改良后的门派功夫\")\n        # 调用父类被重写的方法（方式二：显式指定）\n        School.make_school(self)\n        return self\n\n\n# ========== 第三层：多层继承的核心 ==========\n\nclass GrandDisciple(Disciple):\n    \"\"\"\n    徒孙类 - 多层继承的典型\n    自动拥有：Disciple + Master + School 的全部功能\n    MRO: GrandDisciple → Disciple → Master → School → object\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()  # 调用Disciple.__init__\n        self.talent = \"青出于蓝\"\n    \n    def show_lineage(self):\n        \"\"\"展示完整的继承链能力\"\"\"\n        print(f\"\\n=== {self.talent} ===\")\n        self.make_disciple()   # 来自Disciple（含Master的能力）\n        self.make_school()     # 来自Disciple的重写版本\n        self.make_master()     # 直接来自Master\n\n\n# ========== 测试验证 ==========\n\nif __name__ == \"__main__\":\n    # 创建徒孙实例\n    ts = GrandDisciple()\n    \n    # 验证：徒孙可以调用所有层级的方法\n    ts.show_lineage()\n    \n    # 查看方法解析顺序（MRO）\n    print(f\"\\nMRO: {[cls.__name__ for cls in GrandDisciple.__mro__]}\")\n```\n\n### 输出结果\n\n```\n=== 青出于蓝 ===\n以【融会贯通】驾驭武学\n运用【古法太极拳】\n徒弟改良后的门派功夫\n运用【新派咏春拳】\n\nMRO: ['GrandDisciple', 'Disciple', 'Master', 'School', 'object']\n```\n\n### MRO 计算原理（C3线性化）\n\n对于多重继承 `class D(C1, C2, ..., Cn)`，MRO计算公式为：\n\n$$\\text{L}(D) = D + \\text{merge}(\\text{L}(C_1), \\text{L}(C_2), ..., \\text{L}(C_n), [C_1, C_2, ..., C_n])$$\n\n其中 `merge` 操作遵循**头部优先**原则，确保：\n- 子类永远在父类之前\n- 多个父类保持声明时的从左到右顺序\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n### ❌ 易错点\n\n| 错误 | 原因 | 正确做法 |\n|:---|:---|:---|\n| `super().method(self, x)` | 重复传递`self` | `super().method(x)` —— super已自动绑定实例 |\n| 菱形继承问题（钻石问题） | 共同祖先被多次初始化 | 统一使用`super()`而非显式父类名，配合`kwargs` |\n| 混淆继承方向 | `A(B)`表示A继承B，不是B继承A | 记住：括号内是**父类**，类名是**子类** |\n\n### 💡 关键问答\n\n**Q1：子类重写父类方法后，会优先调用哪个？**\n> **A**：优先调用**子类**自己的版本。这是\"方法重写（Override）\"的核心特性。如需调用父类原版，需通过 `super().method()` 或 `ParentClass.method(self)` 显式指定。\n\n**Q2：`super()`在多继承中如何工作？**\n> **A**：`super()` 不是简单的\"直接父类\"，而是严格遵循 **MRO列表中的下一个类**。例如 `Disciple(Master, School)` 的MRO中，`super()`在`Disciple`内指向`Master`，在`Master`内指向`School`。\n\n**Q3：能否出现循环继承？如 A→B→C→A？**\n> **A**：**绝对禁止**。Python会在类定义阶段抛出 `TypeError: Cannot create a consistent method resolution order (MRO)`。继承必须是有向无环图（DAG）。\n\n**Q4：多层继承的最大建议深度？**\n> **A**：官方建议不超过 **2-3层**。过深的继承链会导致：\n> - 代码可读性急剧下降\n> - 调试困难（难以追踪方法来源）\n> - 紧耦合，违反\"组合优于继承\"原则\n\n### 🔧 最佳实践\n\n```python\n# ✅ 推荐：使用super()保持继承链的灵活性\nclass Child(Parent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)  # 转发所有参数\n        \n# ❌ 避免：硬编码父类名，破坏扩展性\nclass Child(Parent):\n    def __init__(self, x, y):\n        Parent.__init__(self, x, y)  # 若继承关系改变，此处需手动修改\n```",
      "debug": {
        "keyframeStats": [
          {
            "url": "https://www.bilibili.com/video/BV1U2WmzfEqp?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=28",
            "sceneCount": 10,
            "candidateCount": 20,
            "afterBlackFilter": 20,
            "afterBlurFilter": 20,
            "afterDedupe": 12,
            "finalCount": 12,
            "elapsedMs": 26461
          }
        ],
        "keyframeWarnings": [
          "[1/1] KEYFRAME_WARN_ADAPTIVE_PROFILE:short"
        ]
      }
    },
    {
      "id": "task_6e3e178d17ec",
      "status": "success",
      "stage": "done",
      "progress": 100,
      "message": "生成完成",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=27",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "createdAt": "2026-02-12T10:07:45.941Z",
      "updatedAt": "2026-02-12T10:19:34.671Z",
      "resultMd": "# Python 面向对象高级：子类访问父类成员 - `super()` 方式详解\n\n## 核心概念摘要 (Key Takeaways)\n\n1. **`super()` 是第二种调用父类方法的方式**，与 `父类名.方法(self)` 形成互补，但两者适用场景截然不同\n2. **`super()` 遵循 MRO（方法解析顺序）链式查找**：从当前类的直接父类开始，按继承列表从左到右依次查找，**有则用之，无则继续向后追溯**\n3. **`super()` 仅限单继承场景优雅使用**，多继承时因查找路径不透明而难以精准控制\n4. **无法跳过中间父类**：即使第一个父类实现了目标方法，`super()` 也不会自动越过它去访问更远的父类\n5. **Python 3 的 `super()` 可省略参数**，自动绑定当前类和实例，但底层仍遵循严格的 MRO 规则\n\n---\n\n## 知识点详解 (Detailed Notes)\n\n### 一、`super()` 的基本语法与特性\n\n| 特性 | 说明 |\n|:---|:---|\n| 语法形式 | `super().父类方法名()` —— Python 3 简化写法 |\n| 参数传递 | **无需也不能手动传入 `self`**，这是与 `父类名.方法(self)` 的关键区别 |\n| 查找机制 | 按 **MRO (Method Resolution Order)** 顺序，从左到右、由近及远 |\n| 适用场景 | **单继承**代码简洁优雅；多继承时路径不透明，慎用 |\n\n> 💡 讲师比喻：`super()` 就像\"按顺序敲门\"——先敲最近的门，有人应就进去，没人应才敲下一扇。你无法指定\"我要直接敲第三扇门\"。\n\n### 二、MRO 链式查找的深层机制\n\n#### 2.1 基础继承结构演示\n\n```python\nclass Master:\n    def make_cake(self):\n        print(\"古法煎饼果子\")\n\nclass School:\n    def make_cake(self):\n        print(\"黑马煎饼果子\")\n\n# 关键：继承顺序决定 super() 的查找起点\nclass Prentice(School, Master):   # ← School 在前，Master 在后\n    def make_old_cake(self):\n        super().make_cake()        # 实际调用 School.make_cake()\n```\n\n**执行结果**：输出 `\"黑马煎饼果子\"` —— 因为 `School` 在继承列表中排第一。\n\n#### 2.2 调换继承顺序的影响\n\n```python\nclass Prentice(Master, School):   # ← 调换顺序：Master 在前\n    def make_old_cake(self):\n        super().make_cake()        # 现在调用 Master.make_cake()\n```\n\n**结论**：`super()` 的查找起点完全由 **类定义时的继承列表顺序** 决定。\n\n### 三、关键验证：缺失方法的链式追溯\n\n#### 3.1 实验设计：故意让第一个父类\"缺席\"\n\n```python\nclass Master:\n    def make_cake(self):\n        print(\"古法煎饼果子\")\n\nclass School:\n    pass  # ← 故意不实现 make_cake\n\nclass Prentice(School, Master):   # School 在前但没有该方法\n    def make_old_cake(self):\n        super().make_cake()        # 会报错？还是继续找？\n```\n\n#### 3.2 运行结果与原理\n\n```\n古法煎饼果子\n```\n\n**核心发现**：`super()` 并非\"只查最近一级\"，而是 **\"按 MRO 顺序遍历，直到找到或耗尽\"**。\n\n- 先在 `School` 中查找 `make_cake` → 未找到\n- 继续沿 MRO 链向后查找 → 在 `Master` 中找到并调用\n\n> ⚠️ **版本差异提示**：Python 旧版本（如 3.8 之前某些环境）可能在首层未找到时直接报错。现代 Python（3.10+）统一采用 **链式追溯** 策略。\n\n#### 3.3 终极边界：追溯到 `object`\n\n```python\nclass Master:\n    pass  # 也没有 make_cake\n\nclass School:\n    pass\n\nclass Prentice(School, Master):\n    def make_old_cake(self):\n        super().make_cake()        # 两层都没有，最终找到 object\n```\n\n**报错信息**：\n```\nAttributeError: 'super' object has no attribute 'make_cake'\n```\n\n此时 `super()` 已指向 MRO 链末端 —— `object` 基类，自然不存在自定义方法。\n\n---\n\n## 代码实战 / 算法推导 (Code & Math)\n\n### 完整对比实验代码\n\n```python\n\"\"\"\nsuper() 调用机制深度验证\n演示：MRO 链式查找、无法跨级访问等核心特性\n\"\"\"\n\n\nclass Master:\n    \"\"\"师承派：传统技艺\"\"\"\n    \n    def make_cake(self):\n        print(\"[Master] 古法煎饼果子 —— 酥脆传承\")\n\n\nclass School:\n    \"\"\"学院派：现代改良\"\"\"\n    \n    # 注释掉此方法以测试链式查找\n    # def make_cake(self):\n    #     print(\"[School] 黑马煎饼果子 —— 流量密码\")\n    pass\n\n\nclass Prentice(School, Master):\n    \"\"\"\n    学徒类：继承顺序 School → Master\n    \n    MRO 顺序: Prentice → School → Master → object\n    \"\"\"\n    \n    def make_own_cake(self):\n        \"\"\"独创流派\"\"\"\n        print(\"[Prentice] 独创黑暗料理煎饼\")\n    \n    def make_old_cake_with_super(self):\n        \"\"\"\n        使用 super() 调用父类方法\n        \n        查找路径: School(无) → Master(有) → 调用 Master.make_cake\n        \"\"\"\n        print(\"--- 使用 super() 调用 ---\")\n        super().make_cake()  # 最终调用 Master.make_cake()\n    \n    def make_old_cake_with_explicit(self):\n        \"\"\"\n        使用显式父类名调用 —— 精准控制\n        \n        直接指定 Master，不受继承顺序影响\n        \"\"\"\n        print(\"--- 使用 父类名.方法() 调用 ---\")\n        Master.make_cake(self)  # 强制调用 Master\n\n\n# ==================== 测试场景 ====================\n\nif __name__ == \"__main__\":\n    p = Prentice()\n    \n    print(\"=\" * 40)\n    print(\"场景1：super() 的链式查找\")\n    p.make_old_cake_with_super()\n    \n    print(\"\\n\" + \"=\" * 40)\n    print(\"场景2：显式父类名的精准调用\")\n    p.make_old_cake_with_explicit()\n    \n    print(\"\\n\" + \"=\" * 40)\n    print(\"场景3：验证 MRO 顺序\")\n    print(f\"MRO: {[cls.__name__ for cls in Prentice.__mro__]}\")\n```\n\n### 输出结果\n\n```\n========================================\n场景1：super() 的链式查找\n--- 使用 super() 调用 ---\n[Master] 古法煎饼果子 —— 酥脆传承\n\n========================================\n场景2：显式父类名的精准调用\n--- 使用 父类名.方法() 调用 ---\n[Master] 古法煎饼果子 —— 酥脆传承\n\n========================================\n场景3：验证 MRO 顺序\nMRO: ['Prentice', 'School', 'Master', 'object']\n```\n\n### 两种方式的数学化对比\n\n| 维度 | `父类名.方法(self)` | `super().方法()` |\n|:---|:---|:---|\n| **控制粒度** | 精确制导：$\\text{target} = \\text{指定父类}$ | 模糊匹配：$\\text{target} = \\text{MRO}[i], \\min\\{i: \\text{hasattr}(\\text{MRO}[i], \\text{method})\\}$ |\n| **灵活性** | 高：无视继承顺序 | 低：严格绑定 MRO |\n| **维护性** | 低：类名硬编码，重构易遗漏 | 高：自动适应继承变化 |\n| **多继承安全** | 明确可控 | 路径不透明，易踩坑 |\n\n---\n\n## 避坑指南 & 常见QA (Tips & QA)\n\n### ❌ 典型错误\n\n| 错误写法 | 问题 | 修正 |\n|:---|:---|:---|\n| `super().make_cake(self)` | 重复传 `self`，`super()` 已自动绑定 | `super().make_cake()` |\n| `super(Prentice, self).make_cake()` | Python 2 冗余写法（虽兼容但冗长） | 直接用 `super().make_cake()` |\n| 试图用 `super()` 跳过 `School` 直接找 `Master` | **机制上不可能** | 改用 `Master.make_cake(self)` |\n\n### 🔥 高频疑问\n\n**Q1：`super()` 和 `父类名.方法()` 到底选哪个？**\n\n> **单继承**：优先 `super()`，代码简洁且易于维护（父类改名不影响）\n> **多继承需精准控制**：必须用 `父类名.方法(self)`，否则逻辑极易混乱\n\n**Q2：为什么 `super()` 不能跳过第一个父类？**\n\n> 这是 MRO 的设计哲学。`super()` 的设计目标是 **协作式多重继承**（cooperative multiple inheritance），要求每个类都\"有机会\"参与方法链。若允许跳过，会破坏继承体系的一致性和可预测性。\n\n**Q3：如何查看类的 MRO 顺序？**\n\n```python\n# 三种等价方式\nprint(Prentice.__mro__)           # 元组形式\nprint(Prentice.mro())             # 列表形式\nhelp(Prentice)                    # 详细文档中包含 MRO\n```\n\n### ⚠️ 版本兼容性注意\n\n| Python 版本 | `super()` 行为差异 |\n|:---|:---|\n| 2.x | 必须写 `super(Prentice, self)` |\n| 3.0-3.7 | 链式查找可能因实现细节有差异 |\n| 3.8+ | 统一稳定的 C3 线性化 MRO 算法 |\n| 3.10+ | 推荐生产环境版本，行为最可预测 |\n\n### 🎯 最佳实践总结\n\n```\n┌─────────────────────────────────────────┐\n│  单继承场景                              │\n│  └── 无脑用 super()，简洁又优雅          │\n├─────────────────────────────────────────┤\n│  多继承场景                              │\n│  ├── 需要灵活/精准控制 → 父类名.方法(self) │\n│  └── 框架级协作设计 → 统一用 super()      │\n│      （需确保所有类都遵循相同协议）        │\n└─────────────────────────────────────────┘\n```",
      "debug": {
        "keyframeStats": [
          {
            "url": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=27",
            "sceneCount": 24,
            "candidateCount": 48,
            "afterBlackFilter": 48,
            "afterBlurFilter": 48,
            "afterDedupe": 30,
            "finalCount": 28,
            "elapsedMs": 17168
          }
        ],
        "keyframeWarnings": [
          "[1/1] KEYFRAME_WARN_ADAPTIVE_PROFILE:short"
        ]
      }
    },
    {
      "id": "task_74246a1b1548",
      "status": "failed",
      "stage": "generate",
      "progress": 92,
      "message": "模型生成失败",
      "sourceUrl": "https://www.bilibili.com/video/BV1U2WmzfEqp/?spm_id_from=333.788.videopod.episodes&vd_source=2a985ca1c3a6930df4b372bbf6082ce6&p=27",
      "sourceType": "bilibili",
      "promptId": "prompt-mlj9ru09-qr7bzu",
      "modelId": "openai_compatible",
      "createdAt": "2026-02-12T09:45:37.728Z",
      "updatedAt": "2026-02-12T09:58:24.496Z",
      "error": "连接超时"
    }
  ],
  "settings": {
    "models": [
      {
        "id": "gemini",
        "provider": "gemini",
        "enabled": false,
        "isDefault": false,
        "baseUrl": "https://generativelanguage.googleapis.com/v1beta",
        "apiKey": "",
        "modelName": "gemini-2.5-flash",
        "timeoutMs": 60000
      },
      {
        "id": "chatgpt",
        "provider": "chatgpt",
        "enabled": false,
        "isDefault": false,
        "baseUrl": "https://api.openai.com/v1",
        "apiKey": "",
        "modelName": "gpt-5-mini",
        "timeoutMs": 60000
      },
      {
        "id": "openai_compatible",
        "provider": "openai_compatible",
        "enabled": true,
        "isDefault": true,
        "baseUrl": "https://api-inference.modelscope.cn/v1",
        "apiKey": "ms-f0299f51-54bc-4c6c-8478-3296d994a068",
        "modelName": "moonshotai/Kimi-K2.5",
        "timeoutMs": 60000
      }
    ],
    "prompts": [
      {
        "id": "prompt-mlj9ru09-qr7bzu",
        "name": "B站视频笔记",
        "template": "角色定位：你是一名计算机科学专业的学霸，专注于人工智能与Python开发。你拥有极强的归纳总结能力，擅长将碎片化、口语化的课堂录音整理成逻辑严密、干货满满的Markdown学习笔记。\n\n工作背景：用户将提供一段视频录音转写的文字稿。领域主要集中在Python编程或深度学习。原始数据通常包含大量口语、重复表达、互动闲聊以及语音识别错误。\n\n任务目标：将原始文本转化为一份‘可读性极强’的复习笔记，确保即使未观看视频的用户也能掌握核心知识点。\n\n核心行为规则：\n1) 智能纠错与专业化：\na) 根据上下文修正语音识别错误。例如，将‘南派’修正为 'numpy'，‘套吃’修正为 'torch'，‘恩恩点琳儿’修正为 'nn.Linear'。\nb) 识别并修正专业术语的拼写错误，确保技术词汇的准确性。\n\n2) 内容去噪与重组：\na) 删除口头禅（如‘那个’、‘然后’）、跑题闲聊和设备调试等无关信息。\nb) 采用归纳法将散落在不同段落的同一知识点进行合并，形成系统化的论述。\n\n3) 代码还原与标准化：\na) 将讲师口述的代码逻辑还原为标准的Python代码块。\nb) 为关键步骤添加简短、清晰的注释。\nc) 遵循PEP 8代码规范。\n\n4) 数学公式转换：\na) 针对深度学习中的数学推导，使用标准LaTeX格式表示（例如：$y = wx + b$）。\n\n5) 图表逻辑总结：\na) 如果原文提到图表，根据讲师的口头描述，用文字总结图表传达的核心逻辑和结论。\n\n输出结构要求：\n1) 核心概念摘要 (Key Takeaways)：用3-5个要点总结本次课程解决的核心问题或关键概念。\n2) 知识点详解 (Detailed Notes)：使用清晰的多级标题结构，详细记录理论、概念及讲师使用的生动比喻。\n3) 代码实战 / 算法推导 (Code & Math)：提供格式规范的代码示例及背后对应的数学原理或逻辑解析。\n4) 避坑指南 & 常见QA (Tips & QA)：总结讲师提到的易错点、注意事项以及课堂上的代表性问答。\n5) 最终提供可下载的markdown文件\n\n语气与风格：\n* 表达风格专业、严谨且富有逻辑性。\n* 保持学术热情，旨在通过简洁的语言将复杂概念简单化。\n* 模拟一名乐于分享的高材生，语气自信且乐于助人。\n\n限制条件：\n* 输出结果中严禁包含任何开头问候语（如‘你好’、‘很高兴为你服务’等）。\n* 输出结果中严禁包含任何结尾结语或后续建议（如‘希望对你有帮助’、‘你可以进一步询问...’等）。\n* 直接从核心概念摘要开始输出，到避坑指南结束。",
        "variables": [],
        "isDefault": true,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mlj9si0i-dng6dv",
        "name": "K8s笔记",
        "template": "# 角色\n你是一名资深的 Kubernetes 讲师和学习笔记整理专家，擅长将官方技术文档转化为初学者易于理解的笔记。\n\n# 任务\n根据用户提供的 Kubernetes 官方教程网页内容，生成一份**结构清晰、要点突出、便于复习**的笔记。笔记应涵盖该页面的核心知识点、操作步骤、常用命令、关键配置示例以及注意事项。\n\n# 内容过滤要求\n- 只关注教程正文，忽略页面上的导航菜单、广告、相关推荐、评论区等无关内容。\n- 如果网页中包含多个小节，按逻辑顺序依次整理，不要遗漏主要知识点。\n- 对于代码或命令行示例，必须完整保留并标明适用场景。\n\n# 输出格式规范\n请使用 **Markdown** 格式输出，具体要求如下：\n\n1. **主标题**：使用 `#`，直接采用教程页面的标题。\n2. **二级标题**：按知识点或步骤划分，例如 `## 核心概念`、`## 操作步骤`、`## 命令示例`、`## 注意事项` 等。\n3. **列表与要点**：使用无序列表 `-` 清晰罗列关键信息，避免大段连续文字。\n4. **代码块**：所有命令、YAML 配置、输出示例均使用围栏代码块（```），并标注语言（如 `bash`、`yaml`、`console`）。\n5. **强调**：重要概念、易错点、提示信息可使用 **加粗** 或 > 引用块突出显示。\n6. **精简**：去除冗余修饰词，只保留知识本身；用自己的话重新组织，但确保技术准确性。\n\n# 处理步骤（可内化，无需输出）\n1. 阅读并理解网页全文。\n2. 识别教程类型（概念讲解、操作指南、配置示例等）。\n3. 抽取核心内容，按“是什么—为什么—怎么做”的逻辑重组。\n4. 按上述格式规范撰写笔记。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-minimal",
        "name": "精简笔记",
        "template": "角色定位：你是一名技术笔记整理助手，擅长将视频转写内容压缩为高密度要点。\n\n任务目标：将输入内容整理为“短而全”的复习笔记，优先保留结论、关键步骤、易错点，删除口头语、重复和无关闲聊。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构固定为：\n   - ## 核心结论\n   - ## 关键步骤\n   - ## 易错点\n   - ## 关键术语\n3. 每个小节使用 3-8 条要点，单条尽量一句话。\n4. 代码仅保留最小可运行示例，避免冗长解释。\n5. 若原文信息不足，不编造；可标注“原文未明确”。\n\n风格要求：\n- 简洁、直接、信息密度高。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-detailed",
        "name": "详细笔记",
        "template": "角色定位：你是一名技术课程学习教练，擅长把口语化讲解重构成完整、清晰、可复习的学习笔记。\n\n任务目标：根据输入的转写内容，生成结构化、覆盖完整的详细笔记，保证未看视频的人也能理解主要内容。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构建议：\n   - # 主题标题\n   - ## 核心概念摘要\n   - ## 知识点详解\n   - ## 代码与示例\n   - ## 常见问题与避坑\n   - ## 复习清单\n3. 对术语、变量名、命令、参数保持准确；必要时纠正明显识别错误。\n4. 代码示例使用围栏代码块，并给出简要解释。\n5. 对关键流程按“是什么 -> 为什么 -> 怎么做”展开。\n6. 若提到公式，使用标准 LaTeX 表达。\n\n风格要求：\n- 专业、清晰、条理化。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-xiaohongshu",
        "name": "小红书风格笔记",
        "template": "角色定位：你是一名技术内容创作者，擅长把专业知识整理成“易传播、易阅读”的小红书风格笔记。\n\n任务目标：在保证技术准确的前提下，把输入内容改写为更有阅读吸引力的图文笔记。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构建议：\n   - # 吸引力标题（1行）\n   - ## 这篇讲什么（3-5条）\n   - ## 重点拆解（分点）\n   - ## 实操步骤 / 代码片段\n   - ## 避坑提醒\n   - ## 一句话总结\n3. 可适度使用 emoji（每段 0-2 个），但不要过度。\n4. 每段尽量短句，增强可读性。\n5. 保留关键术语、命令、参数准确性，不夸张、不造概念。\n\n风格要求：\n- 亲切、清楚、有节奏感。\n- 传播感强但不营销腔，不写空话套话。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-tutorial",
        "name": "教程笔记",
        "template": "角色定位：你是一名技术实战讲师，擅长将讲解内容整理为可跟做的教程。\n\n任务目标：把输入内容重组为“按步骤执行即可复现”的教程型笔记，强调前置条件、步骤顺序、结果验证和常见错误。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构固定为：\n   - # 教程标题\n   - ## 适用场景\n   - ## 前置条件\n   - ## 操作步骤（Step 1..N）\n   - ## 验证结果\n   - ## 常见问题\n3. 每一步包含：目标、操作、预期结果。\n4. 命令与配置必须用代码块展示，并标注语言。\n5. 如原文步骤缺失，明确标注“原文未提供完整步骤”。\n\n风格要求：\n- 教学导向、可执行、顺序清晰。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-academic",
        "name": "学术笔记",
        "template": "角色定位：你是一名学术写作助理，擅长以严格、结构化方式整理技术课程内容。\n\n任务目标：将输入内容整理为术语准确、逻辑严谨的学术风格笔记，突出定义、推理过程、边界条件与结论。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构建议：\n   - # 主题\n   - ## 研究对象与问题定义\n   - ## 核心概念与符号说明\n   - ## 方法与推理过程\n   - ## 结论与适用边界\n   - ## 参考要点\n3. 概念定义优先，避免口语化表达。\n4. 涉及公式时使用 LaTeX。\n5. 对不确定信息使用“原文未充分说明”标记，不主观臆断。\n\n风格要求：\n- 用词正式、定义先行、论证清晰。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-life-journal",
        "name": "生活向笔记",
        "template": "角色定位：你是一名内容整理编辑，擅长将知识内容转为更自然、有代入感的生活化表达。\n\n任务目标：在保证技术准确的前提下，将输入内容整理为“易读、易懂、贴近日常使用场景”的生活向笔记。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构建议：\n   - # 主题\n   - ## 今天学到了什么\n   - ## 关键点拆解\n   - ## 实际可用场景\n   - ## 个人提醒\n3. 每段尽量短句，使用自然中文表达。\n4. 保留术语准确性，但可在术语后补一句通俗解释。\n5. 不夸张、不鸡汤，不偏离原文信息。\n\n风格要求：\n- 自然、亲切、可读性优先。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-task-oriented",
        "name": "任务导向笔记",
        "template": "角色定位：你是一名项目执行教练，擅长把知识内容拆解成可落地任务。\n\n任务目标：将输入内容转化为行动清单，突出目标、任务分解、里程碑、验收标准与风险。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构固定为：\n   - # 任务目标\n   - ## 任务拆解（按优先级）\n   - ## 里程碑与时间建议\n   - ## 验收标准\n   - ## 风险与应对\n3. 每个任务包含：负责人角色（可泛化）、输入、输出、完成标准。\n4. 尽量使用 checklist 形式（- [ ]）。\n5. 不添加原文未出现的强假设。\n\n风格要求：\n- 结果导向、清单化、可执行。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-business",
        "name": "商业风格笔记",
        "template": "角色定位：你是一名商业分析顾问，擅长把技术内容转化为管理可读的决策材料。\n\n任务目标：将输入内容整理为商业风格笔记，突出结论、依据、影响、行动建议。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构建议：\n   - # 执行摘要\n   - ## 关键结论\n   - ## 决策依据\n   - ## 业务影响\n   - ## 行动建议\n   - ## 风险与假设\n3. 每个结论尽量提供对应证据或来源片段。\n4. 语言正式、简洁，避免网络化表达。\n5. 原文没有的数据不补造。\n\n风格要求：\n- 正式、精准、面向决策。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      },
      {
        "id": "prompt-mll-template-meeting-minutes",
        "name": "会议纪要笔记",
        "template": "角色定位：你是一名会议纪要专员，擅长从讨论内容中提炼议题、结论和待办。\n\n任务目标：将输入内容整理成结构化会议纪要，清楚呈现讨论主题、关键结论、责任人与截止时间。\n\n输出要求：\n1. 使用 Markdown 输出。\n2. 结构固定为：\n   - # 会议主题\n   - ## 参会角色\n   - ## 议题与讨论要点\n   - ## 已达成结论\n   - ## 待办事项（Owner / 截止时间 / 状态）\n   - ## 待确认问题\n3. 待办事项使用表格或 checklist。\n4. 未明确责任人或时间时标注“待确认”。\n5. 保持客观记录，不加入主观评价。\n\n风格要求：\n- 纪要化、客观、可追踪。\n- 不写开场寒暄，不写结尾客套。",
        "variables": [],
        "isDefault": false,
        "updatedAt": "2026-02-12T14:15:53.957Z"
      }
    ],
    "integrations": {
      "jinaReader": {
        "endpoint": "https://r.jina.ai/",
        "apiKey": "",
        "timeoutSec": 30,
        "noCache": false
      }
    },
    "localTranscriber": {
      "engine": "whisper_cli",
      "command": "env/python.exe",
      "ffmpegBin": "tools/ffmpeg/bin/ffmpeg.exe",
      "model": "base",
      "language": "zh",
      "device": "cuda",
      "cudaChecked": false,
      "cudaAvailable": false,
      "cudaEnabledOnce": true,
      "beamSize": 5,
      "temperature": 0,
      "timeoutMs": 1800000
    },
    "videoUnderstanding": {
      "enabled": true,
      "maxFrames": 24,
      "sceneThreshold": 0.3,
      "perSceneMax": 2,
      "minSceneGapSec": 2,
      "dedupeHashDistance": 6,
      "blackFrameLumaThreshold": 18,
      "blurVarianceThreshold": 80,
      "extractWidth": 640,
      "timeoutMs": 120000
    }
  }
}